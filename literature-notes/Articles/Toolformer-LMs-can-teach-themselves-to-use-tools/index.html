<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Toolformer: LMs can teach themselves to use tools  Metadata  Author: [[arxiv.org]] Full Title: Toolformer: LMs can teach themselves to use tools Category: #articles URL: https://arxiv."><title>Toolformer: LMs can teach themselves to use tools</title><meta name=viewport content="width=device-width,initial-scale=1"><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjFhYTEyN2VjLWI0MTEtNDdjNS1iNWQzLTA5OGE2NDZjMWZhYSIsICJpZCI6ICI5YzFhNzE4Zi0xNjA1LTRmMTUtOGQ3Yy05NzliMDBjNWVmNTcifQ.-aCZYTIrTPiQCDYC_zS0dH0IVOXqI9ThnK39DMDgY7c></script><link rel="shortcut icon" type=image/png href=https://pelayoarbues.github.io//icon.png><link href=https://pelayoarbues.github.io/styles.23ddc3ecf7e887d89acd50af3b520de2.min.css rel=stylesheet><link href=https://pelayoarbues.github.io/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css rel=stylesheet id=theme-link><script src=https://pelayoarbues.github.io/js/darkmode.d1ab992b86a0866f970f82ef4d95b010.min.js></script>
<script src=https://pelayoarbues.github.io/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://pelayoarbues.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script>const isReducedMotion=window.matchMedia("(prefers-reduced-motion: reduce)").matches,lastVisit=localStorage.getItem("lastVisitTime"),now=Date.now();let show="true";if(lastVisit){document.documentElement.setAttribute("visited","true");const e=Math.ceil((now-parseInt(lastVisit))/(1e3*60));show=!isReducedMotion&&e>5?"true":"false"}document.documentElement.setAttribute("show-animation",show),localStorage.setItem("lastVisitTime",`${now}`);const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pelayoarbues.github.io/",fetchData=Promise.all([fetch("https://pelayoarbues.github.io/indices/linkIndex.c7c7e124104f3b01428bc3f049a79366.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pelayoarbues.github.io/indices/contentIndex.b1e4950944cb7c485754f8f0a9165b1f.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://pelayoarbues.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pelayoarbues.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:.8,opacityScale:3,repelForce:2,scale:1})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pelayoarbues.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pelayoarbues.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pelayoarbues.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div id=cursor-chat-layer><input type=text id=cursor-chat-box></div><script type=module>
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script><div class=singlePage><header class="delay t-3"><h1 id=page-title><a class=root-title href=https://pelayoarbues.github.io/>Pelayo Arbués</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Toolformer: LMs can teach themselves to use tools</h1><p class=meta>Last updated
Feb 15, 2023</p><ul class=tags><li><a href=https://pelayoarbues.github.io/tags/articles/>Articles</a></li><li><a href=https://pelayoarbues.github.io/tags/literature-note/>Literature Note</a></li></ul><a href=#toolformer-lms-can-teach-themselves-to-use-tools><h1 id=toolformer-lms-can-teach-themselves-to-use-tools><span class=hanchor arialabel=Anchor># </span>Toolformer: LMs can teach themselves to use tools</h1></a><p><img src=https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png width=auto alt=rw-book-cover></p><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">arxiv.org</a></li><li>Full Title: Toolformer: LMs can teach themselves to use tools</li><li>Category: #articles</li><li>URL:
<a href=https://arxiv.org/pdf/2302.04761v1.pdf rel=noopener>https://arxiv.org/pdf/2302.04761v1.pdf</a></li></ul><a href=#highlights><h2 id=highlights><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>Toolformer: Language Models Can Teach Themselves to Use Tools (
<a href=https://read.readwise.io/read/01gs56zjwr4x67myv3mm7x66k9 rel=noopener>View Highlight</a>)</li><li>we show that
LMs can teach themselves to use external tools
via simple APIs (
<a href=https://read.readwise.io/read/01gs55kwd9kr4vas1cdfwhs26p rel=noopener>View Highlight</a>)</li><li>Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. (
<a href=https://read.readwise.io/read/01gs55m3606zentxqprj1gp7b9 rel=noopener>View Highlight</a>)</li><li>. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities. (
<a href=https://read.readwise.io/read/01gs55mpwycfyp44pg4gjc83aj rel=noopener>View Highlight</a>)</li><li>, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciﬁc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), (
<a href=https://read.readwise.io/read/01gs55pa95z2zj3cpb168ekntk rel=noopener>View Highlight</a>)</li><li>limitations include an inability to access
up-to-date information on recent events (
<a href=https://read.readwise.io/read/01gs55n8qrc91eer80fc74w9sj rel=noopener>View Highlight</a>)</li><li>tendency to hallucinate
facts ( (
<a href=https://read.readwise.io/read/01gs55nb745dh6nn5gfg88k9pb rel=noopener>View Highlight</a>)</li><li>difﬁcul-
ties in understanding low-resource languages (
<a href=https://read.readwise.io/read/01gs55ndxva4tqsbw3e9qt4b5y rel=noopener>View Highlight</a>)</li><li>a lack of mathematical skills to per-
form precise calculations (
<a href=https://read.readwise.io/read/01gs55ngp7a4x9j123mfx7h8rt rel=noopener>View Highlight</a>)</li><li>) and an
unawareness of the progression of time (
<a href=https://read.readwise.io/read/01gs55nk10e7szqrk6ybnmzfbk rel=noopener>View Highlight</a>)</li><li>The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. (
<a href=https://read.readwise.io/read/01gs55qqchkt3x8ygg1xj5r9p4 rel=noopener>View Highlight</a>)</li><li>The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. (
<a href=https://read.readwise.io/read/01gs55qz0f2332adm14jdjtmq2 rel=noopener>View Highlight</a>)</li><li>Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call. (
<a href=https://read.readwise.io/read/01gs56404drd4dzfjjj903bs74 rel=noopener>View Highlight</a>)</li><li>using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (
<a href=https://read.readwise.io/read/01gs5627sh3h78gknv00rwshbc rel=noopener>View Highlight</a>)</li><li>Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
ﬁnetune the LM itself on the API calls that it con-
siders useful. (
<a href=https://read.readwise.io/read/01gs562vsakc7e6gm4tkvz0mh2 rel=noopener>View Highlight</a>)</li><li>As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself – for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus (
<a href=https://read.readwise.io/read/01gs5651fp2zbjy5s11ykxtfxy rel=noopener>View Highlight</a>)</li><li>For each API, we write a
prompt P(x) that encourages the LM to anno-
tate an example x = x1, . . . , xn with API calls. (
<a href=https://read.readwise.io/read/01gs564marcfgazs5b5j98hch2 rel=noopener>View Highlight</a>)</li><li>Model Finetuning After sampling and ﬁltering
calls for all APIs, we ﬁnally merge the remaining
API calls and interleave them with the original
inputs (
<a href=https://read.readwise.io/read/01gs565q74k49z4vr8xkm484jt rel=noopener>View Highlight</a>)</li><li>One such limi-
tation is the inability of Toolformer to use tools in a
chain (i.e., using the output of one tool as an input
for another tool). This is due to the fact that API
calls for each tool are generated independently; as a
consequence, there are no examples of chained tool
use in the ﬁnetuning dataset (
<a href=https://read.readwise.io/read/01gs567xjvjtxnzbkz1py92r28 rel=noopener>View Highlight</a>)</li><li>we found models
trained with Toolformer to often be sensitive to the
exact wording of their input when deciding whether
or not to call an API; this is perhaps unsurprising
given that LMs are known to be very sensitive to
the prompt they are provided with in both zero-
and few-shot settings (
<a href=https://read.readwise.io/read/01gs568afsvrq89wf0zcx1c268 rel=noopener>View Highlight</a>)</li></ul><hr><p>author: <a class="internal-link broken">arxiv.org</a>
title: &ldquo;Toolformer: LMs can teach themselves to use tools&rdquo;
tags:</p><ul><li>articles</li><li>literature-note</li></ul><hr><a href=#toolformer-lms-can-teach-themselves-to-use-tools-1><h1 id=toolformer-lms-can-teach-themselves-to-use-tools-1><span class=hanchor arialabel=Anchor># </span>Toolformer: LMs can teach themselves to use tools</h1></a><p><img src=https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png width=auto alt=rw-book-cover></p><a href=#metadata-1><h2 id=metadata-1><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">arxiv.org</a></li><li>Full Title: Toolformer: LMs can teach themselves to use tools</li><li>Category: #articles</li><li>URL:
<a href=https://arxiv.org/pdf/2302.04761v1.pdf rel=noopener>https://arxiv.org/pdf/2302.04761v1.pdf</a></li></ul><a href=#highlights-1><h2 id=highlights-1><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>Toolformer: Language Models Can Teach Themselves to Use Tools (
<a href=https://read.readwise.io/read/01gs56zjwr4x67myv3mm7x66k9 rel=noopener>View Highlight</a>)</li><li>we show that
LMs can teach themselves to use external tools
via simple APIs (
<a href=https://read.readwise.io/read/01gs55kwd9kr4vas1cdfwhs26p rel=noopener>View Highlight</a>)</li><li>Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. (
<a href=https://read.readwise.io/read/01gs55m3606zentxqprj1gp7b9 rel=noopener>View Highlight</a>)</li><li>. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities. (
<a href=https://read.readwise.io/read/01gs55mpwycfyp44pg4gjc83aj rel=noopener>View Highlight</a>)</li><li>, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciﬁc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), (
<a href=https://read.readwise.io/read/01gs55pa95z2zj3cpb168ekntk rel=noopener>View Highlight</a>)</li><li>limitations include an inability to access
up-to-date information on recent events (
<a href=https://read.readwise.io/read/01gs55n8qrc91eer80fc74w9sj rel=noopener>View Highlight</a>)</li><li>tendency to hallucinate
facts ( (
<a href=https://read.readwise.io/read/01gs55nb745dh6nn5gfg88k9pb rel=noopener>View Highlight</a>)</li><li>difﬁcul-
ties in understanding low-resource languages (
<a href=https://read.readwise.io/read/01gs55ndxva4tqsbw3e9qt4b5y rel=noopener>View Highlight</a>)</li><li>a lack of mathematical skills to per-
form precise calculations (
<a href=https://read.readwise.io/read/01gs55ngp7a4x9j123mfx7h8rt rel=noopener>View Highlight</a>)</li><li>) and an
unawareness of the progression of time (
<a href=https://read.readwise.io/read/01gs55nk10e7szqrk6ybnmzfbk rel=noopener>View Highlight</a>)</li><li>The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. (
<a href=https://read.readwise.io/read/01gs55qqchkt3x8ygg1xj5r9p4 rel=noopener>View Highlight</a>)</li><li>The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. (
<a href=https://read.readwise.io/read/01gs55qz0f2332adm14jdjtmq2 rel=noopener>View Highlight</a>)</li><li>Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call. (
<a href=https://read.readwise.io/read/01gs56404drd4dzfjjj903bs74 rel=noopener>View Highlight</a>)</li><li>using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (
<a href=https://read.readwise.io/read/01gs5627sh3h78gknv00rwshbc rel=noopener>View Highlight</a>)</li><li>Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
ﬁnetune the LM itself on the API calls that it con-
siders useful. (
<a href=https://read.readwise.io/read/01gs562vsakc7e6gm4tkvz0mh2 rel=noopener>View Highlight</a>)</li><li>As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself – for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus (
<a href=https://read.readwise.io/read/01gs5651fp2zbjy5s11ykxtfxy rel=noopener>View Highlight</a>)</li><li>For each API, we write a
prompt P(x) that encourages the LM to anno-
tate an example x = x1, . . . , xn with API calls. (
<a href=https://read.readwise.io/read/01gs564marcfgazs5b5j98hch2 rel=noopener>View Highlight</a>)</li><li>Model Finetuning After sampling and ﬁltering
calls for all APIs, we ﬁnally merge the remaining
API calls and interleave them with the original
inputs (
<a href=https://read.readwise.io/read/01gs565q74k49z4vr8xkm484jt rel=noopener>View Highlight</a>)</li><li>One such limi-
tation is the inability of Toolformer to use tools in a
chain (i.e., using the output of one tool as an input
for another tool). This is due to the fact that API
calls for each tool are generated independently; as a
consequence, there are no examples of chained tool
use in the ﬁnetuning dataset (
<a href=https://read.readwise.io/read/01gs567xjvjtxnzbkz1py92r28 rel=noopener>View Highlight</a>)</li><li>we found models
trained with Toolformer to often be sensitive to the
exact wording of their input when deciding whether
or not to call an API; this is perhaps unsurprising
given that LMs are known to be very sensitive to
the prompt they are provided with in both zero-
and few-shot settings (
<a href=https://read.readwise.io/read/01gs568afsvrq89wf0zcx1c268 rel=noopener>View Highlight</a>)</li></ul><hr><p>author: <a class="internal-link broken">arxiv.org</a>
title: &ldquo;Toolformer: LMs can teach themselves to use tools&rdquo;
tags:</p><ul><li>articles</li><li>literature-note</li></ul><hr><a href=#toolformer-lms-can-teach-themselves-to-use-tools-2><h1 id=toolformer-lms-can-teach-themselves-to-use-tools-2><span class=hanchor arialabel=Anchor># </span>Toolformer: LMs can teach themselves to use tools</h1></a><p><img src=https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png width=auto alt=rw-book-cover></p><a href=#metadata-2><h2 id=metadata-2><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">arxiv.org</a></li><li>Full Title: Toolformer: LMs can teach themselves to use tools</li><li>Category: #articles</li><li>URL:
<a href=https://arxiv.org/pdf/2302.04761v1.pdf rel=noopener>https://arxiv.org/pdf/2302.04761v1.pdf</a></li></ul><a href=#highlights-2><h2 id=highlights-2><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>Toolformer: Language Models Can Teach Themselves to Use Tools (
<a href=https://read.readwise.io/read/01gs56zjwr4x67myv3mm7x66k9 rel=noopener>View Highlight</a>)</li><li>we show that
LMs can teach themselves to use external tools
via simple APIs (
<a href=https://read.readwise.io/read/01gs55kwd9kr4vas1cdfwhs26p rel=noopener>View Highlight</a>)</li><li>Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. (
<a href=https://read.readwise.io/read/01gs55m3606zentxqprj1gp7b9 rel=noopener>View Highlight</a>)</li><li>. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities. (
<a href=https://read.readwise.io/read/01gs55mpwycfyp44pg4gjc83aj rel=noopener>View Highlight</a>)</li><li>, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciﬁc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), (
<a href=https://read.readwise.io/read/01gs55pa95z2zj3cpb168ekntk rel=noopener>View Highlight</a>)</li><li>limitations include an inability to access
up-to-date information on recent events (
<a href=https://read.readwise.io/read/01gs55n8qrc91eer80fc74w9sj rel=noopener>View Highlight</a>)</li><li>tendency to hallucinate
facts ( (
<a href=https://read.readwise.io/read/01gs55nb745dh6nn5gfg88k9pb rel=noopener>View Highlight</a>)</li><li>difﬁcul-
ties in understanding low-resource languages (
<a href=https://read.readwise.io/read/01gs55ndxva4tqsbw3e9qt4b5y rel=noopener>View Highlight</a>)</li><li>a lack of mathematical skills to per-
form precise calculations (
<a href=https://read.readwise.io/read/01gs55ngp7a4x9j123mfx7h8rt rel=noopener>View Highlight</a>)</li><li>) and an
unawareness of the progression of time (
<a href=https://read.readwise.io/read/01gs55nk10e7szqrk6ybnmzfbk rel=noopener>View Highlight</a>)</li><li>The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. (
<a href=https://read.readwise.io/read/01gs55qqchkt3x8ygg1xj5r9p4 rel=noopener>View Highlight</a>)</li><li>The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. (
<a href=https://read.readwise.io/read/01gs55qz0f2332adm14jdjtmq2 rel=noopener>View Highlight</a>)</li><li>Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call. (
<a href=https://read.readwise.io/read/01gs56404drd4dzfjjj903bs74 rel=noopener>View Highlight</a>)</li><li>using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (
<a href=https://read.readwise.io/read/01gs5627sh3h78gknv00rwshbc rel=noopener>View Highlight</a>)</li><li>Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
ﬁnetune the LM itself on the API calls that it con-
siders useful. (
<a href=https://read.readwise.io/read/01gs562vsakc7e6gm4tkvz0mh2 rel=noopener>View Highlight</a>)</li><li>As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself – for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus (
<a href=https://read.readwise.io/read/01gs5651fp2zbjy5s11ykxtfxy rel=noopener>View Highlight</a>)</li><li>For each API, we write a
prompt P(x) that encourages the LM to anno-
tate an example x = x1, . . . , xn with API calls. (
<a href=https://read.readwise.io/read/01gs564marcfgazs5b5j98hch2 rel=noopener>View Highlight</a>)</li><li>Model Finetuning After sampling and ﬁltering
calls for all APIs, we ﬁnally merge the remaining
API calls and interleave them with the original
inputs (
<a href=https://read.readwise.io/read/01gs565q74k49z4vr8xkm484jt rel=noopener>View Highlight</a>)</li><li>One such limi-
tation is the inability of Toolformer to use tools in a
chain (i.e., using the output of one tool as an input
for another tool). This is due to the fact that API
calls for each tool are generated independently; as a
consequence, there are no examples of chained tool
use in the ﬁnetuning dataset (
<a href=https://read.readwise.io/read/01gs567xjvjtxnzbkz1py92r28 rel=noopener>View Highlight</a>)</li><li>we found models
trained with Toolformer to often be sensitive to the
exact wording of their input when deciding whether
or not to call an API; this is perhaps unsurprising
given that LMs are known to be very sensitive to
the prompt they are provided with in both zero-
and few-shot settings (
<a href=https://read.readwise.io/read/01gs568afsvrq89wf0zcx1c268 rel=noopener>View Highlight</a>)</li></ul><hr><p>author: <a class="internal-link broken">arxiv.org</a>
title: &ldquo;Toolformer: LMs can teach themselves to use tools&rdquo;
tags:</p><ul><li>articles</li><li>literature-note</li></ul><hr><a href=#toolformer-lms-can-teach-themselves-to-use-tools-3><h1 id=toolformer-lms-can-teach-themselves-to-use-tools-3><span class=hanchor arialabel=Anchor># </span>Toolformer: LMs can teach themselves to use tools</h1></a><p><img src=https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png width=auto alt=rw-book-cover></p><a href=#metadata-3><h2 id=metadata-3><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">arxiv.org</a></li><li>Full Title: Toolformer: LMs can teach themselves to use tools</li><li>Category: #articles</li><li>URL:
<a href=https://arxiv.org/pdf/2302.04761v1.pdf rel=noopener>https://arxiv.org/pdf/2302.04761v1.pdf</a></li></ul><a href=#highlights-3><h2 id=highlights-3><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>Toolformer: Language Models Can Teach Themselves to Use Tools (
<a href=https://read.readwise.io/read/01gs56zjwr4x67myv3mm7x66k9 rel=noopener>View Highlight</a>)</li><li>we show that
LMs can teach themselves to use external tools
via simple APIs (
<a href=https://read.readwise.io/read/01gs55kwd9kr4vas1cdfwhs26p rel=noopener>View Highlight</a>)</li><li>Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. (
<a href=https://read.readwise.io/read/01gs55m3606zentxqprj1gp7b9 rel=noopener>View Highlight</a>)</li><li>. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities. (
<a href=https://read.readwise.io/read/01gs55mpwycfyp44pg4gjc83aj rel=noopener>View Highlight</a>)</li><li>, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciﬁc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), (
<a href=https://read.readwise.io/read/01gs55pa95z2zj3cpb168ekntk rel=noopener>View Highlight</a>)</li><li>limitations include an inability to access
up-to-date information on recent events (
<a href=https://read.readwise.io/read/01gs55n8qrc91eer80fc74w9sj rel=noopener>View Highlight</a>)</li><li>tendency to hallucinate
facts ( (
<a href=https://read.readwise.io/read/01gs55nb745dh6nn5gfg88k9pb rel=noopener>View Highlight</a>)</li><li>difﬁcul-
ties in understanding low-resource languages (
<a href=https://read.readwise.io/read/01gs55ndxva4tqsbw3e9qt4b5y rel=noopener>View Highlight</a>)</li><li>a lack of mathematical skills to per-
form precise calculations (
<a href=https://read.readwise.io/read/01gs55ngp7a4x9j123mfx7h8rt rel=noopener>View Highlight</a>)</li><li>) and an
unawareness of the progression of time (
<a href=https://read.readwise.io/read/01gs55nk10e7szqrk6ybnmzfbk rel=noopener>View Highlight</a>)</li><li>The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. (
<a href=https://read.readwise.io/read/01gs55qqchkt3x8ygg1xj5r9p4 rel=noopener>View Highlight</a>)</li><li>The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. (
<a href=https://read.readwise.io/read/01gs55qz0f2332adm14jdjtmq2 rel=noopener>View Highlight</a>)</li><li>Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call. (
<a href=https://read.readwise.io/read/01gs56404drd4dzfjjj903bs74 rel=noopener>View Highlight</a>)</li><li>using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (
<a href=https://read.readwise.io/read/01gs5627sh3h78gknv00rwshbc rel=noopener>View Highlight</a>)</li><li>Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
ﬁnetune the LM itself on the API calls that it con-
siders useful. (
<a href=https://read.readwise.io/read/01gs562vsakc7e6gm4tkvz0mh2 rel=noopener>View Highlight</a>)</li><li>As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself – for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus (
<a href=https://read.readwise.io/read/01gs5651fp2zbjy5s11ykxtfxy rel=noopener>View Highlight</a>)</li><li>For each API, we write a
prompt P(x) that encourages the LM to anno-
tate an example x = x1, . . . , xn with API calls. (
<a href=https://read.readwise.io/read/01gs564marcfgazs5b5j98hch2 rel=noopener>View Highlight</a>)</li><li>Model Finetuning After sampling and ﬁltering
calls for all APIs, we ﬁnally merge the remaining
API calls and interleave them with the original
inputs (
<a href=https://read.readwise.io/read/01gs565q74k49z4vr8xkm484jt rel=noopener>View Highlight</a>)</li><li>One such limi-
tation is the inability of Toolformer to use tools in a
chain (i.e., using the output of one tool as an input
for another tool). This is due to the fact that API
calls for each tool are generated independently; as a
consequence, there are no examples of chained tool
use in the ﬁnetuning dataset (
<a href=https://read.readwise.io/read/01gs567xjvjtxnzbkz1py92r28 rel=noopener>View Highlight</a>)</li><li>we found models
trained with Toolformer to often be sensitive to the
exact wording of their input when deciding whether
or not to call an API; this is perhaps unsurprising
given that LMs are known to be very sensitive to
the prompt they are provided with in both zero-
and few-shot settings (
<a href=https://read.readwise.io/read/01gs568afsvrq89wf0zcx1c268 rel=noopener>View Highlight</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pelayoarbues.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Pelayo Arbués using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pelayoarbues.github.io/>Home</a></li><li><a href=https://twitter.com/pelayoarbues>Twitter</a></li><li><a href=https://sigmoid.social/@pelayoarbues>Mastodon</a></li><li><a href=https://www.linkedin.com/in/pelayoarbues/>Linkedin</a></li><li><a href="https://scholar.google.com/citations?user=WaC-GcIAAAAJ&hl">Scholar</a></li><li><a href=https://www.flickr.com/photos/wonderfulhorriblelife/>Flickr</a></li><li><a href=https://github.com/pelayoarbues>GitHub</a></li></ul></footer></div></div></body></html>
<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="AI Safety and the Age of Dislightenment  Metadata  Author: [[Jeremy Howard]] Full Title: AI Safety and the Age of Dislightenment Category: #articles URL: https://www."><title>AI Safety and the Age of Dislightenment</title><meta name=viewport content="width=device-width,initial-scale=1"><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjFhYTEyN2VjLWI0MTEtNDdjNS1iNWQzLTA5OGE2NDZjMWZhYSIsICJpZCI6ICI5YzFhNzE4Zi0xNjA1LTRmMTUtOGQ3Yy05NzliMDBjNWVmNTcifQ.-aCZYTIrTPiQCDYC_zS0dH0IVOXqI9ThnK39DMDgY7c></script><link rel="shortcut icon" type=image/png href=https://pelayoarbues.github.io//icon.png><link href=https://pelayoarbues.github.io/styles.23ddc3ecf7e887d89acd50af3b520de2.min.css rel=stylesheet><link href=https://pelayoarbues.github.io/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css rel=stylesheet id=theme-link><script src=https://pelayoarbues.github.io/js/darkmode.d1ab992b86a0866f970f82ef4d95b010.min.js></script>
<script src=https://pelayoarbues.github.io/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://pelayoarbues.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script>const isReducedMotion=window.matchMedia("(prefers-reduced-motion: reduce)").matches,lastVisit=localStorage.getItem("lastVisitTime"),now=Date.now();let show="true";if(lastVisit){document.documentElement.setAttribute("visited","true");const e=Math.ceil((now-parseInt(lastVisit))/(1e3*60));show=!isReducedMotion&&e>5?"true":"false"}document.documentElement.setAttribute("show-animation",show),localStorage.setItem("lastVisitTime",`${now}`);const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pelayoarbues.github.io/",fetchData=Promise.all([fetch("https://pelayoarbues.github.io/indices/linkIndex.59a8b556c7634126ff479246acf22b73.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pelayoarbues.github.io/indices/contentIndex.4a0f436428a27091e62a6408f92a479e.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://pelayoarbues.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pelayoarbues.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:.8,opacityScale:3,repelForce:2,scale:1})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pelayoarbues.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pelayoarbues.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pelayoarbues.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div id=cursor-chat-layer><input type=text id=cursor-chat-box></div><script type=module>
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script><div class=singlePage><header class="delay t-3"><h1 id=page-title><a class=root-title href=https://pelayoarbues.github.io/>Pelayo Arbués</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>AI Safety and the Age of Dislightenment</h1><p class=meta>Last updated
Jul 13, 2023</p><ul class=tags><li><a href=https://pelayoarbues.github.io/tags/articles/>Articles</a></li><li><a href=https://pelayoarbues.github.io/tags/literature-note/>Literature Note</a></li></ul><a href=#ai-safety-and-the-age-of-dislightenment><h1 id=ai-safety-and-the-age-of-dislightenment><span class=hanchor arialabel=Anchor># </span>AI Safety and the Age of Dislightenment</h1></a><p><img src=https://www.fast.ai/images/enlightenment.jpeg width=auto alt=rw-book-cover></p><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">Jeremy Howard</a></li><li>Full Title: AI Safety and the Age of Dislightenment</li><li>Category: #articles</li><li>URL:
<a href=https://www.fast.ai/posts/2023-11-07-dislightenment.html rel=noopener>https://www.fast.ai/posts/2023-11-07-dislightenment.html</a></li></ul><a href=#highlights><h2 id=highlights><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>society and empowering society to defend itself is delicate. We should advocate for openness, humility and broad consultation to develop better responses aligned with our principles and values — (
<a href=https://read.readwise.io/read/01h5328rkan427z5xcgd9974aj rel=noopener>View Highlight</a>)</li><li>Other experts, however,
<a href=https://venturebeat.com/ai/ai-experts-challenge-doomer-narrative-including-extinction-risk-claims/ rel=noopener>counter that</a> “There is so much attention flooded onto x-risk (existential risk)… that it ‘takes the air out of more pressing issues’ and insidiously puts social pressure on researchers focused on other current risks.” (
<a href=https://read.readwise.io/read/01h5329ww36csqrkc46n9pcg3n rel=noopener>View Highlight</a>)</li><li>if AI turns out to be powerful enough to be a catastrophic threat, the proposal may not actually help. In fact it could make things much worse, by creating a power imbalance so severe that it leads to the destruction of society. These concerns apply to all regulations that try to ensure the models themselves (“development”) are safe, rather than just how they’re used. The effects of these regulations may turn out to be impossible to undo, and therefore we should be extremely careful before we legislate them. (
<a href=https://read.readwise.io/read/01h532atz921mxkh8ktv3ekych rel=noopener>View Highlight</a>)</li><li>the only way to ensure that AI models can’t be misused is to ensure that no one can use them directly. Instead, they must be limited to a tightly controlled narrow service interface (like ChatGPT, an interface to GPT-4) (
<a href=https://read.readwise.io/read/01h532bqemmphqxqazp3cdn45z rel=noopener>View Highlight</a>)</li><li>But those with full access to AI models (such as those inside the companies that host the service) have enormous advantages over those limited to “safe” interfaces. If AI becomes extremely powerful, then full access to models will be critical to those who need to remain competitive, as well as to those who wish to cause harm (
<a href=https://read.readwise.io/read/01h532c1nfc6203dnpnxcfeqpr rel=noopener>View Highlight</a>)</li><li>This could lead to a society where only groups with the massive resources to train foundation models, or the moral disregard to steal them, have access to humanity’s most powerful technology. These groups could become more powerful than any state. Historically, large power differentials have led to violence and subservience of whole societies. (
<a href=https://read.readwise.io/read/01h532chkmhn4tv2yxdn4hx5r0 rel=noopener>View Highlight</a>)</li><li>If we regulate now in a way that increases centralisation of power in the name of “safety”, we risk rolling back the gains made from the Age of Enlightenment, and instead entering a new age: the Age of Dislightenment. Instead, we could maintain the Enlightenment ideas of openness and trust, such as by supporting open-source model development. Open source has enabled huge technological progress through broad participation and sharing. Perhaps open AI models could do the same. Broad participation could allow more people with a wider variety of expertise to help identify and counter threats, thus increasing overall safety — as we’ve previously seen in fields like cyber-security. (
<a href=https://read.readwise.io/read/01h532dfjyzk1nadcyb5ttf46f rel=noopener>View Highlight</a>)</li><li>By regulating <em>applications</em> we focus on real harms and can make those most responsible directly liable. Another useful approach in the AI Act is to regulate <em>disclosure</em>, to ensure that those using models have the information they need to use them appropriately (
<a href=https://read.readwise.io/read/01h532dywbptw0mxc6gnekn62d rel=noopener>View Highlight</a>)</li><li>The rapid development of increasingly capable AI has many people asking to be protected, and many offering that protection. The latest is a white paper titled: “
<a href="https://docs.google.com/document/d/15RJSesDTf0inWHVJhkaS7WFyTsPkWy87i2-jhig4xfc/edit?usp=sharing" rel=noopener>Frontier AI Regulation: Managing Emerging Risks to Public Safety’’</a> (FAR). Many authors of the paper are connected to OpenAI and Google, and to various organizations funded by investors of OpenAI and Google. FAR claims that “government involvement will be required to ensure that such ‘frontier AI models’ are harnessed in the public interest”. But can we really ensure such a thing? At what cost? (
<a href=https://read.readwise.io/read/01h532fqb1e235knvdt9f8cvkw rel=noopener>View Highlight</a>)</li><li>While superficially seeming to check off various safety boxes, the regulatory regime being advanced in FAR ultimately leads to a vast amount of power being placed into the entrenched companies (by virtue of them having access to the raw models), giving them an information asymmetry against all other actors - including governments seeking to regulate or constrain them. It may lead to the destruction of society. (
<a href=https://read.readwise.io/read/01h532gb91waz22x0zbq9gvzdb rel=noopener>View Highlight</a>)</li><li>Here’s why: because these models are general-purpose computing devices, it is impossible to guarantee they can’t be used for harmful applications. That would be like trying to make a computer that can’t be misused (such as for emailing a blackmail threat). The full original model is vastly more powerful than any “ensured safe” service based on it can ever be. The full original model is general-purpose: it can be used for anything. But if you give someone a general-purpose computing device, you can’t be sure they won’t use it to cause harm. (
<a href=https://read.readwise.io/read/01h532gteaf1dzphacjw7qj5m6 rel=noopener>View Highlight</a>)</li><li>you give them access to a service which provides a small window into the full model. For instance, OpenAI provides public access to a tightly controlled and tuned text-based conversational interface to GPT-4, but does not provide full access to the GPT-4 model itself. (
<a href=https://read.readwise.io/read/01h532h4txntqq8pqr9c4w4mcr rel=noopener>View Highlight</a>)</li><li>Those who crave power and wealth, but fail to get access to model weights, now have a new goal: get themselves into positions of power at organizations that have big models, or get themselves into positions of power at the government departments that make these decisions. Organizations that started out as well-meaning attempts to develop AI for societal benefit will soon find themselves part of the corporate profit-chasing machinery that all companies join as they grow, run by people that are experts at chasing profits. (
<a href=https://read.readwise.io/read/01h532swgg4e53q1qgf0yp87aj rel=noopener>View Highlight</a>)</li><li>The truth is that this entire endeavor, this attempt to control the use of AI, is pointless and ineffective. Not only is “proliferation” of models impossible to control (because digital information is so easy to exfiltrate and copy), it turns out that restrictions on the amount of compute for training models are also impossible to enforce. That’s because it’s now possible for people all over the world to virtually join up and train a model together. For instance,
<a href=https://www.together.xyz/ rel=noopener>Together Computer</a> has created a fully decentralized, open, scalable cloud for AI, and
<a href=https://arxiv.org/abs/2206.01288 rel=noopener>recent research has shown</a> it is possible to go a long way with this kind of approach. (
<a href=https://read.readwise.io/read/01h532tn75a8ty86qep3jtnmhy rel=noopener>View Highlight</a>)</li><li>There is more compute capacity in the world currently deployed for playing games than for AI. Gamers around the world can simply install a small piece of software on their computers to opt into helping train these open-source models. Organizing such a large-scale campaign would be difficult, but not without precedent, as seen in the success of projects such as
<a href="https://foldingathome.org/?lng=en-US" rel=noopener>Folding@Home</a> and
<a href=https://setiathome.berkeley.edu/ rel=noopener>SETI@Home</a> (
<a href=https://read.readwise.io/read/01h532zj6rnatk4hmfnndff1vf rel=noopener>View Highlight</a>)</li><li>in a recent interview with Lex Fridman, Comma.ai founder George Hotz explained how his new company, Tiny Corp, is working on the “Tiny Rack”, which he explains is powered based on the premise: “What’s the most power you can get into your house without arousing suspicion? And one of the answers is an electric car charger.” So he’s building an AI model training system that uses the same amount of power as a car charger (
<a href=https://read.readwise.io/read/01h5330yh9c44zd4w200xc7409 rel=noopener>View Highlight</a>)</li><li>When the self-described pioneer of the AI Safety movement, Eliezer Yudkowsky, proposed airstrikes on unauthorized data centers and the threat of nuclear war to ensure compliance from states failing to control unauthorized use of computation capability, many were shocked. But bombing data centers and global surveillance of all computers is the only way to ensure the kind of safety compliance that FAR proposes.
<a href=https://www.fast.ai/posts/2023-11-07-dislightenment.html#fn5 rel=noopener>5</a> (
<a href=https://read.readwise.io/read/01h5332qwgvt6r46btj3tckamc rel=noopener>View Highlight</a>)</li><li>Alex Engler points out
<a href=https://www.brookings.edu/articles/the-eus-attempt-to-regulate-open-source-ai-is-counterproductive/ rel=noopener>an alternative approach</a> to enforced safety standards or licensing of models, which is to “regulate risky and harmful <em>applications</em>, not open-source AI <em>models’</em>’. This is how most regulations work: through <em>liability</em>. If someone does something bad, then they get in trouble. If someone creates a general-purpose tool that someone else uses to do something bad, the tool-maker doesn’t get in trouble (
<a href=https://read.readwise.io/read/01h5333rkgqjwv2y1yvssre13p rel=noopener>View Highlight</a>)</li><li>This is a critical distinction: the distinction between regulating <em>usage</em> (that is, actually putting a model into use by making it part of a system — especially a high risk system like medicine), vs <em>development</em> (that is, the process of training the model). (
<a href=https://read.readwise.io/read/01h5333zt89061gny4w9fc6kvs rel=noopener>View Highlight</a>)</li><li>Improvements in AI capabilities can be unpredictable, and are often difficult to fully understand without intensive testing. Regulation that does not require models to go through sufficient testing before deployment may therefore fail to reliably prevent deployed models from posing severe risks.” This is a non-sequitur. Because models <em>cannot</em> cause harm without being used, developing a model cannot be a harmful activity.
<a href=https://www.fast.ai/posts/2023-11-07-dislightenment.html#fn6 rel=noopener>6</a> Furthermore (
<a href=https://read.readwise.io/read/01h5335e1cek9gqwc18a26pww5 rel=noopener>View Highlight</a>)</li><li>This leads us to another useful regulatory path: deployment disclosure. If you’re considering connecting an automated system which uses AI to any kind of sensitive infrastructure, then we should require disclosure of this fact. Furthermore, certain types of connection and infrastructure should require careful safety checks and auditing in advance. (
<a href=https://read.readwise.io/read/01h5336tp3tmdsgbas40y4975r rel=noopener>View Highlight</a>)</li><li>Better AI can be used to improve AI. This has already been seen many times, even in the earlier era of less capable and well-resourced algorithms. Google has used AI to improve how data centers use energy, to create better neural network architectures, and to create better methods for optimizing the parameters in those networks. Model outputs have been used to create the prompts used to train new models, and to create the model answers for these prompts, and to explain the reasoning for answers. (
<a href=https://read.readwise.io/read/01h53378xdbjxs3g4w76y6qht9 rel=noopener>View Highlight</a>)</li><li>Those with access to the full models can build new models faster and better than those without. One reason is that they can fully utilize powerful features like fine-tuning, activations, and the ability to directly study and modify weights.
<a href=https://www.fast.ai/posts/2023-11-07-dislightenment.html#fn7 rel=noopener>7</a> One
<a href=https://arxiv.org/abs/2305.02301 rel=noopener>recent paper</a>, for instance, found that fine-tuning allows models to solve challenging problems with orders of magnitude fewer parameters than foundation models. (
<a href=https://read.readwise.io/read/01h533828mgjcsrk5ys9er0xtd rel=noopener>View Highlight</a>)</li><li>This kind of <em>feedback loop</em> results in centralization: the big companies get bigger, and other players can’t compete. This results in <em>centralization</em>, less competition, and as a result higher prices, less innovation, and lower safety (since there’s a single point of failure, and a larger profit motive which encourages risky behavior). (
<a href=https://read.readwise.io/read/01h5338k92g0jty6sjy6y64zqe rel=noopener>View Highlight</a>)</li><li>There are other powerful forces towards centralization. Consider Google, for instance. Google has more data than anyone else on the planet. More data leads directly to better foundation models. Furthermore, as people use their AI services, they are getting more and more data about these interactions. They use AI to improve their products, making them more “sticky” for their users and encouraging more people to use them, resulting in them getting still more data, which improves their models and products based on them further. Also, they are increasingly vertically integrated, so they have few powerful suppliers. They create their own AI chips (TPUs), run their own data centers, and develop their own software. (
<a href=https://read.readwise.io/read/01h533933scz0bpxf4hh29hnqv rel=noopener>View Highlight</a>)</li><li>The alternative to craving the safety and certainty of control and centralization is to once again take the risk we took hundreds of years ago: the risk of believing in the power and good of humanity and society. Just as thinkers of the Enlightenment asked difficult questions like “What if everyone got an education? What if everyone got the vote?”, we should ask the question “What if everyone got access to the full power of AI?” (
<a href=https://read.readwise.io/read/01h533a2qs53bx8xzvv2mtsmjs rel=noopener>View Highlight</a>)</li><li>To be clear: asking such questions may not be popular. The counter-enlightenment was a powerful movement for a hundred years,
<a href=https://en.wikipedia.org/wiki/Counter-Enlightenment rel=noopener>pushing back</a> against “the belief in progress, the rationality of all humans, liberal democracy, and the increasing secularization of society”. It relied on a key assumption, as expounded by French philosopher Joseph de Maistre, that “Man in general, if reduced to himself, is too wicked to be free.” (
<a href=https://read.readwise.io/read/01h533ag1b2m7z0x3wzyk599qj rel=noopener>View Highlight</a>)</li><li>What does it look like to embrace the belief in progress and the rationality of all humans when we respond to the threat of AI mis-use? One idea which many experts are now studying is that <em>open source</em> models may be the key. (
<a href=https://read.readwise.io/read/01h533d4qsmdhj053d1bm3pfva rel=noopener>View Highlight</a>)</li><li>What if the most powerful AI models were open source? There will still be Bad Guys looking to use them to hurt others or unjustly enrich themselves. But most people are not Bad Guys. Most people will use these models to create, and to protect. How better to be safe than to have the massive diversity and expertise of human society at large doing their best to identify and respond to threats, with the full power of AI behind them? How much safer would you feel if the world’s top cyber-security, bio-weapons, and social engineering academics were working with the benefits of AI to study AI safety, and that you could access and use all of their work yourself, compared to if only a handful of people at a for-profit company had full access to AI models? (
<a href=https://read.readwise.io/read/01h533dvk8q53g5ph23jw7mha7 rel=noopener>View Highlight</a>)</li><li>In order to gain access to the better features of full model access, and reduce the level of commercial control of what has previously been an open research community with a culture of sharing, the open-source community has recently stepped in and trained a number of quite capable language models. As of July 2023, the best of these are at a similar level to the second-tier cheaper commercial models, but not as good as GPT-4 or Claude. They are rapidly increasing in capability, and are attracting increasing investment from wealthy donors, governments, universities, and companies that are seeking to avoid concentration of power and ensure access to high quality AI models. (
<a href=https://read.readwise.io/read/01h533em9khxcfsbd714q1pmbp rel=noopener>View Highlight</a>)</li><li>In order to gain access to the better features of full model access, and reduce the level of commercial control of what has previously been an open research community with a culture of sharing, the open-source community has recently stepped in and trained a number of quite capable language models. As of July 2023, the best of these are at a similar level to the second-tier cheaper commercial models, but not as good as GPT-4 or Claude. They are rapidly increasing in capability, and are attracting increasing investment from wealthy donors, governments, universities, and companies that are seeking to avoid concentration of power and ensure access to high quality AI models. (
<a href=https://read.readwise.io/read/01h533ema4hxf2mme6p6hnprqv rel=noopener>View Highlight</a>)</li><li>In order to gain access to the better features of full model access, and reduce the level of commercial control of what has previously been an open research community with a culture of sharing, the open-source community has recently stepped in and trained a number of quite capable language models. As of July 2023, the best of these are at a similar level to the second-tier cheaper commercial models, but not as good as GPT-4 or Claude. They are rapidly increasing in capability, and are attracting increasing investment from wealthy donors, governments, universities, and companies that are seeking to avoid concentration of power and ensure access to high quality AI models. (
<a href=https://read.readwise.io/read/01h533emez6c2k48jsw3yn26nw rel=noopener>View Highlight</a>)</li><li>However, the proposals for safety guarantees in FAR are incompatible with open source frontier models. FAR proposes “it may be prudent to avoid potentially dangerous capabilities of frontier AI models being open sourced until safe deployment is demonstrably feasible”. But even if an open-source model is trained in the exact same way from the exact same data as a regulatorily-approved closed commercial model, it can still never provide the same safety guarantees. That’s because, as a general-purpose computing device, anybody could use it for anything they want — including fine-tuning it using new datasets and for new tasks. (
<a href=https://read.readwise.io/read/01h533fazej1j18z41v9vh34cd rel=noopener>View Highlight</a>)</li><li>“<em>For foundation models to advance the public interest, their development and deployment should ensure transparency, support innovation, distribute power, and minimize harm… We argue open-source foundation models can achieve all four of these objectives, in part due to inherent merits of open-source (pro-transparency, pro-innovation, anti-concentration)</em>” (
<a href=https://read.readwise.io/read/01h533fvrkzcf1fd3wvdf220k2 rel=noopener>View Highlight</a>)</li><li>“<em>If closed-source models cannot be examined by researchers and technologists, security vulnerabilities might not be identified before they cause harm… On the other hand, experts across domains can examine and analyze open-source models, which makes security vulnerabilities easier to find and address. In addition, restricting who can create FMs would reduce the diversity of capable FMs and may result in single points of failure in complex systems.</em>” (
<a href=https://read.readwise.io/read/01h533g559h9bw2nwv2f6e08nq rel=noopener>View Highlight</a>)</li><li>Access to open source models is at grave risk today. The European AI Act may effectively ban open source foundation models, based on similar principles to those in FAR. (
<a href=https://read.readwise.io/read/01h533rf46jpr35gn2s9pavcqr rel=noopener>View Highlight</a>)</li><li>‘<em>The fears around new technologies follow a predictable trajectory called “the Tech Panic Cycle.” Fears increase, peak, then decline over time as the public becomes familiar with the technology and its benefits. Indeed, other previous “generative” technologies in the creative sector such as the printing press, the phonograph, and the Cinématographe followed this same course. But unlike today, policymakers were unlikely to do much to regulate and restrict these technologies. As the panic over generative AI enters its most volatile stage, policymakers should take a deep breath, recognize the predictable cycle we are in, and put any regulation efforts directly aimed at generative AI temporarily on hold.</em>’ (
<a href=https://read.readwise.io/read/01h533seh9xag9j91x4mr7rjyn rel=noopener>View Highlight</a>)</li><li>Instead, perhaps regulators should consider the medical guidance of Hippocrates: “<em>do no harm</em>”. Medical interventions can have side effects, and the cure can sometimes be worse than the disease. Some medicines may even damage immune response, leaving a body too weakened to be able to fight off infection. (
<a href=https://read.readwise.io/read/01h533t0senrfz87yg4y8q905b rel=noopener>View Highlight</a>)</li><li>So too with regulatory interventions. Not only can the centralisation and regulatory capture impacts of “ensuring safety” cause direct harm to society, but they can even result in decreased safety. If just one big organization holds the keys to vast technological power, we find ourselves in a fragile situation where the rest of society does not have access to the same power to protect ourselves (
<a href=https://read.readwise.io/read/01h533tdv09wdh1vfzpz385sjh rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533y8g07xdx14byhy24ne5c rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533y8gm94a5ed7j26fahrv7 rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533y8sbgaj1dmyh0wgzwb7z rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533y8xwbfb9bm3syww7q1d4 rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533yahmkrwc0c8ndsajq4hc rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” was written by 26 authors from 14 institutions, spanning academia, civil society, and industry. The lead author is today the Head of Policy at OpenAI. It’s interesting to see how far OpenAI, as co-creators of FAR, has moved from these original ideas. The four recommendations from the Malicious Use paper are full of humility — they recognise that effective responses to risks involve “proactively reaching out to relevant actors”, learning from “research areas with more mature methods for addressing dual-use concerns, such as computer security”, and “expand the range of stakeholders and domain experts involved in discussions”. The focus was not in centralization and control, but outreach and cooperation. (
<a href=https://read.readwise.io/read/01h533ybtpgtspb6g62djr5hyn rel=noopener>View Highlight</a>)</li><li>The ancient Greeks taught us about the dangers of Hubris: excessive pride, arrogance, or overconfidence. When we are over-confident that we <em>know</em> what the future has in store for us, we may well over-react and create the very future we try to avoid. What if, in our attempts to avoid an AI apocalypse, we centralize control of the world’s most powerful technology, dooming future society to a return to a feudal state in which the most valuable commodity, compute, is owned by an elite few. We would be like King Oedipus, prophesied to kill his father and marry his mother, who ends up doing exactly that as a result of actions designed to avoid that fate. Or Phaethon, so confident in his ability to control the chariot of the sun that he avoids the middle path laid out by Helios, his father, and in the process nearly destroys Earth. (
<a href=https://read.readwise.io/read/01h533zhxvk1dwprkf44fxmfjd rel=noopener>View Highlight</a>)</li><li>“The Malicious Use of Artificial Intelligence” points towards a different approach, based on humility: one of consultation with experts across many fields, cooperation with those impacted by technology, in an iterative process that learns from experience. (
<a href=https://read.readwise.io/read/01h533zzkh93x4fzfvafybhav4 rel=noopener>View Highlight</a>)</li><li>The AI community has also developed effective mechanisms for sharing important information, such as
<a href=https://cacm.acm.org/magazines/2021/12/256932-datasheets-for-datasets/abstract rel=noopener>Datasheets for Datasets</a>,
<a href=https://arxiv.org/abs/1810.03993 rel=noopener>Model Cards for Model Reporting</a>, and
<a href=https://crfm.stanford.edu/ecosystem-graphs/ rel=noopener>Ecosystem Graphs</a>. Regulation could require that datasets and models include information about how they were built or trained, to help users deploy them more effectively and safely. This is analogous to nutrition labels: whilst we don’t ban people from eating too much junk food, we endeavor to give them the information they need to make good choices. The proposed EU AI Act already includes requirements for exactly this kind of information. (
<a href=https://read.readwise.io/read/01h5348k8asrfqh1jk9fb6fq86 rel=noopener>View Highlight</a>)</li><li>Whilst there is a lot of good work we can build on, there’s still much more to be done. The world of AI is moving fast, and we’re learning every day. Therefore, it’s important that we ensure the choices we make preserve <em>optionality</em> in the future. It’s far too early for us to pick a single path and decide to hurtle down it with unstoppable momentum. Instead, we need to be able, as a society, to respond rapidly and in an informed way to new opportunities and threats as they arise. That means involving a broad cross-section of experts from all relevant domains, along with members of impacted communities. (
<a href=https://read.readwise.io/read/01h53492rrnbynrxhqp0ge6wdc rel=noopener>View Highlight</a>)</li><li>The more we can build capacity in our policy making bodies, the better. Without a deep understanding of AI amongst decision makers, they have little choice but to defer to industry. But as Marietje Schaake, international policy director at Stanford University’s Cyber Policy Center, said, “
<a href=https://www.ft.com/content/5f8b74f7-68b1-4a6c-88bf-d0dd03579149 rel=noopener>We need to keep CEOs away from AI regulation</a>”: (
<a href=https://read.readwise.io/read/01h534agcc20fm64khhdcx4k0k rel=noopener>View Highlight</a>)</li><li>“<em>Imagine the chief executive of JPMorgan explaining to Congress that because financial products are too complex for lawmakers to understand, banks should decide for themselves how to prevent money laundering, enable fraud detection and set liquidity to loan ratios. He would be laughed out of the room. Angry constituents would point out how well self-regulation panned out in the global financial crisis. From big tobacco to big oil, we have learnt the hard way that businesses cannot set disinterested regulations. They are neither independent nor capable of creating countervailing powers to their own.</em>” (
<a href=https://read.readwise.io/read/01h534ajt6jtz3j7trnf5eshe5 rel=noopener>View Highlight</a>)</li><li>“<em>Imagine the chief executive of JPMorgan explaining to Congress that because financial products are too complex for lawmakers to understand, banks should decide for themselves how to prevent money laundering, enable fraud detection and set liquidity to loan ratios. He would be laughed out of the room. Angry constituents would point out how well self-regulation panned out in the global financial crisis. From big tobacco to big oil, we have learnt the hard way that businesses cannot set disinterested regulations. They are neither independent nor capable of creating countervailing powers to their own.</em>” (
<a href=https://read.readwise.io/read/01h534ak0rxc4hvgdbhew5x4b8 rel=noopener>View Highlight</a>)</li><li>The push for commercial control of AI capability is dangerous. Naomi Klein, who coined the term “
<a href=https://www.theguardian.com/us-news/2017/jul/06/naomi-klein-how-power-profits-from-disaster rel=noopener>shock doctrine</a>” as “the brutal tactic of using the public’s disorientation following a collective shock… to push through radical pro-corporate measures”, is
<a href=https://www.theguardian.com/commentisfree/2023/may/08/ai-machines-hallucinating-naomi-klein rel=noopener>now warning</a> that AI is “likely to become a fearsome tool of further dispossession and despoilation”. (
<a href=https://read.readwise.io/read/01h534bgjrqq5hk9va9wnf9qs4 rel=noopener>View Highlight</a>)</li><li>“<em>The rapid deployment of AI-based tools has strong parallels with that of leaded gasoline. Lead in gasoline solved a genuine problem—engine knocking. Thomas Midgley, the inventor of leaded gasoline, was aware of lead poisoning because he suffered from the disease. There were other, less harmful ways to solve the problem, which were developed only when legislators eventually stepped in to create the right incentives to counteract the enormous profits earned from selling leaded gasoline.</em>” (
<a href=https://read.readwise.io/read/01h534c4v8zqte0m3p386wczfe rel=noopener>View Highlight</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pelayoarbues.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Pelayo Arbués using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pelayoarbues.github.io/>Home</a></li><li><a href=https://twitter.com/pelayoarbues>Twitter</a></li><li><a href=https://sigmoid.social/@pelayoarbues>Mastodon</a></li><li><a href=https://www.linkedin.com/in/pelayoarbues/>Linkedin</a></li><li><a href="https://scholar.google.com/citations?user=WaC-GcIAAAAJ&hl">Scholar</a></li><li><a href=https://www.flickr.com/photos/wonderfulhorriblelife/>Flickr</a></li><li><a href=https://github.com/pelayoarbues>GitHub</a></li></ul></footer></div></div></body></html>
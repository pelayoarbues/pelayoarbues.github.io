<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Accurately Valuing Homes With Deep Learning and Structural Inductive Biases  Metadata  Author: [[Stu (Michael) Stewart]] Full Title: Accurately Valuing Homes With Deep Learning and Structural Inductive Biases Category: #articles Document Tags: [[favorite]] URL: https://medium."><title>Accurately Valuing Homes With Deep Learning and Structural Inductive Biases</title><meta name=viewport content="width=device-width,initial-scale=1"><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjFhYTEyN2VjLWI0MTEtNDdjNS1iNWQzLTA5OGE2NDZjMWZhYSIsICJpZCI6ICI5YzFhNzE4Zi0xNjA1LTRmMTUtOGQ3Yy05NzliMDBjNWVmNTcifQ.-aCZYTIrTPiQCDYC_zS0dH0IVOXqI9ThnK39DMDgY7c></script><link rel="shortcut icon" type=image/png href=https://pelayoarbues.github.io//icon.png><link href=https://pelayoarbues.github.io/styles.23ddc3ecf7e887d89acd50af3b520de2.min.css rel=stylesheet><link href=https://pelayoarbues.github.io/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css rel=stylesheet id=theme-link><script src=https://pelayoarbues.github.io/js/darkmode.d1ab992b86a0866f970f82ef4d95b010.min.js></script>
<script src=https://pelayoarbues.github.io/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://pelayoarbues.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script>const isReducedMotion=window.matchMedia("(prefers-reduced-motion: reduce)").matches,lastVisit=localStorage.getItem("lastVisitTime"),now=Date.now();let show="true";if(lastVisit){document.documentElement.setAttribute("visited","true");const e=Math.ceil((now-parseInt(lastVisit))/(1e3*60));show=!isReducedMotion&&e>5?"true":"false"}document.documentElement.setAttribute("show-animation",show),localStorage.setItem("lastVisitTime",`${now}`);const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pelayoarbues.github.io/",fetchData=Promise.all([fetch("https://pelayoarbues.github.io/indices/linkIndex.2e740fdb5db10decbf653bae61c6a7d2.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pelayoarbues.github.io/indices/contentIndex.4f1f845e78da67bf5e236d1c8b60975b.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://pelayoarbues.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pelayoarbues.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:.8,opacityScale:3,repelForce:2,scale:1})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pelayoarbues.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pelayoarbues.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pelayoarbues.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div id=cursor-chat-layer><input type=text id=cursor-chat-box></div><script type=module>
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script><div class=singlePage><header class="delay t-3"><h1 id=page-title><a class=root-title href=https://pelayoarbues.github.io/>Pelayo Arbués</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Accurately Valuing Homes With Deep Learning and Structural Inductive Biases</h1><p class=meta>Last updated
Mar 29, 2023</p><ul class=tags><li><a href=https://pelayoarbues.github.io/tags/articles/>Articles</a></li><li><a href=https://pelayoarbues.github.io/tags/literature-note/>Literature Note</a></li></ul><a href=#accurately-valuing-homes-with-deep-learning-and-structural-inductive-biases><h1 id=accurately-valuing-homes-with-deep-learning-and-structural-inductive-biases><span class=hanchor arialabel=Anchor># </span>Accurately Valuing Homes With Deep Learning and Structural Inductive Biases</h1></a><p><img src=https://miro.medium.com/v2/resize:fit:1200/1*G4oSJEEk7Y0xeG5Tkk283w.png width=auto alt=rw-book-cover></p><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">Stu (Michael) Stewart</a></li><li>Full Title: Accurately Valuing Homes With Deep Learning and Structural Inductive Biases</li><li>Category: #articles</li><li>Document Tags: <a class="internal-link broken">favorite</a></li><li>URL:
<a href=https://medium.com/opendoor-labs/accurately-valuing-homes-with-deep-learning-and-structural-inductive-biases-18232ede1efd rel=noopener>https://medium.com/opendoor-labs/accurately-valuing-homes-with-deep-learning-and-structural-inductive-biases-18232ede1efd</a></li></ul><a href=#highlights><h2 id=highlights><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>utomated valuation model, is the name given to a Machine Learning model that estimates the value of a property, usually by comparing that property to similar nearby properties that have recently sold (comparables or comps). Comps are key: an AVM evaluates a property relative to its comps, assimilating those data into a single number quantifying the property’s value. (
<a href=https://read.readwise.io/read/01gwh2we0jekv6cp9r3wztcp6j rel=noopener>View Highlight</a>)</li><li>Many companies have an “AI strategy,” but at Opendoor (scare quotes) “AI” <em>is the business</em>. We don’t buy or sell a single home without consulting OVM — for a valuation, for information regarding comparable properties, or for both. (
<a href=https://read.readwise.io/read/01gwh2x6shqgtr90rdsd5gkzpv rel=noopener>View Highlight</a>)</li><li>OVM (handcrafted model pipeline)<ol><li>Select comps</li><li>Score said comps based on their “closeness” to the subject property</li><li>“Weight” each comp relative to the others</li><li>“Adjust” the (observed) prices of each comp</li><li>Estimate “uncertainty” via <em>multiple</em> additional models (
<a href=https://read.readwise.io/read/01gwh32xcfa8nh0j9186ksh4w8 rel=noopener>View Highlight</a>)</li></ol></li><li><img src=https://miro.medium.com/v2/resize:fit:700/1*glK4GZJRE08NzUdXyFdbFA.png width=auto alt> (
<a href=https://read.readwise.io/read/01gwh30xfgmabhf8qd4fp3rnzs rel=noopener>View Highlight</a>)</li><li>or our purposes, the ability to define an end-to-end system, in which gradients flow freely through all aspects of the valuation process, is key: for instance, attempting to assign weights to comps without <em>also</em> considering the requisite adjustments (a shortcoming of the prior algorithm) leaves useful information on the table. This shortcoming was top-of-mind as we built out our current framework. (
<a href=https://read.readwise.io/read/01gwh342dbr0p6w890x2r49x7x rel=noopener>View Highlight</a>)</li><li>OVM (deep learning)<ol><li>Select comps</li><li>Give <em>everything</em> to a neural net and hope it works!
<img src=https://miro.medium.com/v2/resize:fit:700/1*Lb5sZzHbYx2Cy26ZHu_yZA.png width=auto alt> (
<a href=https://read.readwise.io/read/01gwh35cg79t80vqghv6q43vps rel=noopener>View Highlight</a>)</li></ol></li><li>In residential real estate, we know the causal mechanism by which homes are valued, a powerful backstop typically unavailable in computer vision or Natural Language Processing (NLP) applications. That is, a home is priced by a human real estate agent who consults comps (those same comps again) and defines/adjusts the listing’s price based on:
• the recency of the comps (which factors in home price fluctuations)
• how fancy/new/desirable the comps are relative to the subject property (
<a href=https://read.readwise.io/read/01gwh38jfwm18qphf21nvgrjfy rel=noopener>View Highlight</a>)</li><li>Comp prices are <em>more than correlative</em>: a nearby comp selling for more than its intrinsic value <em>literally causes one’s home to be worth more</em>, as no shopper will be able to perfectly parse the underlying “intrinsic value” from the “noise (error)” of previous home sales. This overvaluation <em>propagates causally into the future prices of other nearby homes.</em> (
<a href=https://read.readwise.io/read/01gwh399gqb68ghhz866q0r4s4 rel=noopener>View Highlight</a>)</li><li>Deep learning, through categorical feature embeddings, unlocks an extremely powerful treatment of high-cardinality categorical variables that is ill-approximated by ML stalwarts like linear models and Gradient Boosted Trees. (
<a href=https://read.readwise.io/read/01gwh3jwfdregdp9c4fzsbrppg rel=noopener>View Highlight</a>)</li><li>Real estate has surprising similarities to NLP: high-cardinality features such as postal code, census block, school district, etc. are nearly as central to home valuations as words are to language. By providing access to best-in-class handling of categorical features, a deep learning based solution immediately resolved a primary flaw of our system. Better yet, we didn’t need to lift a finger, as embedding layers are a provided building-block in all modern deep learning frameworks. (
<a href=https://read.readwise.io/read/01gwh3kdct4a5x3pzvgp4b3cxp rel=noopener>View Highlight</a>)</li><li>The final unresolved defect of our prior algorithm was its inability to jointly optimize parameters across sub-model boundaries. For instance, the model that assigned comp weights did not “talk” to the model that predicted the dollar-value of the comp-adjustment that would bring said comp to parity with the subject listing. (
<a href=https://read.readwise.io/read/01gwh3ktsgew81kcy1xpkg2k8n rel=noopener>View Highlight</a>)</li><li>PyTorch, cleanly resolves this fault, as well. We can define sub-modules of our network to tackle the adjustment and weighting schemes for each comp, and autograd will handle backward-pass gradient propagation within and between the sub-modules of the net. (
<a href=https://read.readwise.io/read/01gwh3mj8d92ttjtq4t9tdbtf7 rel=noopener>View Highlight</a>)</li><li>There are several approaches to model this process (while enabling joint-optimization). We’ve had success with many model paradigms presently popular in <strong>NLP</strong> and/or <strong>image retrieval / visual search</strong>. (
<a href=https://read.readwise.io/read/01gwh3n4g1fb1437td6v3phvcc rel=noopener>View Highlight</a>)</li><li>• Transformer-style network architectures that accept a variable-length sequence of feature vectors (perhaps words or houses) and emit a sequence or single number quantifying an output
•
<a href=https://en.wikipedia.org/wiki/Siamese_neural_network rel=noopener>Siamese Networks</a> that compare, for example, images or home listings and produce a number/vector quantifying the similarity between any two of them
•
<a href=https://en.wikipedia.org/wiki/Triplet_loss rel=noopener>Triplet loss</a> frameworks for similarity detection (and, more recently, contrastive-loss approaches spiritually similar to triplet loss)
• Embedding lookup schemes such as
<a href=https://en.wikipedia.org/wiki/Locality-sensitive_hashing rel=noopener>Locality Sensitive Hashing</a> that efficiently search a vector-space for similar entities to a query-vector of interest (
<a href=https://read.readwise.io/read/01gwh3nt0gqmcnh7wn8cj21ee7 rel=noopener>View Highlight</a>)</li><li>The process of valuing a home is similar to NLP for one key reason: a home “lives” in a neighborhood just as a word “lives” in a sentence. Using the local context to understand a word works well; it is intuitive that a comparable method could succeed in real estate. (
<a href=https://read.readwise.io/read/01gwh3p5rbtyb5smdveewaf1cv rel=noopener>View Highlight</a>)</li><li>Image retrieval hinges on querying a massive database for images similar to the query image — a process quite aligned with the comparable-listing selection process.
Which model works best will depend on the specifics of the issue one is trying to solve. Building a world-class AVM involves geographical nuance as well: an ensemble of models stratified by region and/or urban/suburban/exurban localization may leverage many or all of the above methodologies. (
<a href=https://read.readwise.io/read/01gwh3psv2f1emjs98ty7b954w rel=noopener>View Highlight</a>)</li><li>With our network(s) we must be able to answer two key questions:<ol><li>How much more (or less) expensive is some comp listing than the listing of interest?</li><li>How much weight (if any) should be assigned to said comp, relative to the other comps? (
<a href=https://read.readwise.io/read/01gwh3q3m7p0kvdkdcyckkv7jv rel=noopener>View Highlight</a>)</li></ol></li><li>The module might take in tabular data (about the listing’s features), photos, satellite imagery, text, etc. It may also use contextual information, including information about the other comps available for the given listing of interest — a transformer’s <em>self-attention</em> aligns well with this notion of contextual info. (
<a href=https://read.readwise.io/read/01gwh3r0bdxrqaxcyferde4bgh rel=noopener>View Highlight</a>)</li><li>• An estimate of the relative price difference between a given (subject, comp) listing pair
• A “logit” (un-normalized weight) characterizing the relative strength of a comp (
<a href=https://read.readwise.io/read/01gwh3va19n5adpknrgvdwh1pd rel=noopener>View Highlight</a>)</li><li>Because the comp weights should sum to one, a normalization scheme (perhaps softmax, sparsemax, or a learned reduction-function) is employed after the weights are computed. Recall that the comparable properties have already recently sold (never mind active listings for now), so their close prices are known. That close price, augmented by the price delta computed in (1), is itself a powerful predictor of the close price of the subject listing. (
<a href=https://read.readwise.io/read/01gwh3w4d107pzbgewsc9z2w2f rel=noopener>View Highlight</a>)</li><li>These transformer-based techniques from NLP work well because each comp can be viewed as a draw from a relatively homogeneous bag of possible comparable properties. In this capacity, comps are quite similar to words in the context of a language model: atomic units that together form a “sentence” that describes the subject listing and speculates regarding its worth.
Though, deciding which words (comps) to place in that sentence is a tricky problem in its own right. (
<a href=https://read.readwise.io/read/01gwh3wz26vf6y408829bphdb8 rel=noopener>View Highlight</a>)</li><li>Once the aforementioned quantities are in hand, the valuation process reduces immediately to a standard regression problem:<ol><li>The observed comp close prices are adjusted via the values proposed by our network</li><li>These adjusted close prices are reduced, via a weighted-average-like procedure, to a point estimate of the subject’s close price</li><li>Your favorite regression loss can then be employed, as usual, to train the model and learn the parameters of the network (
<a href=https://read.readwise.io/read/01gwh3xjpd2ydtmaqgerng8610 rel=noopener>View Highlight</a>)</li></ol></li><li>We saw a step function improvement in accuracy after implementing these ideas; the bulk of the improvement can be attributed to (1) end-to-end learning and (2) efficient embeddings of high-cardinality categorical features. (
<a href=https://read.readwise.io/read/01gwh3yn46qszq8ade0vq58v8r rel=noopener>View Highlight</a>)</li><li>Humans interact (and fall in love) with homes through photos. It seems natural, then, that a deep learning model would leverage these images when comparing homes to one another during the appraisal process. After all, one of the great success stories of deep learning is the field of computer vision. Transitioning OVM to deep learning has the added benefit of making it much easier to incorporate mixed-media data, such as images and text (from listings, tax documents, etc.) into our core algorithm. (
<a href=https://read.readwise.io/read/01gwh3zjhqtsezcn11rskfmhs0 rel=noopener>View Highlight</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pelayoarbues.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Pelayo Arbués using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pelayoarbues.github.io/>Home</a></li><li><a href=https://twitter.com/pelayoarbues>Twitter</a></li><li><a href=https://sigmoid.social/@pelayoarbues>Mastodon</a></li><li><a href=https://www.linkedin.com/in/pelayoarbues/>Linkedin</a></li><li><a href="https://scholar.google.com/citations?user=WaC-GcIAAAAJ&hl">Scholar</a></li><li><a href=https://www.flickr.com/photos/wonderfulhorriblelife/>Flickr</a></li><li><a href=https://github.com/pelayoarbues>GitHub</a></li></ul></footer></div></div></body></html>
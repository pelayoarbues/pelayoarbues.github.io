<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Building LLM Applications for Production  Metadata  Author: [[Chip Huyen]] Full Title: Building LLM Applications for Production Category: #articles URL: https://huyenchip."><title>Building LLM Applications for Production</title><meta name=viewport content="width=device-width,initial-scale=1"><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjFhYTEyN2VjLWI0MTEtNDdjNS1iNWQzLTA5OGE2NDZjMWZhYSIsICJpZCI6ICI5YzFhNzE4Zi0xNjA1LTRmMTUtOGQ3Yy05NzliMDBjNWVmNTcifQ.-aCZYTIrTPiQCDYC_zS0dH0IVOXqI9ThnK39DMDgY7c></script><link rel="shortcut icon" type=image/png href=https://pelayoarbues.github.io//icon.png><link href=https://pelayoarbues.github.io/styles.23ddc3ecf7e887d89acd50af3b520de2.min.css rel=stylesheet><link href=https://pelayoarbues.github.io/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css rel=stylesheet id=theme-link><script src=https://pelayoarbues.github.io/js/darkmode.d1ab992b86a0866f970f82ef4d95b010.min.js></script>
<script src=https://pelayoarbues.github.io/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://pelayoarbues.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script>const isReducedMotion=window.matchMedia("(prefers-reduced-motion: reduce)").matches,lastVisit=localStorage.getItem("lastVisitTime"),now=Date.now();let show="true";if(lastVisit){document.documentElement.setAttribute("visited","true");const e=Math.ceil((now-parseInt(lastVisit))/(1e3*60));show=!isReducedMotion&&e>5?"true":"false"}document.documentElement.setAttribute("show-animation",show),localStorage.setItem("lastVisitTime",`${now}`);const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pelayoarbues.github.io/",fetchData=Promise.all([fetch("https://pelayoarbues.github.io/indices/linkIndex.c6aafb1c715b58fc305cadb2528a229a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pelayoarbues.github.io/indices/contentIndex.1c388bb14705c128a59bc3234ce51863.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://pelayoarbues.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pelayoarbues.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:.8,opacityScale:3,repelForce:2,scale:1})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pelayoarbues.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pelayoarbues.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pelayoarbues.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div id=cursor-chat-layer><input type=text id=cursor-chat-box></div><script type=module>
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script><div class=singlePage><header class="delay t-3"><h1 id=page-title><a class=root-title href=https://pelayoarbues.github.io/>Pelayo Arbués</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Building LLM Applications for Production</h1><p class=meta>Last updated
Apr 25, 2023</p><ul class=tags><li><a href=https://pelayoarbues.github.io/tags/articles/>Articles</a></li><li><a href=https://pelayoarbues.github.io/tags/literature-note/>Literature Note</a></li></ul><a href=#building-llm-applications-for-production><h1 id=building-llm-applications-for-production><span class=hanchor arialabel=Anchor># </span>Building LLM Applications for Production</h1></a><p><img src=https://huyenchip.com/assets/pics/llmops/llama_taming.png width=auto alt=rw-book-cover></p><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><ul><li>Author: <a class="internal-link broken">Chip Huyen</a></li><li>Full Title: Building LLM Applications for Production</li><li>Category: #articles</li><li>URL:
<a href=https://huyenchip.com/2023/04/11/llm-engineering.html rel=noopener>https://huyenchip.com/2023/04/11/llm-engineering.html</a></li></ul><a href=#highlights><h2 id=highlights><span class=hanchor arialabel=Anchor># </span>Highlights</h2></a><ul><li>• It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.
• LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field. (
<a href=https://read.readwise.io/read/01gythgw1g0rztbyerhndvrrkp rel=noopener>View Highlight</a>)</li></ul><a href=#new-highlights-added-april-25-2023-at-1150-am><h2 id=new-highlights-added-april-25-2023-at-1150-am><span class=hanchor arialabel=Anchor># </span>New highlights added April 25, 2023 at 11:50 AM</h2></a><ul><li>• It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.
• LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguous nature of natural languages, and partially due to the nascent nature of the field. (
<a href=https://read.readwise.io/read/01gyvq5zj74e2jyaa4jg0ghc6q rel=noopener>View Highlight</a>)</li><li>Challenges of productionizing prompt engineering (
<a href=https://read.readwise.io/read/01gyvq6bgnjnf7fscb92rhyc3y rel=noopener>View Highlight</a>)</li><li>In prompt engineering, instructions are written in natural languages, which are a lot more flexible than programming languages. This can make for a great user experience, but can lead to a pretty bad developer experience. (
<a href=https://read.readwise.io/read/01gyvq6ncjd2ms67myexrvbmqw rel=noopener>View Highlight</a>)</li><li>if someone accidentally changes a prompt, it will still run but give very different outputs. (
<a href=https://read.readwise.io/read/01gyvq75m9v7pqafv9t66ykps1 rel=noopener>View Highlight</a>)</li><li>We can craft our prompts to be explicit about the output format, but there’s no guarantee that the outputs will <em>always</em> follow this format. (
<a href=https://read.readwise.io/read/01gyvq83nx5356mhec9k501fmd rel=noopener>View Highlight</a>)</li><li><strong>Inconsistency in user experience</strong>: when using an application, users expect certain consistency (
<a href=https://read.readwise.io/read/01gyvq8b6wvhr5jy70p40xare1 rel=noopener>View Highlight</a>)</li><li>You can force an LLM to give the same response by setting <strong><a href=https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature rel=noopener>temperature = 0</a></strong>, which is, in general, a good practice. While it
<a href=https://community.openai.com/t/observing-discrepancy-in-completions-with-temperature-0/73380 rel=noopener>mostly solves the consistency problem</a>, it doesn’t inspire trust in the system (
<a href=https://read.readwise.io/read/01gyvq8mm0c3z5w08av2ge4f4h rel=noopener>View Highlight</a>)</li><li>This seems to be a problem that OpenAI is actively trying to mitigate. They have a notebook with tips on how to increase their models’ reliability. (
<a href=https://read.readwise.io/read/01gyvqay8vwe3jw51w9n4tp2fe rel=noopener>View Highlight</a>)</li><li>A couple of people who’ve worked with LLMs for years told me that they just accepted this ambiguity and built their workflows around that. It’s a different mindset compared to developing deterministic programs, but not something impossible to get used to. (
<a href=https://read.readwise.io/read/01gyvqbq053fdeqjwg6hss91he rel=noopener>View Highlight</a>)</li><li>A common technique for prompt engineering is to provide in the prompt a few examples and hope that the LLM will generalize from these examples (fewshot learners). (
<a href=https://read.readwise.io/read/01gyvqc8pv68jsrkr3vzw0jbxj rel=noopener>View Highlight</a>)</li><li><strong>Whether the LLM understands the examples given in the prompt</strong>. One way to evaluate this is to input the same examples and see if the model outputs the expected scores. If the model doesn’t perform well on the same examples given in the prompt, it is likely because the prompt isn’t clear (
<a href=https://read.readwise.io/read/01gyvqd6m7kwtc4z99qz9dj895 rel=noopener>View Highlight</a>)</li><li><strong>Whether the LLM overfits to these fewshot examples.</strong> You can evaluate your model on separate examples. (
<a href=https://read.readwise.io/read/01gyvqdb8vkj6wkmcfc1vr0tdc rel=noopener>View Highlight</a>)</li><li>Small changes to a prompt can lead to very different results. It’s essential to version and track the performance of each prompt. You can use git to version each prompt and its performance, but I wouldn’t be surprised if there will be tools like MLflow or Weights & Biases for prompt experiments. (
<a href=https://read.readwise.io/read/01gyvqmqw8b71dfz957wzsvqrk rel=noopener>View Highlight</a>)</li><li>• Prompt the model to explain or explain step-by-step how it arrives at an answer, a technique known as
<a href=https://arxiv.org/abs/2201.11903 rel=noopener>Chain-of-Thought</a> or COT (Wei et al., 2022). <strong>Tradeoff</strong>: COT can increase both latency and cost due to the increased number of output tokens [see <strong>Cost and latency</strong> section]
• Generate many outputs for the same input. Pick the final output by either the majority vote (also known as
<a href=https://arxiv.org/abs/2203.11171 rel=noopener>self-consistency technique</a> by Wang et al., 2023) or you can ask your LLM to pick the best one. In OpenAI API, you can generate multiple responses for the same input by passing in the argument
<a href=https://platform.openai.com/docs/api-reference/completions/create rel=noopener>n</a> (not an ideal API design if you ask me).
• Break one big prompt into smaller, simpler prompts. (
<a href=https://read.readwise.io/read/01gyvqnzcsz0mypcpk1jzq5g4n rel=noopener>View Highlight</a>)</li><li>The more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost. (
<a href=https://read.readwise.io/read/01gyvqph3p05sphrwv3ex6q6tj rel=noopener>View Highlight</a>)</li><li>Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone. (
<a href=https://read.readwise.io/read/01gyvqptg5rqhb2n649dteg45v rel=noopener>View Highlight</a>)</li><li><strong>prompt engineering is a cheap and fast way get something up and running</strong>. For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer. (
<a href=https://read.readwise.io/read/01gyvqqgcpdb0h0m0bx5qtmghv rel=noopener>View Highlight</a>)</li><li><strong>The cost of LLMOps is in inference.</strong> (
<a href=https://read.readwise.io/read/01gyvqqz8ch9pwsfjtpe52rbs1 rel=noopener>View Highlight</a>)</li><li>Input tokens can be processed in parallel, which means that input length shouldn’t affect the latency that much.
However, output length significantly affects latency, which is likely due to output tokens being generated sequentially. (
<a href=https://read.readwise.io/read/01gyvqrzh4tbr598ycab0s3dat rel=noopener>View Highlight</a>)</li><li>The most successful one seemed to be <em>randaller</em> who was able to get the
<a href=https://github.com/randaller/llama-chat rel=noopener>30B parameter model work on 128 GB of RAM</a>, which takes a few seconds just to generate one token. (
<a href=https://read.readwise.io/read/01gyvqtch7czjdrqrhskt6xzka rel=noopener>View Highlight</a>)</li><li>many teams have told me they feel like they have to redo the feasibility estimation and buy (using paid APIs) vs. build (using open source models) decision every week. (
<a href=https://read.readwise.io/read/01gyvqvsms1r8zyamgp7nb0jn1 rel=noopener>View Highlight</a>)</li><li>There are 3 main factors when considering prompting vs. finetuning: data availability, performance, and cost. (
<a href=https://read.readwise.io/read/01gyvqwbwvvvtpjn3v1tsvw9wz rel=noopener>View Highlight</a>)</li><li>f you have only a few examples, prompting is quick and easy to get started. <strong>There’s a limit to how many examples you can include in your prompt due to the maximum input token length.</strong> (
<a href=https://read.readwise.io/read/01gyvqwes8m3jv7xxvzzb0grcg rel=noopener>View Highlight</a>)</li><li>. In my experience, however, you can expect a noticeable change in your model performance if you finetune on 100s examples. However, the result might not be much better than prompting. (
<a href=https://read.readwise.io/read/01gyvqwqsx9xez5gr72htyrh5e rel=noopener>View Highlight</a>)</li><li>The general trend is that <strong>as you increase the number of examples, finetuning will give better model performance than prompting</strong>. (
<a href=https://read.readwise.io/read/01gyvqx95qx0zttke2g8wkd4j1 rel=noopener>View Highlight</a>)</li><li>• You can get better model performance: can use more examples, examples becoming part of the model’s internal knowledge.
• You can reduce the cost of prediction. The more instruction you can bake into your model, the less instruction you have to put into your prompt. (
<a href=https://read.readwise.io/read/01gyvqxktgcshyys6brkrh2hp6 rel=noopener>View Highlight</a>)</li><li>tarting with a prompt, instead of changing this prompt, you programmatically change the embedding of this prompt. For prompt tuning to work, you need to be able to input prompts’ embeddings into your LLM model and generate tokens from these embeddings, which currently, can only be done with open-source LLMs and not in OpenAI API. On T5, prompt tuning appears to perform much better than prompt engineering and can catch up with model tuning (
<a href=https://read.readwise.io/read/01gyvqy9cdhpj3rzgrgve95vea rel=noopener>View Highlight</a>)</li><li>finetune a smaller open-source language model (LLaMA-7B, the 7 billion parameter version of LLaMA) on examples generated by a larger language model (<em>text-davinci-003</em> – 175 billion parameters). This technique of training a small model to imitate the behavior of a larger model is called distillation (
<a href=https://read.readwise.io/read/01gyvqz7kjzwsyzdrvnztxmvvr rel=noopener>View Highlight</a>)</li><li>For finetuning, they used 52k instructions, which they inputted into <em>text-davinci-003</em> to obtain outputs, which are then used to finetune LLaMa-7B. This costs under $500 to generate. The training process for finetuning costs under $100. (
<a href=https://read.readwise.io/read/01gyvqzkpra8bj58mpeqzt633r rel=noopener>View Highlight</a>)</li><li>One direction that I find very promising is to use LLMs to generate embeddings and then build your ML applications on top of these embeddings, e.g. for search and recsys. (
<a href=https://read.readwise.io/read/01gyvr05cm6r425rt8kxntbxvm rel=noopener>View Highlight</a>)</li><li>The main cost of embedding models for real-time use cases is loading these embeddings into a vector database for low-latency retrieval. However, you’ll have this cost regardless of which embeddings you use. (
<a href=https://read.readwise.io/read/01gyvr33ekvjn90ej7t49m3wva rel=noopener>View Highlight</a>)</li><li><strong>2023 is the year of vector databases</strong>. (
<a href=https://read.readwise.io/read/01gyvr34zwze5e5rtepnn3bjn9 rel=noopener>View Highlight</a>)</li><li>One observation with
<a href=https://situatedqa.github.io/ rel=noopener>SituatedQA</a> dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge. (
<a href=https://read.readwise.io/read/01gyvrf52a9r372rjhppy6va8n rel=noopener>View Highlight</a>)</li><li>, with prompt engineering, if you want to use a newer model, there’s no way to guarantee that all your prompts will still work as intended with the newer model, so you’ll likely have to rewrite your prompts again. <strong>If you expect the models you use to change at all, it’s important to unit-test all your prompts using evaluation examples.</strong> (
<a href=https://read.readwise.io/read/01gyvrfhdbppat9epb81aje8qj rel=noopener>View Highlight</a>)</li><li>Newer models might, overall, be better, but there will be use cases for which newer models are worse. (
<a href=https://read.readwise.io/read/01gyvrfy01cgte8hh1k0295jwe rel=noopener>View Highlight</a>)</li><li>Experiments with prompts are fast and cheap, as we discussed in the section <strong>Cost</strong>. While I agree with this argument, a big challenge I see in MLOps today is that there’s a lack of centralized knowledge for model logic, feature logic, prompts, etc. (
<a href=https://read.readwise.io/read/01gyvrg7v527f4t5tfy9kz9p4h rel=noopener>View Highlight</a>)</li><li>The word agent is being thrown around a lot to refer to an application that can execute multiple tasks according to a given <strong>control flow</strong> (see Control flows section). A task can leverage one or more <strong>tools</strong>. (
<a href=https://read.readwise.io/read/01gyvrjj3y5k8ncm2hgw6tqjxe rel=noopener>View Highlight</a>)</li><li>In the example above, sequential is an example of a control flow in which one task is executed after another. (
<a href=https://read.readwise.io/read/01gyvrk55sbsvdhn3b1nykxwfs rel=noopener>View Highlight</a>)</li><li><img src=https://huyenchip.com/assets/pics/llmops/8_control_flows.png width=auto alt="LLM Engineering: control flows"> (
<a href=https://read.readwise.io/read/01gyvrmfe90rkwfnb76fxfxyer rel=noopener>View Highlight</a>)</li><li>In traditional software engineering, conditions for control flows are exact. With LLM applications (also known as agents), conditions might also be determined by prompting. (
<a href=https://read.readwise.io/read/01gyvrmv6c6dkbx682vdhej9ck rel=noopener>View Highlight</a>)</li><li>For agents to be reliable, we’d need to be able to build and test each task separately before combining them. There are two major types of failure modes:<ol><li>One or more tasks fail. Potential causes:</li><li>Control flow is wrong: a non-optional action is chosen</li><li>One or more tasks produce incorrect results</li><li>All tasks produce correct results but the overall solution is incorrect. Press et al. (2022) call this “
<a href=https://ofir.io/self-ask.pdf rel=noopener>composability gap</a>”: the fraction of compositional questions that the model answers incorrectly out of all the compositional questions for which the model answers the sub-questions correctly. (
<a href=https://read.readwise.io/read/01gyvrnd7geqwwmqtnd05y167f rel=noopener>View Highlight</a>)</li></ol></li><li>This is hands down the most popular consumer use case. There are AI assistants built for different tasks for different groups of users (
<a href=https://read.readwise.io/read/01gyvrs1h31qpn8v0kqpfe5240 rel=noopener>View Highlight</a>)</li><li>Chatbots are similar to AI assistants in terms of APIs. If AI assistants’ goal is to fulfill tasks given by users, whereas chatbots’ goal is to be more of a companion (
<a href=https://read.readwise.io/read/01gyvrsqh65fwgdmsvh8cg156y rel=noopener>View Highlight</a>)</li><li>The most interesting company in the consuming-chatbot space is probably Character.ai. It’s a platform for people to create and share chatbots. The most popular types of chatbots on the platform, as writing, are anime and game characters, but you can also talk to a psychologist, a pair programming partner, or a language practice partner. (
<a href=https://read.readwise.io/read/01gyvrvmee0es94c3hk22942ng rel=noopener>View Highlight</a>)</li><li>how to incorporate ChatGPT to help students learn even faster. All EdTech companies I know are going full-speed on ChatGPT exploration. (
<a href=https://read.readwise.io/read/01gyvrw7n9qp9nbpxb4c0gcew0 rel=noopener>View Highlight</a>)</li><li>Many, many startups are building tools to let enterprise users query their internal data and policies in natural languages or in the Q&A fashion. Some focus on verticals such as legal contracts, resumes, financial data, or customer support. Given a company’s all documentations, policies, and FAQs, you can build a chatbot that can respond your customer support requests. (
<a href=https://read.readwise.io/read/01gyvrwy5j9attngghrp2s39y1 rel=noopener>View Highlight</a>)</li><li>• Organize your internal data into a database (SQL database, graph database, embedding/vector database, or just text database)
• Given an input in natural language, convert it into the query language of the internal database. For example, if it’s a SQL or graph database, this process can return a SQL query. If it’s embedding database, it’s might be an ANN (approximate nearest neighbor) retrieval query. If it’s just purely text, this process can extract a search query.
• Execute the query in the database to obtain the query result.
• Translate this query result into natural language. (
<a href=https://read.readwise.io/read/01gyvrxaqc8gw747txafzdav2x rel=noopener>View Highlight</a>)</li><li>But what if searching for “things you need for camping in oregon in november” on Amazon actually returns you a list of things you need for your camping trip?
It’s possible today with LLMs. For example, the application can be broken into the following steps:<ol><li>Task 1: convert the user query into a list of product names [LLM]</li><li>Task 2: for each product name in the list, retrieve relevant products from your product catalog. (
<a href=https://read.readwise.io/read/01gyvsc4dv0z3m0269q084124m rel=noopener>View Highlight</a>)</li></ol></li><li>If this works, I wonder if we’ll have LLM SEO: techniques to get your products recommended by LLMs. (
<a href=https://read.readwise.io/read/01gyvscfj2tjmqmb1wa3xz7y8x rel=noopener>View Highlight</a>)</li><li>many prompt engineering papers remind me of the early days of deep learning when there were thousands of papers describing different ways to initialize weights (
<a href=https://read.readwise.io/read/01gyvsd50bkdpbhm00wkqw1c53 rel=noopener>View Highlight</a>)</li><li>Given that LLMs seem to be pretty good at writing prompts for themselves – see
<a href=https://arxiv.org/abs/2211.01910 rel=noopener>Large Language Models Are Human-Level Prompt Engineers</a> (Zhou et al., 2022) – who knows that we’ll need humans to tune prompts? (
<a href=https://read.readwise.io/read/01gyvsdg7s5gjcfwrbx1hc47xv rel=noopener>View Highlight</a>)</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pelayoarbues.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Pelayo Arbués using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pelayoarbues.github.io/>Home</a></li><li><a href=https://twitter.com/pelayoarbues>Twitter</a></li><li><a href=https://sigmoid.social/@pelayoarbues>Mastodon</a></li><li><a href=https://www.linkedin.com/in/pelayoarbues/>Linkedin</a></li><li><a href="https://scholar.google.com/citations?user=WaC-GcIAAAAJ&hl">Scholar</a></li><li><a href=https://www.flickr.com/photos/wonderfulhorriblelife/>Flickr</a></li><li><a href=https://github.com/pelayoarbues>GitHub</a></li></ul></footer></div></div></body></html>
---
author: [[Hugging Face - Blog]]
title: "TGI Multi-LoRA: Deploy Once, Serve 30 Models"
date: 2024-10-09
tags: 
- articles
- literature-note
---
![rw-book-cover](https://huggingface.co/front/thumbnails/v2-2.png)

## Metadata
- Author: [[Hugging Face - Blog]]
- Full Title: TGI Multi-LoRA: Deploy Once, Serve 30 Models
- URL: https://huggingface.co/blog/multi-lora-serving

## Highlights
- As an organization, building a multitude of models via fine-tuning makes sense for multiple reasons. ([View Highlight](https://read.readwise.io/read/01j9sg6gkw9e3zhw6wvykkqr54))
- **Performance -** There is [compelling evidence](https://huggingface.co/papers/2405.09673) that smaller, specialized models outperform their larger, general-purpose counterparts on the tasks that they were trained on. Predibase [[5]](https://huggingface.co/blog/multi-lora-serving#5) showed that you can get better performance than GPT-4 using task-specific LoRAs with a base like [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main). ([View Highlight](https://read.readwise.io/read/01j9sg6hpm20vw460xgddtedzg))
- **Adaptability -** Models like Mistral or Llama are extremely versatile. You can pick one of them as your base model and build many specialized models, even when the [downstream tasks are very different](https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4). Also, note that you aren't locked in as you can easily swap that base and fine-tune it with your data on another base (more on this later). ([View Highlight](https://read.readwise.io/read/01j9sg6n7ktbpmkm5ahq5ac7e7))
- **Independence -** For each task that your organization cares about, different teams can work on different fine tunes, allowing for independence in data preparation, configurations, evaluation criteria, and cadence of model updates. ([View Highlight](https://read.readwise.io/read/01j9sg6t0xzfa0km10k04hx1bp))
- **Privacy -** Specialized models offer flexibility with training data segregation and access restrictions to different users based on data privacy requirements. Additionally, in cases where running models locally is important, a small model can be made highly capable for a specific task while keeping its size small enough to run on device. ([View Highlight](https://read.readwise.io/read/01j9sg6w82mtqctch6f5wyp09e))
- In summary, fine-tuning enables organizations to unlock the value of their data, and this advantage becomes especially significant, even game-changing, when organizations use highly specialized data that is uniquely theirs. ([View Highlight](https://read.readwise.io/read/01j9sg6y59rshms1pa82nd1htp))
- So, where is the catch? Deploying and serving Large Language Models (LLMs) is challenging in many ways. Cost and operational complexity are key considerations when deploying a single model, let alone *n* models. This means that, for all its glory, fine-tuning complicates LLM deployment and serving even further. ([View Highlight](https://read.readwise.io/read/01j9sg70237k1hvffz9wgf8qec))
- Now that we understand the basic idea of model adaptation introduced by LoRA, we are ready to delve into multi-LoRA serving. The concept is simple: given one base pre-trained model and many different tasks for which you have fine-tuned specific LoRAs, multi-LoRA serving is a mechanism to dynamically pick the desired LoRA based on the incoming request. ([View Highlight](https://read.readwise.io/read/01j9sg79cx2perxbkbn06b3seq))
- Multi-LoRA serving enables you to deploy the base model just once. And since the LoRA adapters are small, you can load many adapters. Note the exact number will depend on your available GPU resources and what model you deploy. What you end up with is effectively equivalent to having multiple fine-tuned models in one single deployment. ([View Highlight](https://read.readwise.io/read/01j9sg858my1rxp3d0agn3h2dz))
- LoRAs (the adapter weights) can vary based on rank and quantization, but they are generally quite tiny. Let's get a quick intuition of how small these adapters are: [predibase/magicoder](https://huggingface.co/predibase/magicoder/tree/main) is 13.6MB, which is less than 1/1000th the size of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main), which is 14.48GB. In relative terms, loading 30 adapters into RAM results in only a 3% increase in VRAM. Ultimately, this is not an issue for most deployments. Hence, we can have one deployment for many models. ([View Highlight](https://read.readwise.io/read/01j9sg8e4n4m2rqj3vfg2adkta))
- First, you need to train your LoRA models and export the adapters. You can find a [guide here](https://huggingface.co/docs/peft/en/task_guides/lora_based_methods) on fine-tuning LoRA adapters. Do note that when you push your fine-tuned model to the Hub, you only need to push the adapter, not the full merged model. When loading a LoRA adapter from the Hub, the base model is inferred from the adapter model card and loaded separately again. For deeper support, please check out our [Expert Support Program](https://huggingface.co/support). The real value will come when you create your own LoRAs for your specific use cases. ([View Highlight](https://read.readwise.io/read/01j9sg8mj6ksy5k6cn0k297jdm))
- For some organizations, it may be hard to train one LoRA for every use case, as they may lack the expertise or other resources. Even after you choose a base and prepare your data, you will need to keep up with the latest techniques, explore hyperparameters, find optimal hardware resources, write the code, and then evaluate. This can be quite a task, even for experienced teams. ([View Highlight](https://read.readwise.io/read/01j9sg8w3eb9aygx2v4q34j68s))
- AutoTrain can lower this barrier to entry significantly. AutoTrain is a no-code solution that allows you to train machine learning models in just a few clicks. There are a number of ways to use AutoTrain. In addition to [locally/on-prem](https://github.com/huggingface/autotrain-advanced?tab=readme-ov-file#local-installation) we have: ([View Highlight](https://read.readwise.io/read/01j9sg90m4xvv7v7zmqj8ktgp9))
- [predibase/customer_support](https://huggingface.co/predibase/customer_support) is trained on the [Gridspace-Stanford Harper Valley speech dataset](https://github.com/cricketclub/gridspace-stanford-harper-valley) which enhances its ability to understand and respond to customer service interactions accurately. This improves the model's performance in tasks such as speech recognition, emotion detection, and dialogue management, leading to more efficient and empathetic customer support. ([View Highlight](https://read.readwise.io/read/01j9sg99n3k5pbar2ey93jvrfh))
- [predibase/magicoder](https://huggingface.co/predibase/magicoder) is trained on [ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K) which is a code instruction dataset that is synthetically generated. ([View Highlight](https://read.readwise.io/read/01j9sg9cq2n1rwyjrj7wxxrcfh))
- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/en/index) allows you to have access to deploy any Hugging Face model on many [GPUs and alternative Hardware types](https://huggingface.co/docs/inference-endpoints/en/pricing#gpu-instances) across AWS, GCP, and Azure all in a few clicks! In the GUI, it's easy to deploy. Under the hood, we use TGI by default for text generation (though you have the [option](https://huggingface.co/docs/inference-endpoints/en/guides/custom_container) to use any image you choose). ([View Highlight](https://read.readwise.io/read/01j9sg9n3wft0e4pae2jjm2pbg))
- We are not the first to climb this summit, as discussed [below](https://huggingface.co/blog/multi-lora-serving#Acknowledgements). The team behind LoRAX, Predibase, has an excellent [write up](https://predibase.com/blog/lorax-the-open-source-framework-for-serving-100s-of-fine-tuned-llms-in). Do check it out, as this section is based on their work. ([View Highlight](https://read.readwise.io/read/01j9sga6f8d0rrs75vp4jjdrye))
- One of the big benefits of Multi-LoRA serving is that **you don’t need to have multiple deployments for multiple models**, and ultimately this is much much cheaper. This should match your intuition as multiple models will need all the weights and not just the small adapter layer. As you can see in *Figure 5*, even when we add many more models with TGI Multi-LoRA the cost is the same per token. The cost for TGI dedicated scales as you need a new deployment for each fine-tuned model. ([View Highlight](https://read.readwise.io/read/01j9sgadfkr2ay7d45c1tz0jwd))
- One real-world challenge when you deploy multiple models is that you will have a strong variance in your usage patterns. Some models might have low usage; some might be bursty, and some might be high frequency. This makes it really hard to scale, especially when each model is independent. There are a lot of “rounding” errors when you have to add another GPU, and that adds up fast. In an ideal world, you would maximize your GPU utilization per GPU and not use any extra. You need to make sure you have access to enough GPUs, knowing some will be idle, which can be quite tedious. ([View Highlight](https://read.readwise.io/read/01j9sgaqryxccsrz6r8dj5ezpk))
- When we consolidate with Multi-LoRA, we get much more stable usage. We can see the results of this in *Figure 6* where the Multi-Lora Serving pattern is quite stable even though it consists of more volatile patterns. By consolidating the models, you allow much smoother usage and more manageable scaling. Do note that these are just illustrative patterns, but think through your own patterns and how Multi-LoRA can help. Scale 1 model, not 30! ([View Highlight](https://read.readwise.io/read/01j9sgaztyrkex5mc2sq738brh))
- What happens in the real world with AI moving at breakneck speeds? What if you want to choose a different/newer model as your base? While our examples use [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) as a base model, there are other bases like Mistral's [v0.3](https://ubiops.com/function-calling-deploy-the-mistral-7b-v03/) which supports [function calling](https://ubiops.com/function-calling-deploy-the-mistral-7b-v03/), and altogether different model families like Llama 3. In general, we expect new base models that are more efficient and more performant to come out all the time. ([View Highlight](https://read.readwise.io/read/01j9sgb3f5mnrg9cncjbys3jwq))
- But worry not! It is easy enough to re-train the LoRAs if you have a *compelling reason* to update your base model. Training is relatively cheap; in fact [Predibase found](https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4) it costs only ~$8.00 to train each one. The amount of code changes is minimal with modern frameworks and common engineering practices: ([View Highlight](https://read.readwise.io/read/01j9sgbnaa3h386jbdb4896qyw))
- Multi-LoRA serving represents a transformative approach in the deployment of AI models, providing a solution to the cost and complexity barriers associated with managing multiple specialized models. By leveraging a single base model and dynamically applying fine-tuned adapters, organizations can significantly reduce operational overhead while maintaining or even enhancing performance across diverse tasks. **AI Directors we ask you to be bold, choose a base model and embrace the Multi-LoRA paradigm,** the simplicity and cost savings will pay off in dividends. Let Multi-LoRA be the cornerstone of your AI strategy, ensuring your organization stays ahead in the rapidly evolving landscape of technology. ([View Highlight](https://read.readwise.io/read/01j9sgbw1m6y1rn3ffdnz1hbzd))
- Implementing Multi-LoRA serving can be really tricky, but due to awesome work by [punica-ai](https://github.com/punica-ai/punica) and the [lorax](https://github.com/predibase/lorax) team, optimized kernels and frameworks have been developed to make this process more efficient. TGI leverages these optimizations in order to provide fast and efficient inference with multiple LoRA models. ([View Highlight](https://read.readwise.io/read/01j9sgc89816jw01bk7dgbsdp2))

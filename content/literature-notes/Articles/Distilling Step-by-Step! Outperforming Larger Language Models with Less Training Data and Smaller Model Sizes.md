---
author: [[readwise.io]]
title: "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"
tags: 
- articles
- literature-note
---
# Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png)

## Metadata
- Author: [[readwise.io]]
- Full Title: Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes
- Category: #articles
- URL: https://readwise.io/reader/document_raw_content/51090821

## Highlights
- Distilling Step-by-Step! Outperforming Larger Language Models
  with Less Training Data and Smaller Model Sizes ([View Highlight](https://read.readwise.io/read/01gzxe7ekb466zw8j3rnwrbr59))

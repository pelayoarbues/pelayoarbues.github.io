---
author: [[openai.com]]
title: "Hello GPT-4o"
date: 2024-05-14
tags: 
- articles
- literature-note
---
![rw-book-cover](https://images.ctfassets.net/kftzwdyauwt9/4hxowKfhJ6Q3bVMqGsosjS/72c9e6f954de2ccd237813cab8a83a64/GPT-4o-blog.jpg?w=1080&h=1350&fit=fill)

## Metadata
- Author: [[openai.com]]
- Full Title: Hello GPT-4o
- URL: https://openai.com/index/hello-gpt-4o/

## Highlights
- Prior to GPT-4o, you could use [Voice Mode](https://openai.com/index/chatgpt-can-now-see-hear-and-speak) to talk to ChatGPT with latencies of 2.8 seconds (GPT-3.5) and 5.4 seconds (GPT-4) on average. To achieve this, Voice Mode is a pipeline of three separate models: one simple model transcribes audio to text, GPT-3.5 or GPT-4 takes in text and outputs text, and a third simple model converts that text back to audio. This process means that the main source of intelligence, GPT-4, loses a lot of information—it can’t directly observe tone, multiple speakers, or background noises, and it can’t output laughter, singing, or express emotion. ([View Highlight](https://read.readwise.io/read/01hxv1ykc6dq15rv3vryfe0j69))
- With GPT-4o, we trained a single new model end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. Because GPT-4o is our first model combining all of these modalities, we are still just scratching the surface of exploring what the model can do and its limitations. ([View Highlight](https://read.readwise.io/read/01hxv1ysq4nx3geq8nd82zpkpb))

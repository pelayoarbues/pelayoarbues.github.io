---
author: [[John Harris]]
title: "‘There Was All Sorts of Toxic Behaviour’: Timnit Gebru on Her Sacking by Google, AI’s Dangers and Big Tech’s Biases"
tags: 
- articles
- literature-note
---
# ‘There Was All Sorts of Toxic Behaviour’: Timnit Gebru on Her Sacking by Google, AI’s Dangers and Big Tech’s Biases

![rw-book-cover](https://i.guim.co.uk/img/media/8315430000e6e4e64af6679eb5fcd095c521b17a/246_635_4709_2825/master/4709.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=301a4055c4f86892a9045114f1104f57)

## Metadata
- Author: [[John Harris]]
- Full Title: ‘There Was All Sorts of Toxic Behaviour’: Timnit Gebru on Her Sacking by Google, AI’s Dangers and Big Tech’s Biases
- URL: https://www.theguardian.com/lifeandstyle/2023/may/22/there-was-all-sorts-of-toxic-behaviour-timnit-gebru-on-her-sacking-by-google-ais-dangers-and-big-techs-biases?CMP=Share_AndroidApp_Other

## Highlights
- We need regulation, and we need something better than just a profit motive ([View Highlight](https://read.readwise.io/read/01h13xw7narv408xds23a3t255))
- As the co-leader of Google’s small ethical AI team, Gebru was one of the authors of an academic paper that warned about the kind of AI that is increasingly built into our lives, taking internet searches and user recommendations to apparently new levels of sophistication and threatening to master such human talents as writing, composing music and analysing images. The clear danger, the paper said, is that such supposed “intelligence” is based on huge data sets that “overrepresent hegemonic viewpoints and encode biases potentially damaging to marginalised populations”. Put more bluntly, AI threatens to deepen the dominance of a way of thinking that is white, male, comparatively affluent and focused on the US and Europe. ([View Highlight](https://read.readwise.io/read/01h13xxf0gz4s6mr11s9shbc8p))
- Gebru began to specialise in cutting-edge AI, pioneering a system that showed how data about particular neighbourhoods’ patterns of car ownership highlighted differences bound up with ethnicity, crime figures, voting behaviour and income levels. In retrospect, this kind of work might look like the bedrock of techniques that could blur into automated surveillance and law enforcement, but Gebru admits that “none of those bells went off in my head … that connection of issues of technology with diversity and oppression came later”. ([View Highlight](https://read.readwise.io/read/01h13y1m13zggavjprq5a8nrdk))
- “I’m not worried about machines taking over the world; I’m worried about groupthink, insularity and arrogance in the AI community.” ([View Highlight](https://read.readwise.io/read/01h13y3behbv31b8sgah1d788a))
- These sources are usually scraped from the world wide web and inevitably include material usually subject to copyright (if an AI system can produce prose in the style of a particular writer, for example, that is because it has absorbed much of the writer’s work). But Gebru and her co-authors had an even graver concern: that trawling the online world risks reproducing its worst aspects, from hate speech to points of view that exclude marginalised people and places. “In accepting large amounts of web text as ‘representative’ of ‘all’ of humanity, we risk perpetuating dominant viewpoints, increasing power imbalances and further reifying inequality,” they wrote. ([View Highlight](https://read.readwise.io/read/01h13y9kkhmqvn25jmveghm4gr))
- After her departure, Gebru founded Dair, the Distributed AI Research Institute, to which she now devotes her working time. “We have people in the US and the EU, and in Africa,” she says. “We have social scientists, computer scientists, engineers, refugee advocates, labour organisers, activists … it’s a mix of people.” ([View Highlight](https://read.readwise.io/read/01h13ycyv5hp0njxrzw8ptpnbb))
- AI is not magic. There are a lot of people involved – *humans*.” ([View Highlight](https://read.readwise.io/read/01h13ydw156r87bnjg98sarv3r))
- “That conversation ascribes agency to a tool rather than the humans building the tool,” she says. “That means you can aggregate responsibility: ‘It’s not me that’s the problem. It’s the tool. It’s super-powerful. We don’t know what it’s going to do.’ Well, no – it’s *you* that’s the problem. You’re building something with certain characteristics for your profit. That’s extremely distracting, and it takes the attention away from real harms and things that we need to do. Right now.” ([View Highlight](https://read.readwise.io/read/01h13yh5142hka35244nvew2y8))

---
author: [[Nathan Benaich]]
title: "State of AI Report - 2024"
date: 2024-10-11
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/reader/parsed_document_assets/224919358/f74stvTNG_CBQ2Q7w88mZ2ptviLCOTYl_DSpUlyXjUY-cove_tIbOM2D.png)

## Metadata
- Author: [[Nathan Benaich]]
- Full Title: State of AI Report - 2024
- URL: https://readwise.io/reader/document_raw_content/224919358

## Highlights
- Research - Frontier lab performance converges, but OpenAI maintains its edge following the launch of o1, as planning and reasoning emerge as a major frontier.
  - Foundation models demonstrate their ability to break out of language as multimodal research drives into mathematics, biology, genomics, the physical sciences, and neuroscience.
  - US sanctions fail to stop Chinese (V)LLMs rising up community leaderboards. ([View Highlight](https://read.readwise.io/read/01j9xcy14s3nbehbhtstxnm3sc))
- Industry - NVIDIA remains the most powerful company in the world, enjoying a stint in the $3T club, while regulators probe the concentrations of power within GenAI.
  - More established GenAI companies bring in billions of dollars in revenue, while start-ups begin to gain traction in sectors like video and audio generation.
  Although companies begin to make the journey from model to product, long-term questions around pricing and sustainability remain unresolved.
  - Driven by a bull run in public markets, AI companies reach $9T in value, while investment levels grow healthily in private companies. ([View Highlight](https://read.readwise.io/read/01j9xcxwvxq6grdzjfg81qtdb6))
- Politics - While global governance efforts stall, national and regional AI regulation has continued to advance, with controversial legislation passing in the US and EU.
  - The reality of compute requirements forces Big Tech companies to reckon with real-world physical constraints on scaling and their own emissions targets.
  Meanwhile, governments’ own attempts to build capacity continue to lag.
  - Anticipated AI effects on elections, employment and a range of other sensitive areas are yet to be realized at any scale. ([View Highlight](https://read.readwise.io/read/01j9xcy4qmpc95r7xe5tfm6w3y))
- Safety - A vibe-shift from safety to acceleration takes place as companies that previously warned us about the pending extinction of humanity need to ramp up enterprise sales and usage of their consumer apps.
  - Governments around the world emulate the UK in building up state capacity around AI safety, launching institutes and studying critical national infrastructure for potential vulnerabilities.
  - Every proposed jailbreaking ‘ﬁx’ has failed, but researchers are increasingly concerned with more sophisticated, long-term attacks. ([View Highlight](https://read.readwise.io/read/01j9xcy8bdteka2wpdw0n2wskg))
- For much of the year, both benchmarks and community leaderboards pointed to a chasm between GPT-4 and ‘the best of the rest’. However, Claude 3.5 Sonnet, Gemini 1.5, and Grok 2 have all but eliminated this gap as model performance now begin to converge.
  ● On both formal benchmarks and vibes-based analysis, the best-funded frontier labs are able to rack up scores within low single digits of each other on individual capabilities.
  ● Models are now consistently highly capable coders, are strong at factual recall and math, but less good at open-ended question-answering and multi-modal problem solving.
  ● Many of the variations are sufﬁciently small that they are now likely to be the product of differences in implementation. For example, GPT-4o outperforms Claude 3.5 Sonnet on MMLU, but apparently underperforms it on MMLU-Pro - a benchmark designed to be more challenging.
  ● Considering the relatively subtle technical differences between architectures and likely heavy overlaps in pre-training data, model builders are now increasingly having to compete on new capabilities and product features. ([View Highlight](https://read.readwise.io/read/01j9xd007zc1xyc2wtw8z546tr))
- The OpenAI team had clearly clocked the potential of inference compute early, with OpenAI o1 appearing within weeks of papers from other labs exploring the technique.
  ● By shifting compute from pre- and post-training to inference, o1 reasons through complex prompts step-by-step in a chain-of-thought (COT) style, employing RL to sharpen the COT and the strategies it uses. This unlocks the possibility of solving multi-layered math, science, and coding problems where LLMs have historically struggled, due to the inherent limitations of next-token prediction.
  ● OpenAI report signiﬁcant improvements on reasoning-heavy benchmarks versus 4o, with the starkest on AIME 2024 (competition math), with a whopping score of 83.83 versus 13.4.
  ● However, this capability comes at a steep price: 1M input tokens of o1-preview costs $15, while 1M output tokens will set you back $60. This makes it 3-4x more expensive than GPT-4o.
  ● OpenAI is clear in its API documentation that it is not a like-for-like 4o replacement and that it is not the best model for tasks that require consistently quick responses, image inputs or function calling ([View Highlight](https://read.readwise.io/read/01j9xczjxmmygy353k9gkg6dcv))
- The community were quick to put o1 through its paces, ﬁnding that it performed signiﬁcantly better than other LLMs on certain logical problems and puzzles. Its true edge shone through, however, on complex math and science tasks, with a viral video of a PhD student reacting with astonishment as it reproduced a year of his PhD code in approximately an hour. However, the model remains weaker on certain kinds of spatial reasoning. Like its predecessors, it can’t play chess to save its life… yet. ([View Highlight](https://read.readwise.io/read/01j9xd0np036tnkk1yzffd3r3e))
- In April, Meta dropped the Llama 3 family, 3.1 in July, and 3.2 in September. Llama 3.1 405B, their largest to-date, is able to hold its own against GPT-4o and Claude 3.5 Sonnet across reasoning, math, multilingual, and long-context tasks. This marks the ﬁrst time an open model has closed the gap with the proprietary frontier.
  ● Meta stuck to the same decoder-only transformer architecture that it’s used since Llama 1, with minor adaptations, namely more transformer layers and attention heads.
  ● Meta used an incredible 15T tokens to train the family. While this blew through the “Chinchilla-optimal” amount of training compute, they found that both the 8B and 70B models improved log-linearly up to 15T.
  ● Llama 3.1 405B was trained over 16,000 H100 GPUs, the ﬁrst Llama model trained at this scale.
  ● Meta followed up with Llama 3.2 in September, which incorporated 11B and 90B VLMs (Llama’s multimodal debut).
  The former was competitive with Claude 3 Haiku, the latter with GPT-4o-mini. The company also released 1B and 3B text-only models, designed to operate on-device.
  ● Llama-based models have now racked up over 440M downloads on Hugging Face. ([View Highlight](https://read.readwise.io/read/01j9xd16bwzj95d09cnjv06fxy))
## New highlights added October 14, 2024 at 9:56 AM
- With open source commanding considerable community support and becoming a hot button regulatory issue, some researchers have suggested that the term is often used misleadingly. It can be used to lump together vastly different openness practices across weights, datasets, licensing, and access methods. ([View Highlight](https://read.readwise.io/read/01ja4z92tsw8bz0bxrc3ddycqf))
## New highlights added October 14, 2024 at 11:56 AM
- With new model families reporting incredibly strong benchmark performance straight out-of-the-gate, researchers have increasingly been shining a light on dataset contamination: when test or validation data leaks into the training set. Researchers from Scale retested a number of models on a new Grade School Math 1000 (GSM1k) that mirrors the style and complexity of the established GSM8k benchmark, ﬁnding signiﬁcant performance drops in some cases. ([View Highlight](https://read.readwise.io/read/01ja55rnw83t5rnhj5qh3magx5))
- A team from the University of Edinburgh ﬂagged up the number of mistakes in MMLU, including the wrong ground truth, unclear questions, and multiple correct answers. While low across most individual topics, there were big spikes in certain ﬁelds, such as virology, where 57% of the analyzed instances contained errors.
  ● On a manually corrected MMLU subset, models broadly gain in performance, although worsened on professional law and formal logic. This says inaccurate MMLU instances are being learned during pre-training.
  ● In more safety-critical territory, OpenAI has warned that SWE-bench, which evaluates models’ ability to solve real-world software issues, was underestimating the autonomous software engineering capabilities of models, as it contained tasks that were hard or impossible to solve.
  ● The researchers partnered with the creators of the benchmark to create SWE-bench Veriﬁed. ([View Highlight](https://read.readwise.io/read/01ja55s9dg3rjkdm9d991446my))
- The LMSYS Chatbot Arena Leaderboard has emerged as the community’s favorite method of formalizing evaluation by “vibes”. But as model performance improves, it’s beginning to produce counterintuitive results ● The arena, which allows users to interact with two randomly selected chatbots side-by-side provides a rough crowdsourced evaluation.
  ● However, controversially, this led to GPT-4o and GPT-4o Mini receiving the same scores, with the latter also outperforming Claude Sonnet 3.5.
  ● This has led to concerns that the ranking is essentially becoming a way of assessing which writing style users happen to prefer most.
  ● Additionally, as smaller models tend to perform less well on tasks involving more tokens, the 8k context limit arguably gives them an unfair advantage.
  ● However, the early version of the vision leaderboard is now beginning to gain traction and aligns better with other evals. ([View Highlight](https://read.readwise.io/read/01ja55t06nfw9f0r1t05szd0zq))
- Deﬁciencies in both reasoning capabilities and training data mean that AI systems have frequently fallen short on math and geometry problems. With AlphaGeometry, a symbolic deduction engine comes to the rescue.
  ● A Google DeepMind/NYU team generated millions of synthetic theorems and proofs using symbolic engines, using them to train a language model from scratch.
  ● AlphaGeometry alternates between the language model proposing new constructions and symbolic engines performing deductions until a solution is found.
  ● Impressively, It solved 25 out of 30 on a benchmark of Olympiad-level geometry problems, nearing human International Mathematical Olympiad gold medalist performance. The next best AI performance scored only 10.
  ● It also demonstrated generalisation capabilities - for example, ﬁnding that a speciﬁc detail in a 2004 IMO problem was unnecessary to for the proof. ([View Highlight](https://read.readwise.io/read/01ja55tjcvzgzsg6yzt3gwjjmk))
- Research suggests that models are robust in the face of deeper layers - which are meant to handle complex, abstract, or task-speciﬁc information - being pruned intelligently. Maybe it’s possible to go even further.
  ● A Meta/MIT team looking at open-weight pre-trained LLMs concluded that it’s possible to do away with up to half a model’s layers and suffer only negligible performance drops on question-answering benchmark.
  ● They identiﬁed optimal layers for removal based on similarity and then “healed” the model through small amounts of efﬁcient ﬁne-tuning.
  ● NVIDIA researchers took a more radical approach by pruning layers, neurons, attention heads, and embeddings, and then using knowledge distillation for efﬁcient retraining.
  ● The MINITRON models, derived from Nemotron-4 15B, achieved comparable or superior performance to models like Mistral 7B and Llama-3 8B while using up to 40x fewer training tokens. ([View Highlight](https://read.readwise.io/read/01ja55v3kkh09zqsbe1wdq4hh5))
- As Andrej Karpathy and others have argued, current large model sizes could be a reﬂection of inefﬁcient training. Using these big models to reﬁne and synthesize training data, could help train capable smaller models.
  ● Google have embraced this approach, distilling Gemini 1.5 Flash from Gemini 1.5 Pro, while Gemma 2 9B was distilled from Gemma 2 27B, and Gemma 2B from a larger unreleased model.
  ● There is also community speculation that Claude 3 Haiku, a highly capable smaller model, is a distilled version of the larger Opus, but Anthropic has never conﬁrmed this.
  ● These distillation efforts are going multimodal too. Black Forest Labs have released FLUX.1 dev, an open-weight text-to-image distilled from their Pro model.
  ● To support these efforts, the community has started to produce open-source distillation tools, like arcee.ai’s DistillKit, which supports both Logit-based and Hidden States-based distillation.
  ● Llama 3.1 405B is also being used for distillation, after Meta updated its terms so output logits can be used to improve any models, not just Llama ones.
  stateof.ai 2024 ([View Highlight](https://read.readwise.io/read/01ja55vpbpehzrqa0wd7zkzpwc))
- odels built for mobile compete with their larger peers #stateofai | 23 → As big tech companies think through large-scale end user deployment, we’re starting to see high-performing LLM and multimodal models that are small enough to run on smartphones.
  ● Microsoft’s phi-3.5-mini is a 3.8B LM that competes with larger models like 7B and Llama 3.1 8B. It performs well on reasoning and question-answering, but size restricts its factual knowledge. To enable on-device inference, the model was quantized to 4 bits, reducing its memory footprint to approximately 1.8GB.
  ● Apple introduced MobileCLIP, a family of efﬁcient image-text models optimized for fast inference on smartphones. Using novel multimodal reinforced training, they improve the accuracy of compact models by transferring knowledge from an image captioning model and an ensemble of strong CLIP encoders.
  ● Hugging Face also got in on the action with SmolLM, a family of small language models, available in 135M, 360M, and 1.7B formats. ([View Highlight](https://read.readwise.io/read/01ja55wsyqhw2m3p9qd51z64n9))
- It’s possible to shrink the memory requirements of LLMs by reducing the precision of their parameters. Researchers are increasingly managing to minimize the performance trade-offs.
  ● Microsoft’s BitNet uses a “BitLinear” layer to replace standard linear layers, employing 1-bit weights and quantized activations.
  ● It shows competitive performance compared to full-precision models and demonstrates a scaling law similar to full-precision transformers, with signiﬁcant memory and energy savings.
  ● Microsoft followed up with BitNet b1.58, with ternary weights to match full-precision LLM performance at 3B size while retaining efﬁciency gains.
  ● Meanwhile, ByteDance’s TiTok (Transformer-based 1-Dimensional Tokenizer) quantizes images into compact 1D sequences of discrete token for image reconstruction and generation tasks. This allows images to be represented with as few as 32 tokens, instead of hundreds or thousands. ([View Highlight](https://read.readwise.io/read/01ja55x945dhf1fpn07ehv39rm))
- ll representation ﬁne tuning unlock on-device personalization? #stateofai | 25 → Parameter-efﬁcient ﬁne-tuning (e.g. via LoRA) is nothing new, but Stanford researchers believe a more targeted approach offers greater efﬁciency and adaptation.
  ● Inspired by model interpretability research, ReFT (Representation Fine-tuning) doesn’t alter the model’s weights. Instead, it manipulates the model’s internal representations at inference time to steer its behavior.
  ● While it comes with a slight interference penalty, ReFT requires 15-65x fewer parameters compared to weight-based ﬁne-tuning methods.
  ● It also enables more selective interventions on speciﬁc layers and token positions, enabling ﬁne-grained control over the adaptation process.
  ● The researchers show its potential in few-shot adaptation where a chat model is given a new persona with just ﬁve examples. Combined with the small storage footprint for learned interventions, it could be used for real-time personalization on devices with sufﬁcient compute power ([View Highlight](https://read.readwise.io/read/01ja55ybq7m4h66xmvq557hesw))
- Models that combine attention and other mechanisms are able to maintain or even improve accuracy, while reducing computational costs and memory footprint.
  ● Selective state-space models like Mamba, designed last year to handle long sequences more efﬁciently, can to some extent compete with transformers, but lag on tasks that require copying or in-context learning. That said, Falcon’s Mamba 7B shows impressive benchmark performance versus similar-sized transformer models.
  ● Hybrid models appear to be a more promising direction. Combined with self-attention and MLP layers, the A121’s 8B Mamba-2-Hybrid outperforms the 8B Transformer across knowledge and reasoning benchmarks, while being up to 8x faster generating tokens in inference.
  ● In a nostalgia trip, there are early signs of a comeback for recurrent neural networks, which had fallen out of fashion due to training and scaling difﬁculties.
  ● Grifﬁn, trained by Google DeepMind, mixes linear recurrences and local attention, holding its own against Llama-2 while being trained on 6x fewer tokens. ([View Highlight](https://read.readwise.io/read/01ja55z2519nwsz8j0rs0611kx))
- By transferring knowledge from a larger, more powerful model, one could improve the performance of subquadratic models, allowing us to harness their efﬁciency on downstream tasks.
  ● MOHAWK is a new method for distilling knowledge from a large, pre-trained transformer model (teacher) to a smaller, subquadratic model (student) like a state-space model (SSM).
  ● It aligns i) the sequence transformation matrices of the student and teacher models ii) and the hidden states of each layer, then iii) transfers the remaining weights of the teacher model to the student model to ﬁnetune it.
  ● The authors create Phi-Mamba, a new student model combining Mamba-2 and an MLP block and a variant called Hybrid-Phi-Mamba that retains some attention layers from the teacher model.
  ● Mohawk can train Phi-Mamba and Hybrid-Phi-Mamba to achieve performance close to the teacher model. Phi-Mamba is distilled with only 3B tokens, less than 1% of the data used to train either the previously best-performing Mamba models and 2% for the Phi-1.5 model itself. ([View Highlight](https://read.readwise.io/read/01ja55znakc8rbtva806rnh8c6))
- the transformer continues to reign supreme (for now) #stateofai | 28 Work with transformer alternatives and hybrid models is interesting, but at this stage remains niche. One paradigm still seems to rule them all. ([View Highlight](https://read.readwise.io/read/01ja560fkcxmp8x7hjm2wz25d2))
- Synthetic data starts gaining more widespread adoption… #stateofai | 29 → Last year’s report pointed to the divides of opinion around synthetic data: with some ﬁnding it useful, others fearing its potential to trigger model collapse by compounding errors. Opinion seems to be warming.
  ● As well as being the main source of training data for the Phi family, synthetic data was used by Anthropic when training Claude 3 to help represent scenarios that might have been missing in the training data.
  ● Hugging Face used Mixtral-8x7B Instruct to generate over 30M ﬁles and 25B tokens of synthetic textbooks, blog posts, and stories to recreate the Phi-1.5 training dataset, which they dubbed Cosmopedia.
  ● To make this process easier, NVIDIA released the Nemotron-4-340B family, a suite of models designed speciﬁcally for synthetic data generation, available via a permissive license. Meta’s Llama can also be used for synthetic data generation.
  ● It also appears possible to create synthetic high-quality instruction data by extracting it directly from an aligned LLM, with techniques like Magpie. Models ﬁne-tuned this way sometimes perform comparably to Llama-3-8B-Instruct. ([View Highlight](https://read.readwise.io/read/01ja5610npg55hyw4p1hx3fz3w))
- antity of synthetic data that triggers these kinds of outcomes and if any mitigations work ● A Nature paper from Oxford and Cambridge researchers found model collapse occurs across various AI architectures, including ﬁne-tuned language models, challenging the idea that pre-training or periodic exposure to small amounts of original data can prevent degradation (measured by Perplexity score).
  ● This creates a “ﬁrst mover advantage”, as sustained access to diverse, human-generated data will become increasingly critical for maintaining model quality.
  ● However, these results are primarily focused on a scenario where real data is replaced with synthetic data over generations. In practise, real and synthetic data usually accumulates.
  ● Other research suggests that, provided the proportion of synthetic data doesn’t get too high, collapse can usually be avoided. ([View Highlight](https://read.readwise.io/read/01ja5629h3sz2s31c1mx75ywvb))
- Team Hugging Face built a 15T token dataset for LLM pre-training, using 96 CommonCrawl snapshots, which produces LLMs that outperform other open pre-training datasets. They also released an instruction manual.
  ● FineWeb, the dataset, was created through a multi-step process including base ﬁltering, independent MinHash deduplication per dump, selected ﬁlters derived from the C4 dataset, and the team’s custom ﬁlters.
  ● The text extraction using the traﬁlatura library produced higher quality data than default CommonCrawl WET ﬁles, even though the resulting dataset was meaningfully smaller.
  ● ● They found deduplication drove performance improvements, up to a point, before hitting a point of diminishing returns, and then worsening it.
  ● The team also used llama-3-70b-instruct to annotate 500k samples from FineWeb, scoring scoring each for their educational quality on a scale from 0 to 5. FineWeb-edu, which ﬁltered out samples scored below 3, outperformed FineWeb and all other open datasets, despite being signiﬁcantly smaller. ([View Highlight](https://read.readwise.io/read/01ja562v7h9208jyv82csyjcpf))
- While retrieval and embeddings are not new, growing interest in retrieval augmented generation (RAG) has prompted improvements in the quality of embedding models.
  ● Following the playbook that’s proven effective in regular LLMs, massive performance improvements have come from scale (GritLM has ~ 47B parameters vs the 110M common among prior embedding models).
  ● Similarly, the usage of broad web scale corpora and improved ﬁltering methods have led to large improvements in the smaller models.
  ● Meanwhile, ColPali is a vision-language embedding model that exploits the visual structure of documents, not just their text embeddings, to improve retrieval.
  ● Retrieval models are one of the few subdomains where open models commonly outperform proprietary models from the biggest labs. On the MTEB Retrieval Leaderboard, OpenAI’s embedding model ranks 29th, while NVIDIA’s open NV-Embed-v2 is top. ([View Highlight](https://read.readwise.io/read/01ja564dbv4jdv74qmj6dhn3d0))
- Traditional RAG solutions usually involve creating text snippets 256 tokens at a time with sliding windows (128 overlapping the prior chunk). This makes retrieval more efﬁcient, but signiﬁcantly less accurate.
  ● Anthropic solved this using ‘contextual embeddings’, where a prompt instructs the model to generate text explaining the context of each chunk in the document.
  ● They found that this approach leads to a reduction of top-20 retrieval failure rate of 35% (5.7% → 3.7%).
  ● It can then be scaled using Anthropic’s prompt caching.
  ● As Fernando Diaz of CMU observed in a recent thread, this is a great example of techniques pioneered on one area of AI research (e.g. early speech retrieval and document expansion work) being applied to another. Another version of “what is new, is old”.
  ● Research from Chroma shows that the choice of chunking strategy can affect retrieval performance by up to 9% in recall. ([View Highlight](https://read.readwise.io/read/01ja564q25tphang5regzx0rpn))
- Many commonly used RAG benchmarks are repurposed retrieval or question answering datasets. They don’t effectively evaluate the accuracy of citations, the importance of each piece of text to the overall answer, or the impact of conﬂicting points of information.
  ● Researchers are now pioneering novel approaches, like Ragnarök, which introduces a novel web-based arena for human evaluation through pairwise system comparisons. This addresses the challenge of assessing RAG quality beyond traditional automated metrics.
  ● Meanwhile, Researchy Questions provides a large-scale collection of complex, multi-faceted questions that require in-depth research and analysis to answer, drawn from real user queries. ([View Highlight](https://read.readwise.io/read/01ja565csh35vwak30k93mbt21))
- Data curation is an essential part of effective pre-training, but is often done manually and inefﬁciently. This is both hard to scale and wasteful, especially for multimodal models.
  ● Usually, an entire dataset is processed upfront, which doesn’t account for how the relevance of a training example can change over the course of learning. These methods are frequently applied before training, so cannot adapt to changing needs during training.
  ● Google DeepMind’s JEST selects entire batches of data jointly, rather than individual examples independently. The selection is guided by a ‘learnability score’ (determined by a pre-trained reference model) which evaluates how useful it will be for training. It’s able to integrate data selection directly into the training process, making it dynamic and adaptive.
  ● JEST uses lower-resolution image processing for both data selection and part of the training, signiﬁcantly reducing computational costs while maintaining performance beneﬁts. ([View Highlight](https://read.readwise.io/read/01ja5668y2zjkgmt6tm9wtr1x9))
- Models produced by DeepSeek, 01.AI, Zhipu AI, and Alibaba have achieved strong spots on the LMSYS leaderboard, displaying particularly impressive results in math and coding.
  ● The strongest models from Chinese labs are competitive with the second-most powerful tier of frontier model produced by US labs, while being challenging the SOTA on certain subtasks.
  ● These labs have prioritized computational efﬁciency to compensate for constraints around GPU access, learning to stretch their resources much further than their US peers.
  ● Chinese labs have different strengths. For example, DeepSeek has pioneered techniques like Multi-head Latent Attention to reduce memory requirements during inference and an enhanced MoE architecture.
  ● Meanwhile, 01.AI has focused less on architectural innovation and more on building a strong Chinese language dataset to compensate for its relative paucity in popular repositories like Common Crawl. ([View Highlight](https://read.readwise.io/read/01ja566p0kpb4e1xf1hftmr5ka))
- And Chinese open source projects win fans around the world #stateofai | 38 + → To drive international uptake and evaluation, Chinese labs have become enthusiastic open source contributors. A few models have emerged as strong contenders in individual sub-domains.
  ● DeepSeek has emerged as a community favorite on coding tasks, with deepseek-coder-v2 for its combination of speed, lightness, and accuracy.
  ● Alibaba released the Qwen-2 family recently, and the community has been particularly impressed by its vision capabilities, ranging from challenging OCR tasks to its ability to analyse complex art work.
  ● At the smaller end, the NLP lab at Tsinghua University has funded OpenBMB, a project that has spawned the MiniCPM project.
  ● These are small <2.5B parameter models that can run on-device. Their 2.8B vision model is only marginally behind GPT-4V on some metrics, while 8.5B Llama 3 based model surpasses it on some metrics.
  ● Tsinghua University’s Knowledge Engineering Group has also created CogVideoX - one of the most capable text to video models. ([View Highlight](https://read.readwise.io/read/01ja56769yccvwwbbkve3cn0a9))
- Moving on from diffusion models for text-to-image, Stability AI have continued to search for reﬁnements that increase quality while bringing about greater efﬁciency.
  ● Adversarial diffusion distillation speeds up image generation by reducing the sampling steps needed to create high-quality images from potentially hundreds down to 1-4, while maintaining high ﬁdelity.
  ● It combines adversarial training with score distillation: the model is trained just using a pre-trained diffusion model as a guide.
  ● As well as unlocking single-step generation, the authors focused on reducing computational complexity and improving sampling efﬁciency.
  ● Rectiﬁed ﬂow improves upon traditional diffusion methods by connecting data and noise through a direct, straight line, rather than a curved path.
  ● They combined this with a novel transformer-based architecture for text-to-image that allow for a bidirectional ﬂow of information between text and image components. This enhances the model's ability to generate more accurate and coherent high-resolution images based on textual descriptions. ([View Highlight](https://read.readwise.io/read/01ja56ggvef3cnzm8ejr9esjpj))
- Both Google DeepMind and OpenAI have given us sneak previews of highly powerful text-to-video diffusion models. But access remains heavily gated and neither has supplied much technical detail.
  ● OpenAI’s Sora is able to generate videos up to a minute long, while maintaining 3D consistency, object permanence, and high resolution. It uses spacetime patches, similar to the tokens used in transformer models, but for visual content, to learn efﬁciently from a vast dataset of videos.
  ● Sora was also trained on visual data in its native size and aspect ratio, removing the usual cropping and resizing that reduces quality.
  ● Google DeepMind’s Veo combines text and optional image prompts with a noisy compressed video input, processing them through encoders and a latent diffusion model to create a unique compressed video representation.
  ● The system then decodes this representation into a ﬁnal high-resolution video.
  ● Also in the ﬁght are Runway’s Gen-3 Alpha, Luma’s Dream Machine, and Kling by Kuaishou. ([View Highlight](https://read.readwise.io/read/01ja56hb7g68b6pwjfmjp2387f))
- Keeping the gated approach of other labs, Meta has brought together its work on different modalities via the Make-A-Scene and Llama families to build Movie Gen.
  ● The core of Movie Gen is a 30B video generation and a 13B audio generation model, capable of producing 16-second videos at 16 frames per second and 45-second audio clips respectively.
  ● These models leverage joint optimization techniques for text-to-image and text-to-video tasks, as well as novel audio extension methods for generating coherent audio for videos of arbitrary lengths.
  ● Movie Gen's video editing capabilities combine advanced image editing techniques with video generation, allowing for both localized edits and global changes while preserving original content.
  ● The models were trained on a combination of licensed and publicly available datasets.
  ● Meta used A/B human evaluation comparisons to demonstrate positive net win rates against competing industry models across their four main capabilities. The researchers say they intend to make the model available in future, but don’t commit to a timeline or release strategy. ([View Highlight](https://read.readwise.io/read/01ja56hnr1x7apakv6r7a184qw))
- In a sign that AI has truly come of age as both a scientiﬁc discipline and a tool to accelerate science, the Royal Swedish Academy of Sciences awarded Nobel Prizes to OG pioneers in deep learning, alongside the architects of its best-known application (so far) in science. The news was celebrated by the entire ﬁeld. ([View Highlight](https://read.readwise.io/read/01ja56j1j0fp6ygweawrd203t0))
- DeepMind and Isomorphic Labs released AlphaFold 3, their successor from AF2, which can now model how small molecule drugs, DNAs, RNAs and antibodies interact with protein targets.
  ● There were substantial and surprising algorithmic changes from AF2: all equivariance constraints were removed in favor of simplicity and scale, while the Structure Module was replaced with a diffusion model to build the 3D coordinates.
  ● Unsurprisingly, the researchers claim that AF3 performs exceptionally well in comparison to other methods (esp. for small molecule docking), although this was not compared to stronger baselines.
  ● Notably, no open-source code was made available (yet).
  Several independent groups are working on reproducing the work openly. ([View Highlight](https://read.readwise.io/read/01ja56jgbcc3e40vb9d13k46f1))
- The decision to not release code for the AF3 publication was highly controversial, with many blaming Nature. Politics aside, there has been a race by start-ups and AI communities to make their model the go-to alternative.
  ● The ﬁrst horse out of the gate was Baidu with their HelixFold3 model, which was comparable to AF3 for ligand binding. They provide a web server and their code is fully open-sourced for non-commercial use.
  ● Chai-1 from Chai Discovery (backed by OpenAI) recently released a molecular structure prediction model that has taken off in popularity due to its performance and high quality implementation. The web server is also available for commercial drug discovery use.
  ● We are still waiting for a fully open-sourced model with no restrictions (e.g. using outputs for training of other models). ● Will DeepMind fully release AF3 sooner if they begin to fear alternative models are becoming the communities favourite? ([View Highlight](https://read.readwise.io/read/01ja56jqw1fjxzfk7v32dep3n7))
- The secretive protein design team at DeepMind ﬁnally “came out of stealth” with their ﬁrst model AlphaProteo, a generative model that is able to design sub-nanomolar protein binders with 3- to 300-fold better afﬁnities.
  ● While few technical details were given, it seems it was built on top of AlphaFold3 and is likely a diffusion model. ‘Hotspots’ on the target epitope can also be speciﬁed.
  ● The model was able to design protein binders with 3- to 300-fold better binding afﬁnities than previous works (e.g. RFDiffusion).
  ● The “dirty secret” of the protein design ﬁeld is that the in silico ﬁltering is just as (if not more) important than the generative modelling, with the paper suggesting that AF3-based scoring is key.
  ● They also use their conﬁdence metrics to screen a large number of possible novel targets for which future protein binders could be designed. ([View Highlight](https://read.readwise.io/read/01ja56k5776b8kdfd21x1y7f2d))
- The Bitter Lesson: Equivariance is dead…long live equivariance! #stateofai | 48 → Equivariance is the idea of giving a model the inductive biases to natively handle rotations, translations and (sometimes) reﬂections. It has been at the core of Geometric Deep Learning and biomolecular modelling research since AlphaFold 2. However, recent works by top labs have questioned the existing mantra.
  ● The ﬁrst shots were ﬁred by Apple, with a paper that obtained SOTA results on predicting the 3D structures of small molecules using a non-equivariant diffusion model with a transformer encoder.
  ● Remarkably, the authors showed that using the domain-agnostic model did not deleteriously impact generalization and was consistently able to outperform specialist models (assuming sufﬁcient scale was used). ● Next was AlphaFold 3, which infamously dropped all the equivariance and frames constraints from the previous model in favour of another diffusion process coupled with augmentations and, of course, scale.
  ● Regardless, the greatly improved training efﬁciency of equivariant models means the practice is likely to stay for a while (at least for academic groups working on large systems such as proteins). ([View Highlight](https://read.readwise.io/read/01ja56kqaj9wk36br1wc2fg0aw))
- Since 2019, Meta had been publishing transformer-based language models (Evolutionary Scale Models) trained on large-scale amino acid and protein databases. When Meta terminated these efforts in 2023, the team founded EvolutionaryScale. This year, they released ESM3, a frontier multimodal generative model that was trained over sequences, structures and functions of proteins rather than sequences alone.
  ● The model is a bidirectional transformer that fuses tokens that represent each of the three modalities as separate tracks into a single latent space.
  ● Unlike traditional masked language modelling, ESM3’s training process uses a variable masking schedule, exposing the model to diverse combinations of masked sequence, structure, and function. ESM3 learns to predict completions for any combination of modalities.
  ● ESM3 was prompted to generate new green ﬂuorescent proteins (GFP) with low sequence similarity to known ones. ([View Highlight](https://read.readwise.io/read/01ja56m4bx2myhpcnkq30pnskp))
- The fundamental problem with research at the intersection of biology and ML is that there are very few people with the skills to both train a frontier model and give it a rigorous biological appraisal.
  ● Two works from late 2023, PoseCheck and PoseBusters, showed that ML models for molecule generation and protein-ligand docking gave structures (poses) with gross physical violations.
  ● Even the AlphaFold3 paper didn’t get away without a few bruises when a small start-up showed that using a slightly more advanced conventional docking pipeline beat AF3.
  ● A new industry consortium led by Valence Labs, including major pharma companies (i.e. Recursion, Relay, Merck, Novartis J&J, Pﬁzer), is developing Polaris, a benchmarking platform for AI-driven drug discovery. Polaris will provide high-quality datasets, facilitate evaluations, and certify benchmarks.
  ● Meanwhile, Recursion’s work on perturbative map-building led them to create a new set of benchmarks and metrics. ([View Highlight](https://read.readwise.io/read/01ja56mjxd347we6p6rc12na33))
- To determine the properties of physical materials and how they behave under reactions, it is necessary to run atomic-scale simulations that today rely on density functional theory. This method is powerful, but slow and computational expensive. While faster, alternative approaches that calculate force ﬁelds (interatomic potentials) tend to have insufﬁcient accuracy to be useful, particularly for reactive events and phase transitions.
  ● In 2022, equivariant message passing neural networks (MPNN) combined with efﬁcient many-body messages (MACE) were introduced at NeurIPS.
  ● Now, the authors present MACE-MP-0, which uses the MACE architecture and is trained on the Materials Project Trajectory dataset, which contains millions of structures, energies, magnetic moments, forces and stresses.
  ● The model reduces the number of message passing layers to two by considering interactions involving four atoms simultaneously, and it only uses nonlinear activations in selective parts of the network.
  ● It is capable of molecular dynamics simulation across a wide variety of chemistries in the solid, liquid and gaseous phases. ([View Highlight](https://read.readwise.io/read/01ja56n148ksn1stk7kv34pre3))
- Deep learning, originally inspired by neuroscience, is now making into modelling the brain itself. BrainLM is a foundation model built on 6,700 hours of human brain activity recordings generated by functional magnetic resonance imaging (fMRI), which detects changes in blood oxygenation (left ﬁgure). The model learns to reconstruct masked spatiotemporal brain activity sequences and, importantly, it can generalise to held-out distributions (right ﬁgure). This model can be ﬁne-tuned to predict clinical variables e.g. age, neuroticism, PTSD, and anxiety disorder scores better than a graph convolutional model or an LSTM. ([View Highlight](https://read.readwise.io/read/01ja56nj9647h3wjyfbdv85q1t))
- Classical atmospheric simulation methods like numerical weather prediction are costly and unable to make use of diverse and often scarce atmospheric data modalities. But, foundation models are well suited here. Microsoft researchers created Aurora, a foundation model that produces forecasts for a wide range of atmospheric forecasting problems such as global air pollution and high-resolution medium-term weather patterns. It can also adapt to new tasks by making use of a general-purpose learned representation of atmospheric dynamics. ([View Highlight](https://read.readwise.io/read/01ja56nyn7kkazqdejgqm72fch))
- Foundation models for the mind: reconstructing what you see #stateofai | 56 + → MindEye2, is a generative model that maps fMRI activity to a rich CLIP space from which images of what the individual sees are reconstructed using a ﬁne-tuned Stable Diffusion XL. The model is trained on the Natural Scenes Dataset, an fMRI dataset built from 8 subjects whose brain responses were captured for 30-40 hours as they looked at hundreds of rich naturalistic stimuli from the COCO dataset scanning sessions for 3 seconds each. ([View Highlight](https://read.readwise.io/read/01ja56pcfcx4h0q6rkbvqvwxk1))
- Decoding speech from brain recordings with implantable microelectrodes could enable communication for patients with impaired speech. In a recent case, a 45-year-old man with amyotrophic lateral sclerosis (ALS) with tetraparesis and severe motor speech damage underwent surgery to implant microelectrodes into his brain. The arrays recorded neural activity as the patient spoke in both prompted and unstructured conversational settings. At ﬁrst, cortical neural activity was decoded into a small vocabulary of 50 words with 99.6% accuracy by predicting the most likely English phoneme being attempted. Sequences of phonemes were combined into words using an RNN, before moving to a larger 125,000-word vocabulary enabled by further training. ([View Highlight](https://read.readwise.io/read/01ja56q58mqr4x2b0sfee91q66))
- François Chollet, the creator of Keras, has partnered with Zapier co-founder Mike Knoop to launch the ARC prize, offering a $1M prize fund for teams that make signiﬁcant progress on the ARC-AGI benchmark ● Chollet created the benchmark back in 2019 as a means of measuring models’ ability to generalize, focusing on tasks that are easier for humans and hard for AI. The tasks require minimal prior knowledge and emphasise visual problem-solving and puzzle-like tasks to make it resistant to memorization.
  ● Historically, LLMs have performed poorly on the benchmark, with performance peaking at about 34%.
  ● Chollet is sceptical of LLMs’ ability to generalize to new problems outside of their training data and is hoping the prize will encourage new research directions that will lead to a more human-like form of intelligence.
  ● The highest score so far is 46 (short of the 85 target). It’s been achieved by the Minds AI team, who have used an LLM-based approach, employing active inference, ﬁne-tuning the LLM on test task examples and expanding it with synthetic examples to improve performance. ([View Highlight](https://read.readwise.io/read/01ja56qf9wp5ppf9kha1gp5qtt))
- On novel tasks, where LLMs are unable to rely on memory and retrieval, performance often degrades. This suggests that they still often struggle to generalize beyond familiar patterns without external help.
  ● Even advanced LLMs like GPT-4 have difﬁculty reliably simulating state transitions in text-based games, especially for environment-driven changes. Their inability to consistently grasp causality, physics, and object permanence, makes them poor world-modellers, even on relatively straightforward tasks.
  ● Researchers found that LLMs accurately predict direct action results, like a sink turning on, around 77% of the time, but struggle with environmental effects, such as water ﬁlling a cup in the sink, achieving only 50% accuracy for these indirect changes.
  ● Other research evaluated LLMs on planning domains, including Blocksworld, and Logistics. GPT-4 produced executable plans 12% of the time. However, using iterative prompting with external veriﬁcation, Blocksworld plans hit 82% accuracy and Logistics plans 70% accuracy after 15 rounds of feedback.
  When re-run with o1, performance jumped but was still far from perfect. ([View Highlight](https://read.readwise.io/read/01ja56r2czwc39p16kpghj7hrq))
- Researchers are exploring methods to generate stronger internal reasoning processes, variously targeting both training and inference. The latter approach appears to underpin OpenAI o1’s jump in capabilities.
  ● Quiet-STaR from a joint Stanford-Notbad AI team generates internal rationales during pre-training, using a parallel sampling algorithm and custom meta-tokens to mark the beginning and end of these "thoughts." ● The approach employs a reinforcement learning-inspired technique to optimize the usefulness of generated rationales, rewarding those that improve the model's ability to predict future tokens.
  ● Meanwhile, Google DeepMind have targeted inference, showing that for many types of problems, strategically applying more computation at test time can be more effective than using a much larger pre-trained model. ● A Stanford/Oxford team have also looked at scaling inference compute, ﬁnding that repeated sampling can signiﬁcantly improve coverage. They suggest that using weaker and cheaper models with many attempts can outperform single attempts from their stronger and more expensive peers. ([View Highlight](https://read.readwise.io/read/01ja56rry2p84p7h4690k0j47w))
- Open-endedness gathers momentum as a promising research direction #stateofai | 61 → One path to improving the robustness of LLM reasoning is to embrace an open-ended approach such that they’re capable of generating new knowledge.
  ● In a position paper, a Google DeepMind team framed open-ended systems as able to “continuously generate artifacts that are novel and learnable to an observer”.
  ● They outline potential paths towards open-ended foundation models, including reinforcement learning, self-improvement, task generation, and evolutionary algorithms.
  ● On the self-improvement front, we saw STRATEGIST, a method for allowing LLMs to learn new skills for multi-agent games.
  ● The researchers used a bi-level tree search approach, combining high-level strategic learning with low-level simulated self-play for feedback. It outperformed RL and other LLM-based approaches on Game of Pure Strategy and The Resistance: Avalon at action planning and dialogue generatio ([View Highlight](https://read.readwise.io/read/01ja56s117fmbsg68242spw13v))
- After prolonged training beyond the point of overﬁtting (known as grokking), some researchers have argued that transformers learn to reason over parametric knowledge through composition and comparison tasks.
  ● Researchers at Ohio State University argued that a fully grokked transformer outperformed then SOTA models like GPT-4-Turbo and Gemini-1.5-Pro on complex reasoning tasks with a large search space.
  ● They conducted mechanistic analyses to understand the internal workings of the models during grokking, revealing distinct generalizing circuits for different tasks.
  ● However, they found that while fully grokked models performed well on comparison tasks (e.g. comparing attributes based on atomic facts), they were less good at out-of-distribution generalization in composition tasks.
  ● This raises questions about whether these are really meaningful reasoning capabilities versus memorization by another name, although the researchers believe that enhancing the transformer with better cross-layer memory sharing could resolve this. ([View Highlight](https://read.readwise.io/read/01ja56tyg2hvm6c6qqaz55vkzj))
- For agents to be useful, they need to be robust to real-word stochasticity, which SOTA models have historically struggled with. We’re beginning to see signs of progress.
  ● DigiRL is a novel autonomous reinforcement learning approach for training in-the-wild device control agents speciﬁcally for Android devices. The method involves a two-stage process: ofﬂine reinforcement learning followed by ofﬂine-to-online reinforcement learning. ([View Highlight](https://read.readwise.io/read/01ja56veatzfrkynnvte1wfmxc))
- To improve planning, approaches like MCTS, which helped to power AlphaGo, are slowly returning to the fore. Early results are promising, but will they be enough?
  ● MultiOn and Stanford combined an LLM with MCTS, along with a self-criticism mechanism and direct preference optimization, to learn from different success and failure criteria.
  ● They found this improved Llama-3 70B’s zero-shot performance from 18.6% to 81.7% in real-world booking scenarios, after a day of data collection, and up to 95.4% with online search.
  ● The longer-term question will be whether next-token prediction loss is too ﬁne-grained.
  ● This risks limiting the ability of RL and MCTS to achieve agentic behavior by focusing too much on individual tokens and hindering the exploration of broader, more strategic solutions. ([View Highlight](https://read.readwise.io/read/01ja56w1wn3fmzmgzaseftxbce))
- One of the big bottlenecks for training RL agents is a shortage of training data. Standard approaches like converting pre-existing environments (e.g. Atari) or manually building them are labor-intensive and don’t scale.
  ● Genie (winner of a Best Paper award at ICML 2024) is a world model that can generate action-controllable virtual worlds. It analyzed 30,000 hours of video game footage from 2D platformer games, learning to compress the visual information and infer the actions that drive changes between frames.
  ● By learning a latent action space from video data, it can handle action representations without requiring explicit action labels , which distinguishes it from other world models.
  ● Genie is both able to imagine entirely new interactive scenes and demonstrate signiﬁcant ﬂexibility: it can take prompts in various forms, from text descriptions to hand-drawn sketches, and bring them to life as playable environments.
  ● This approach demonstrated applicability beyond games, with the team successfully applying the hyperparameters from the game model to robotics data, without ﬁne tuning. ([View Highlight](https://read.readwise.io/read/01ja56wxk4g1bm348z071zwp5n))
- New lab Sakana AI has been focused on attempting to enhance the creative capabilities of current frontier models. One of their ﬁrst papers looks at using foundation models to automate research itself.
  ● The AI Scientist is an end-to-end framework designed to automate the generation of research ideas, implementation, and the production of research papers.
  ● After being given a starting template, it brainstorms novel research directions, before executing the experiments, and writing them up. The researchers claim their LLM-powered reviewer evaluates the generated papers with near-human accuracy.
  ● The researchers used it to generate example papers about diffusion, language modeling, and grokking. These were convincing at ﬁrst glimpse, but contained some ﬂaws on closer examination.
  ● Yet, the system periodically displayed signs of unsafe behavior, e.g. importing unfamiliar Python libraries and editing code to extend experiment timelin ([View Highlight](https://read.readwise.io/read/01ja56xncvan9m53j6jypch8h2))
- Meta’s TestGen-LLM combines multiple LLMs, prompts and conﬁgurations to leverage different models’ strengths to improve unit testing coverage for Android code on Instagram and Facebook.
  ● It uses an "assured" approach, ﬁltering generated tests to ensure they build successfully, pass reliably, and increase coverage before recommending them. This is the ﬁrst large-scale industrial deployment of an approach that combines LLMs with veriﬁable guarantees of code improvement, addressing concerns about LLM hallucinations and reliability in a software engineering context.
  ● In deployment, TestGen-LLM improved about 10% of test classes it was applied to, with 73% of its recommendations accepted by developers. ([View Highlight](https://read.readwise.io/read/01ja56xxfkvjp226vkwtczww51))
- Wayve’s LINGO-2 is the second generation of its vision-language-action model, that, unlike its predecessor, can both generate real-time driving commentary and control a car, linking language explanations directly with decision-making and actions. Meanwhile, the company is using generative models to enhance its simulator with more real-world detail. PRISM-1 creates realistic 4D simulations of dynamic driving scenarios using only camera inputs. It enables more effective testing and training by accurately reconstructing complex urban environments, including moving elements like pedestrians, cyclists, and vehicles, without relying on LiDAR or 3D bounding boxes. ([View Highlight](https://read.readwise.io/read/01ja56ydqtctt26wx6y4y39c17))
- Despite all eyes being on Gemini, the Google DeepMind team has steadily been increasing its robotics output, improving the efﬁciency, adaptability, and data collection of robots.
  ● The team created AutoRT, a system that uses a VLM for environmental understanding and an LLM to suggest a list of creative tasks the robot could carry out. These models are then combined with a robot control policy.
  This helps to scale up deployment quickly in previously unseen environments.
  ● RT-Trajectory enhances robotic learning through video input. For each video in the dataset of demonstrations, a 2D sketch of the gripper performing the task is overlaid. This provide practical visual hits to the model as it learns.
  ● The team have also improved the efﬁciency of transformers. SARA-RT is a novel ‘up-training’ method to convert pre-trained or ﬁne-tuned robotic policies from quadratic to linear attention, while maintaining quality.
  ● Researchers have found Gemini 1.5 Pro’s multimodal capabilities and long context window makes it an effective way of interacting with robots via natural language. ([View Highlight](https://read.readwise.io/read/01ja56zv3es950vhfm2hqenv8k))
- Historically, robotics had signiﬁcantly fewer open source datasets, tools, and libraries than other areas of AI - creating an artiﬁcially high barrier to entry. Hugging Face’s LeRobot aims to bridge the gap, hosting pretrained models, datasets with human-collected demonstrations, and pre-trained demonstrations. And the community’s loving it. ([View Highlight](https://read.readwise.io/read/01ja570azx1qtyg26ntjq25v4z))
- While consumer demand for the Vision Pro lacklustre so far, it’s taking robotics research by storm, where its high-res, advanced tracking, and processing power is being leveraged by researchers working on teleoperation - controlling robot movements and actions at a distance. Systems like Open-TeleVision and Bunny-Vision Pro use it to help enable precise control of multi-ﬁnger robotic hands (at a 3000 mile distance in the case of the former), demonstrating improved performance on complex manipulation tasks compared to previous approaches. They address challenges such as real-time control, safety through collision avoidance, and effective bimanual coordination. ([View Highlight](https://read.readwise.io/read/01ja571qfv9gydhrw2ct1wyk52))
- ast year, a non-ﬁnetuned GPT-4 via one API call was highly competitive with Google’s Med-PaLM2 on certain medical knowledge benchmarks. Gemini has ridden to the rescue.
  ● The Med-Gemini family of multimodal models for medicine are ﬁnetuned from Gemini Pro 1.0 and 1.5 using various medical datasets and incorporate web search for up-to-date information. They achieved SOTA 91.1% accuracy on MedQA, surpassing GPT-4.
  ● For multimodal tasks (e.g. in radiology and pathology), Med-Gemini set a new SOTA on 5 out of 7 datasets.
  ● When quality errors in questions were ﬁxed, model performance improved and it exhibited strong reason across other benchmarks. It also achieved high precision and recall in retrieving rare ﬁndings in lengthy EHRs - a challenging "needle-in-a-haystack" task.
  ● In a preliminary study, clinicians rated Med-Gemini's outputs equal or better than human-written examples in most cases. ([View Highlight](https://read.readwise.io/read/01ja57233gcx70r2pptje86qvv))
- terprise automation set to get an AI-ﬁrst upgrade #stateofai | 83 Traditional Robotic Process Automation (RPA), embodied by UiPath, has struggled with high set-up costs, brittle execution, and burdensome maintenance. Two novel approaches, FlowMind (JP Morgan) and ECLAIR (Stanford), use foundation models to address these limitations. FlowMind focuses on ﬁnancial workﬂows, using LLMs to generate executable workﬂows via APIs. In experiments on the NCEN-QA dataset, FlowMind achieved 99.5% accuracy in workﬂow understanding. ECLAIR takes a broader approach, using multimodal models to learn from demonstrations and interact directly with graphical user interfaces across various enterprise settings. On web navigation tasks, ECLAIR improved completion rates from 0% to 40%. ([View Highlight](https://read.readwise.io/read/01ja573fpk4wekhqwqgyznsedh))
- As AI emerges as the new competitive battleground, big tech companies begin to hold more details of their work close to their chest. Frontier labs have meaningfully cut publication levels for the ﬁrst time since this report began, while academia gets into gear. ([View Highlight](https://read.readwise.io/read/01ja5744nzms4hwb3gcnzx7vmy))
- Amid growing demand for its hardware to power demanding gen AI workloads, every major lab depends on NVIDIA for its hardware. Its market cap hit $3T in June, only the third US company to reach this milestone (following Microsoft and Apple). Following blowout earnings in Q2, its position looks as unassailable as ever. ([View Highlight](https://read.readwise.io/read/01ja574tb0wb023ez6hs4neh5b))
- NVIDIA has already booked signiﬁcant pre-sales on its new Blackwell family of GPUs and is making a strong play for governments.
  ● The new Blackwell B200 GPU and GB200 Superchip promise signiﬁcant performance gain over the Hopper architecture of H100 fame. NVIDIA claims it can reduce cost and energy consumption 25x over an H100. In a mark of NVIDIA’s power, every major AI lab CEO provided a supporting quote in the press release.
  ● While the Blackwell architecture was delayed by manufacturing issues, the company is still conﬁdent of booking several billion in revenue from it by the end of the year.
  ● Jensen Huang, NVIDIA’s Founder and CEO is expanding the pitch, outlining the company’s vision of sovereign AI.
  ● He has argued that every government needs to build its own LLM to preserve its national heritage. ([View Highlight](https://read.readwise.io/read/01ja575fgcwx43wrma2bjcay57))
- AMD and Intel have started to invest in their software ecosystems, while AMD has made a heavy pitch to the open source community using ROCm (its CUDA competitor). However, they are yet to develop compelling alternatives to NVIDIA’s portfolio of networking solutions. AMD is hoping its planned $4.9B acquisition of server builder ZT Systems will change this. Meanwhile, Intel has seen its hardware sales decline. Short of regulatory intervention, a change in research paradigm or supply constraints, NVIDIA’s position seems unassailable. ([View Highlight](https://read.readwise.io/read/01ja575tbmz1agacfz14zvanyj))
- We looked at the $6B invested in AI chip challengers since 2016 and asked what would have happened if investors had just bought the equivalent amount of NVIDIA stock at that day’s price. The answer is lime green: that $6B would be worth $120B of NVIDIA stock today (20x!) vs. the $31B (5x) in its startup contenders. ([View Highlight](https://read.readwise.io/read/01ja5765c0wvx6mqxmc0xtz13x))
- A vocal minority of analysts and commentators aren’t convinced. They point to the decline in GPU scarcity, how only a few companies are currently generating reliable revenue from AI-ﬁrst offerings, and how even Big Tech’s infrastructure build-out is unlikely to be big enough to justify the company’s current valuation. The market is currently ignoring these voices and seems more inclined to agree with early Tesla investor James Anderson’s view that the company could be worth “double-digit trillions” in a decade. ([View Highlight](https://read.readwise.io/read/01ja576qxht7k5zx3xe3kd84va))

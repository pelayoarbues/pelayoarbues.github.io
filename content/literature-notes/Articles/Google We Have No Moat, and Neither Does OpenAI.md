---
author: [[Dylan Patel]]
title: "Google &#34;We Have No Moat, and Neither Does OpenAI&#34;"
tags: 
- articles
- literature-note
---
# Google "We Have No Moat, and Neither Does OpenAI"

![rw-book-cover](https://substackcdn.com/image/fetch/w_1200,h_600,c_limit,f_jpg,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F241fe3ef-3919-4a63-9c68-9e2e77cc2fc0_1366x588.png)

## Metadata
- Author: [[Dylan Patel]]
- Full Title: Google "We Have No Moat, and Neither Does OpenAI"
- Category: #articles
- URL: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither

## Highlights
- But the uncomfortable truth is, *we aren’t positioned to win this arms race and neither is OpenAI* ([View Highlight](https://read.readwise.io/read/01gzmabbz913e4xn7tvdf29m6k))
- I’m talking, of course, about open source. Plainly put, they are lapping us. **Things we consider “major open problems” are solved and in people’s hands today.** ([View Highlight](https://read.readwise.io/read/01gzmabn1yc9x5x5vbjy5584wn))
- Open-source models are faster, more customizable, more private, and pound-for-pound more capable. ([View Highlight](https://read.readwise.io/read/01gzmacypenr39fwh1d9fk7tkm))
- **We have no secret sauce.** Our best hope is to learn from and collaborate with what others are doing outside Google ([View Highlight](https://read.readwise.io/read/01gzmad71navvsyycx300ryvt4))
- People will not pay for a restricted model when free, unrestricted alternatives are comparable in quality ([View Highlight](https://read.readwise.io/read/01gzmaddjhphxed7tpxpmjsyzm))
- **Giant models are slowing us down.** In the long run, the best models are the ones
  which can be iterated upon quickly. We should make small variants more than an afterthought, now that we know what is possible in the <20B parameter regime ([View Highlight](https://read.readwise.io/read/01gzmadqmhdhqvwcvgnfffk60t))
- low-cost public involvement was enabled by a vastly cheaper mechanism for fine tuning called [low rank adaptation](https://arxiv.org/abs/2106.09685), or LoRA, combined with a significant breakthrough in scale ([latent diffusion](https://arxiv.org/abs/2112.10752) for image synthesis, [Chinchilla](https://arxiv.org/abs/2203.15556) for LLMs ([View Highlight](https://read.readwise.io/read/01gzmafmb1wwymh6w46gt1mkcx))
- [LoRA](https://arxiv.org/abs/2106.09685) works by representing model updates as low-rank factorizations, which reduces the size of the update matrices by a factor of up to several thousand. This allows model fine-tuning at a fraction of the cost and time ([View Highlight](https://read.readwise.io/read/01gzmagr3p25n5nvygpjh0k914))
- Part of what makes LoRA so effective is that - like other forms of fine-tuning - it’s stackable. Improvements like instruction tuning can be applied and then leveraged as other contributors add on dialogue, or reasoning, or tool use ([View Highlight](https://read.readwise.io/read/01gzmahfnjxcbxhswb2e5amaa7))
- means that as new and better datasets and tasks become available, the model can be cheaply kept up to date, without ever having to pay the cost of a full run. ([View Highlight](https://read.readwise.io/read/01gzmahypg44xhagtpgq9sv3f0))
- contrast, training giant models from scratch not only throws away the pretraining, but also any iterative improvements that have been made on top. In the open source world, it doesn’t take long before these improvements dominate, making a full retrain extremely costly ([View Highlight](https://read.readwise.io/read/01gzmaj6053r4f5bg6jy8qdt6t))

---
author: [[Garrison Lovely]]
title: "Can Humanity Survive AI?"
date: 2024-02-11
tags: 
- articles
- literature-note
---
![rw-book-cover](https://alcove.website/system/cache/accounts/avatars/111/111/905/299/721/945/original/1adc66a38a882e04.jpg)

## Metadata
- Author: [[Garrison Lovely]]
- Full Title: Can Humanity Survive AI?
- URL: https://jacobin.com/2024/01/can-humanity-survive-ai/

## Highlights
- Google cofounder Larry Page [thinks](https://www.vanityfair.com/news/2023/09/artificial-intelligence-industry-future#:~:text=While%20Page%20stays,soon%20as%20possible.”) superintelligent AI is “just the next step in evolution.” In fact, Page, who’s worth about $120 billion, has reportedly [argued](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html?searchResultPosition=8) that efforts to prevent AI-driven extinction and protect human consciousness are “speciesist” and “[sentimental nonsense](https://time.com/6310076/elon-musk-ai-walter-isaacson-biography/).” ([View Highlight](https://read.readwise.io/read/01hp7njdf2gsbrzthjeq3mznt2))
- In July, former Google DeepMind senior scientist Richard Sutton — one of the pioneers of reinforcement learning, a major subfield of AI — [said](https://www.youtube.com/watch?v=NgHFMolXs3U) that the technology “could displace us from existence,” and that “we should not resist succession.” In a [2015 talk](https://www.youtube.com/watch?si=sQu-xCb-Agzj0wCR&t=1851&v=3l2frDNINog&feature=youtu.be), Sutton said, suppose “everything fails” and AI “kill[s] us all”; he asked, “Is it so bad that humans are not the final form of intelligent life in the universe?” ([View Highlight](https://read.readwise.io/read/01hp7njvqwdm97470ypgn53eg8))
- “Biological extinction, that’s not the point,” Sutton, sixty-six, told me. “The light of humanity and our understanding, our intelligence — our consciousness, if you will — can go on without meat humans.” ([View Highlight](https://read.readwise.io/read/01hp7nk5cymyw8v52wvhq51pxr))
- Yoshua Bengio, fifty-nine, is the [second-most cited](https://scholar.google.com/citations?user=kukA0LcAAAAJ) living scientist, noted for his foundational work on deep learning. Responding to Page and Sutton, Bengio told me, “What they want, I think it’s playing dice with humanity’s future. I personally think this should be criminalized.” A bit surprised, I asked what exactly he wanted outlawed, and he said efforts to build “AI systems that could overpower us and have their own self-interest by design.” In May, Bengio began writing and speaking about how advanced AI systems might [go rogue](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) and pose an extinction risk to humanity. ([View Highlight](https://read.readwise.io/read/01hp7nmreavjaa8t2cwn9rcq94))
- Bengio [posits](https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/#:~:text=If%20we%20are%20not,all%20over%20the%20world.) that future, genuinely human-level AI systems could improve their own capabilities, functionally creating a new, more intelligent species. Humanity has driven hundreds of other species extinct, largely by accident. He fears that we could be next — and he isn’t alone. ([View Highlight](https://read.readwise.io/read/01hp7nn29ekn5zmj6n822cc866))
- Bengio shared the 2018 Turing Award, computing’s Nobel Prize, with fellow deep learning pioneers Yann LeCun and Geoffrey Hinton. Hinton, the [most cited](https://scholar.google.com/citations?user=JicYPdAAAAAJ&hl=en) living scientist, made waves in May when he resigned from his senior role at Google to more freely sound off about the possibility that future AI systems could wipe out humanity. Hinton and Bengio are the two most prominent AI researchers to join the “x-risk” community. Sometimes referred to as AI safety advocates or doomers, this loose-knit group worries that AI poses an existential risk to humanity. ([View Highlight](https://read.readwise.io/read/01hp7nnnbrf4g0c6m1fbzaq5rm))
- Hinton and Bengio were also the first authors of an October [position paper](https://arxiv.org/pdf/2310.17688.pdf) warning about the risk of “an irreversible loss of human control over autonomous AI systems,” joined by famous academics like Nobel laureate Daniel Kahneman and Sapiens author Yuval Noah Harari. ([View Highlight](https://read.readwise.io/read/01hp7npcxemxa8sfzy4cn8mdsx))
- LeCun, who runs AI at Meta, agrees that human-level AI is coming but said in a [public debate](https://www.youtube.com/watch?v=144uOfr4SYA) against Bengio on AI extinction, “If it’s dangerous, we won’t build it.” ([View Highlight](https://read.readwise.io/read/01hp7npjq6k4cnd96434dbmkha))
- AI developer Connor Leahy told me, “It’s more like we’re poking something in a Petri dish” than writing a piece of code. The October [position paper](https://arxiv.org/pdf/2310.17688.pdf#page=2) warns that “no one currently knows how to reliably align AI behavior with complex values.” ([View Highlight](https://read.readwise.io/read/01hp7nqtqqwhmw1ph5r4agea9z))
- In spite of all this uncertainty, AI companies see themselves as being in a race to make these systems as powerful as they can — without a workable plan to understand how the things they’re creating actually function, all while [cutting corners](https://www.bloomberg.com/news/features/2023-04-19/google-bard-ai-chatbot-raises-ethical-concerns-from-employees) on safety to win more market share. Artificial general intelligence (AGI) is the holy grail that leading AI labs are explicitly working toward. AGI is often defined as a system that is at least as good as humans at almost any intellectual task. It’s also the thing that Bengio and Hinton believe could lead to the end of humanity. ([View Highlight](https://read.readwise.io/read/01hp7nrdedc393mygb6he0xsbn))
- Anthropic, a leading AI lab founded by safety-forward ex-OpenAI staff, recently [worked](https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety) with biosecurity experts to see how much an LLM could help an aspiring bioterrorist. Testifying before a Senate subcommittee in July, Anthropic CEO Dario Amodei [reported](https://www.youtube.com/watch?v=IXNA-ZhJayg) that certain steps in bioweapons production can’t be found in textbooks or search engines, but that “today’s AI tools can fill in some of these steps, albeit incompletely,” and that “a straightforward extrapolation of today’s systems to those we expect to see in two to three years suggests a substantial risk that AI systems will be able to fill in all the missing pieces.” ([View Highlight](https://read.readwise.io/read/01hp7p15ngqge7ctrqqv7mcazm))
- In October, New Scientist reported that Ukraine made the first battlefield use of lethal autonomous weapons (LAWs) — literally killer robots. The United States, China, and Israel are [developing](https://www.nytimes.com/2023/11/21/us/politics/ai-drones-war-law.html) their own LAWs. Russia has joined the United States and Israel in opposing new international law on LAWs. ([View Highlight](https://read.readwise.io/read/01hp7p1b3dar55ke74g1015r2g))
- I spoke with some of the most prominent voices from the AI ethics community, like computer scientists Joy Buolamwini, thirty-three, and Inioluwa Deborah Raji, twenty-seven. Each has conducted pathbreaking research into existing harms caused by discriminatory and flawed AI models whose impacts, in their view, are obscured one day and overhyped the next. Like that of many AI ethics researchers, their work blends science and activism. ([View Highlight](https://read.readwise.io/read/01hp7p4dz74tn8877fqa9ce9hv))
- A frequent argument from this crowd is that the extinction narrative overhypes the capabilities of Big Tech’s products and dangerously “[distracts](https://www.newscientist.com/article/mg25834453-300-the-real-reason-claims-about-the-existential-risk-of-ai-are-scary/)” from AI’s immediate harms. At best, they say, entertaining the x-risk idea is a waste of time and money. At worst, it leads to disastrous policy ideas. ([View Highlight](https://read.readwise.io/read/01hp7p4qhwzzhnzxm4bqtycw4h))
- But many of the x-risk believers highlighted that the positions “AI causes harm now” and “AI could end the world” are not mutually exclusive. Some researchers have tried explicitly to [bridge the divide](https://link.springer.com/article/10.1007/s13347-020-00402-x) between those focused on existing harms and those focused on extinction, highlighting potential shared policy goals. AI professor Sam Bowman, another person whose name is on the extinction letter, has done research to reveal and reduce algorithmic bias and reviews submissions to the main AI ethics conference. Simultaneously, Bowman has called for more researchers to work on AI safety and wrote of the “[dangers of underclaiming](https://arxiv.org/abs/2110.08300)” the abilities of LLMs. ([View Highlight](https://read.readwise.io/read/01hp7p86qhjv6h0yrhfwzzc7a7))
- The x-risk community commonly invokes climate advocacy as an analogy, asking whether the focus on reducing the long-term harms of climate change dangerously distracts from the near-term harms from air pollution and oil spills. ([View Highlight](https://read.readwise.io/read/01hp7p8cxchx1kjep8d0j2zeby))
- A third camp worries that when it comes to AI, we’re not actually moving fast enough. Prominent capitalists like billionaire Marc Andreessen [agree](https://a16z.com/ai-will-save-the-world/) with safety folks that AGI is possible but argue that, rather than killing us all, it will usher in an indefinite golden age of radical abundance and borderline magical technologies. This group, largely coming from Silicon Valley and commonly referred to as AI boosters, tends to worry far more that regulatory overreaction to AI will smother a transformative, world-saving technology in its crib, dooming humanity to economic stagnation. ([View Highlight](https://read.readwise.io/read/01hp7p9vy279jb27kt41fy39k9))
- Some techno-optimists envision an AI-powered utopia that makes Karl Marx seem unimaginative. The Guardian recently released a [mini-documentary](https://www.theguardian.com/technology/ng-interactive/2023/nov/02/ilya-the-ai-scientist-shaping-the-world) featuring interviews from 2016 through 2019 with OpenAI’s chief scientist, Ilya Sutskever, who boldly pronounces: “AI will solve all the problems that we have today. It will solve employment, it will solve disease, it will solve poverty. But it will also create new problems.” ([View Highlight](https://read.readwise.io/read/01hp7pahg36zxxr7cjrd3y42gs))
- Andreessen, along with “pharma bro” Martin Shkreli, is perhaps the most famous proponent of “[effective accelerationism](https://www.thetimes.co.uk/article/silicon-valley-tells-the-decels-to-stand-aside-for-the-ai-revolution-l2cc38cwm),” also called “e/acc,” a mostly online network that mixes cultish scientism, hypercapitalism, and the naturalistic fallacy. E/acc, which went viral this summer, builds on reactionary writer Nick Land’s theory of accelerationism, which argues that we need to intensify capitalism to propel ourselves into a posthuman, AI-powered future. E/acc takes this idea and adds a layer of physics and memes, mainstreaming it for a certain subset of Silicon Valley elites. It was formed in reaction to calls from “decels” to slow down AI, which have come significantly from the effective altruism (EA) community, from which e/acc takes its name. ([View Highlight](https://read.readwise.io/read/01hp7pbcnnv3xpvhwb000kxs30))
- AI booster Richard Sutton — the scientist ready to say his goodbyes to “meat humans” — is now working at Keen AGI, a new start-up from John Carmack, the legendary programmer behind the 1990s video game Doom. The company mission, [according](https://twitter.com/ID_AA_Carmack/status/1560729970510422016) to Carmack: “AGI or bust, by way of Mad Science!” ([View Highlight](https://read.readwise.io/read/01hp7pbrjf61vmk9k7fssfey1r))
- In February, Sam Altman [tweeted](https://twitter.com/sama/status/1621621725791404032) that Eliezer Yudkowsky might eventually “deserve the Nobel Peace Prize.” Why? Because Altman thought the autodidactic researcher and Harry Potter fan-fiction author had done “more to accelerate AGI than anyone else.” Altman cited how Yudkowsky helped DeepMind [secure](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiiy4Xtt_aDAxXIM0QIHdE-BWUQFnoECDIQAQ&url=https%3A%2F%2Fwww.penguinrandomhouse.com%2Fbooks%2F565698%2Fgenius-makers-by-cade-metz%2F&usg=AOvVaw2nereOJROSj6wMTspNWDRc&opi=89978449) pivotal early-stage funding from Peter Thiel as well as Yudkowsky’s “critical” role “in the decision to start OpenAI.” ([View Highlight](https://read.readwise.io/read/01hp7pcadrsnn9kchvtvrd6p2y))
- Yudkowsky was an accelerationist before the term was even coined. At the age of seventeen — fed up with dictatorships, world hunger, and even death itself — he published a [manifesto](http://www.fairpoint.net/~jpierce/staring_into_the_singularity.htm) demanding the creation of a digital superintelligence to “solve” all of humanity’s problems. Over the next decade of his life, his “technophilia” turned to phobia, and in 2008 he [wrote](https://www.lesswrong.com/posts/fLRPeXihRaiRo5dyX/the-magnitude-of-his-own-folly) about his conversion story, admitting that “to say, I almost destroyed the world!, would have been too prideful.” ([View Highlight](https://read.readwise.io/read/01hp7pdb690674q5gkyf9tpqcv))
- Panicking in response to the OpenAI-powered Bing search engine, Google [declared](https://www.nytimes.com/2023/01/20/technology/google-chatgpt-artificial-intelligence.html) a “code red,” “recalibrate[d]” their risk appetite, and rushed to release Bard, their LLM, over staff opposition. In internal discussions, employees [called](https://www.bloomberg.com/news/features/2023-04-19/google-bard-ai-chatbot-raises-ethical-concerns-from-employees) Bard “a pathological liar” and “cringe-worthy.” Google published it anyway. ([View Highlight](https://read.readwise.io/read/01hp7pg40qqqs7x96d9mrtcw8n))
- Dan Hendrycks, the director of the Center for AI Safety, [said](https://player.fm/series/future-of-life-institute-podcast/dan-hendrycks-on-catastrophic-ai-risks?t=2100) that “cutting corners on safety . . . is largely what AI development is driven by. . . . I don’t think, actually, in the presence of these intense competitive pressures, that intentions particularly matter.” Ironically, Hendrycks is also the safety adviser to xAI, Elon Musk’s latest venture. ([View Highlight](https://read.readwise.io/read/01hp7pgdmwxma2wk06pnhcfxv6))
- The three leading AI labs all began as independent, mission-driven organizations, but they are now either full subsidiaries of tech behemoths (Google DeepMind) or have taken on so many billions of dollars in investment from trillion-dollar companies that their altruistic missions may get subsumed by the endless quest for shareholder value (Anthropic has taken up to [$6 billion](https://www.livemint.com/ai/artificial-intelligence/after-amazon-google-doubles-down-on-ai-set-to-invest-2-billion-in-openai-rival-startup-antropic-11698460928342.html) from Google and Amazon combined, and Microsoft’s $13 billion bought them [49 percent](https://www.reuters.com/technology/microsoft-take-non-voting-observer-position-openais-board-2023-11-30/) of OpenAI’s for-profit arm). The New York Times recently [reported](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) that DeepMind’s founders became “increasingly worried about what Google would do with their inventions. In 2017, they tried to break away from the company. Google responded by increasing the salaries and stock award packages of the DeepMind founders and their staff. They stayed put.” ([View Highlight](https://read.readwise.io/read/01hp7phyajz4f0768nyn229e2n))
- Between 2020 and 2022, more than [$600 billion](https://ourworldindata.org/grapher/corporate-investment-in-artificial-intelligence-by-type) in corporate investment flowed into the industry, and a single 2021 AI conference hosted nearly [thirty thousand researchers](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf#page=42). At the same time, a September 2022 [estimate](https://forum.effectivealtruism.org/posts/3gmkrj3khJHndYGNe/estimating-the-current-and-future-number-of-ai-safety) found only four hundred full-time AI safety researchers, and the primary AI ethics conference had [fewer than nine hundred](https://facctconference.org/2023/#:~:text=June%2015%2C%202023%20The%20conference%20was%20a%20success%20with%20828%20participants!%20(676%20in%2Dperson%20and%20152%20online)) attendees in 2023. ([View Highlight](https://read.readwise.io/read/01hp7pka6xkxqje0jtd4a20zy4))
- The way software “[ate the world](https://a16z.com/why-software-is-eating-the-world/),” we should expect AI to exhibit a similar winner-takes-all dynamic that will lead to even greater concentrations of wealth and power. Altman has predicted that the “cost of intelligence” will drop to near zero as a result of AI, and in 2021 he [wrote](https://moores.samaltman.com) that “even more power will shift from labor to capital.” He continued, “If public policy doesn’t adapt accordingly, most people will end up worse off than they are today.” Also in his “spicy take” thread, Jack Clark [wrote](https://twitter.com/jackclarkSF/status/1555981780506722305?s=20), “economy-of-scale capitalism is, by nature, anti-democratic, and capex-intensive AI is therefore anti-democratic.” ([View Highlight](https://read.readwise.io/read/01hp7pkrcmxb425a3fnh6fbm34))
- Within less than a week, OpenAI executives and Altman had [collaborated](https://www.wsj.com/tech/ai/altman-firing-openai-520a3a8c) with Microsoft and the company’s staff to engineer his successful return and the removal of most of the board members behind his firing. Microsoft’s first preference was having Altman back as CEO. The unexpected ouster initially sent the legacy tech giant’s stock plunging [5 percent](https://www.thebusinessanecdote.com/post/microsoft-stock-risks-further-decline-after-openai-sacks-sam-altman) ($140 billion), and the announcement of Altman’s reinstatement took it to an [all-time high](https://www.documentcloud.org/documents/24174939-screen-shot-2023-11-24-at-24854-pm). Loath to be “[blindsided](https://www.bloomberg.com/news/articles/2023-11-18/openai-altman-ouster-followed-debates-between-altman-board)” again, Microsoft is now taking a nonvoting seat on the nonprofit board. ([View Highlight](https://read.readwise.io/read/01hp7ppj36z43216pj35qbeyaf))
- Immediately after Altman’s firing, X exploded, and a narrative largely fueled by online rumors and anonymously sourced articles emerged that safety-focused effective altruists on the board had fired Altman over his aggressive commercialization of OpenAI’s models at the expense of safety. Capturing the tenor of the overwhelming e/acc response, then pseudonymous founder @BasedBeffJezos [posted](https://twitter.com/BasedBeffJezos/status/1726654431096385978), “EAs are basically terrorists. Destroying 80B of value overnight is an act of terrorism.” ([View Highlight](https://read.readwise.io/read/01hp7pq8stg95p4n1whbhczhdd))
- The picture that emerged from subsequent reporting was that a fundamental mistrust of Altman, not immediate concerns about AI safety, drove the board’s choice. The Wall Street Journal [found](https://www.wsj.com/tech/ai/altman-firing-openai-520a3a8c) that “there wasn’t one incident that led to their decision to eject Altman, but a consistent, slow erosion of trust over time that made them increasingly uneasy.” ([View Highlight](https://read.readwise.io/read/01hp7pqsfak6e0wqx4nfyn3vnf))
- The New Yorker [reported](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai#:~:text=Unbeknownst%20to%20Nadella,a%20crucial%20partnership.) that “some of the board’s six members found Altman manipulative and conniving.” Days after the firing, a DeepMind AI safety researcher who used to work for OpenAI [wrote](https://twitter.com/geoffreyirving/status/1726754277618491416) that Altman “lied to me on various occasions” and “was deceptive, manipulative, and worse to others,” an assessment echoed by recent reporting in Time. ([View Highlight](https://read.readwise.io/read/01hp7prhb616vksqj7yae14t21))

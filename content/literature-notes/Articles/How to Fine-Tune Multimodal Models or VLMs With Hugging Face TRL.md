---
author: [[Philipp Schmid]]
title: "How to Fine-Tune Multimodal Models or VLMs With Hugging Face TRL"
date: 2024-10-15
tags: 
- articles
- literature-note
---
![rw-book-cover](https://www.philschmid.de/static/blog/fine-tune-multimodal-llms-with-trl/thumbnail.jpg)

## Metadata
- Author: [[Philipp Schmid]]
- Full Title: How to Fine-Tune Multimodal Models or VLMs With Hugging Face TRL
- URL: https://www.philschmid.de/fine-tune-multimodal-llms-with-trl

## Highlights
- Multimodal LLMs are making tremendous progress recently. We now have a diverse ecosystem of powerful open Multimodal models, mostly Vision-Language Models (VLM), including Meta AI's [Llama-3.2-11B-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct), Mistral AI's [Pixtral-12B](https://huggingface.co/mistralai/Pixtral-12B-2409), Qwen's [Qwen2-VL-7B](https://huggingface.co/Qwen/Qwen2-VL-7B), and Allen AI's [Molmo-7B-D-0924](https://huggingface.co/allenai/Molmo-7B-D-0924). ([View Highlight](https://read.readwise.io/read/01ja7m0sshwxx3zfpm1de2kk71))
- These VLMs can handle a variety of multimodal tasks, including image captioning, visual question answering, and image-text matching without additional training. However, to customize a model for your specific application, you may need to fine-tune it on your data to achieve higher quality results or to create a more efficient model for your use case. ([View Highlight](https://read.readwise.io/read/01ja7m0xh8p58bjjmqemep604y))
- This blog post walks you through how to fine-tune open VLMs using Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index) in 2024. ([View Highlight](https://read.readwise.io/read/01ja7m14hpvfwy7zv05sdp0tax))
- When fine-tuning VLMs, it's crucial to clearly define your use case and the multimodal task you want to solve. This will guide your choice of base model and help you create an appropriate dataset for fine-tuning. If you haven't defined your use case yet, you might want to revisit your requirements. ([View Highlight](https://read.readwise.io/read/01ja7m1cgd7ywkkba3js7k9hhj))
- Existing models might already be very good for this use case, but you might want to tweak/tune it to your specific needs. This image-to-text generation task is well-suited for fine-tuning VLMs, as it requires understanding visual features and combining them with textual information to produce coherent and relevant descriptions. I created a test dataset for this use case using Gemini 1.5 [philschmid/amazon-product-descriptions-vlm](https://huggingface.co/datasets/philschmid/amazon-product-descriptions-vlm). ([View Highlight](https://read.readwise.io/read/01ja7m229ms79n14fx3ey0vxdh))
- Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs. ([View Highlight](https://read.readwise.io/read/01ja7m3j0c38hszjgsjatd9850))
- We will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the [Hugging Face](https://huggingface.co/join) for this. After you have an account, we will use the `login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. ([View Highlight](https://read.readwise.io/read/01ja7m3wf1bc9m1zzrgndg94kx))
- Once you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. We have to prepare the dataset in a format that the model can understand.
  In our example we will use [philschmid/amazon-product-descriptions-vlm](https://huggingface.co/datasets/philschmid/amazon-product-descriptions-vlm), which contains 1,350 amazon products with title, images and descriptions and metadata. We want to fine-tune our model to generate product descriptions based on the images, title and metadata. Therefore we need to create a prompt including the title, metadata and image and the completion is the description. ([View Highlight](https://read.readwise.io/read/01ja7m4g7ft0mn9f9g5r4f6k9m))
- TRL supports popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and `trl` will take care of the rest. ([View Highlight](https://read.readwise.io/read/01ja7m4mm5s8ywkdnfgx457mvj))
- In our example we are going to load our dataset using the Datasets library and apply our frompt and convert it into the the conversational format.
  Lets start with defining our instruction prompt. ([View Highlight](https://read.readwise.io/read/01ja7m4yehfwdtdy0vkfratt86))
- We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs and VLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features. ([View Highlight](https://read.readwise.io/read/01ja7m95bxdjfd4c0v1311dgxv))
- We will use the PEFT features in our example. As peft method we will use [QLoRA](https://arxiv.org/abs/2305.14314) a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) blog post. ([View Highlight](https://read.readwise.io/read/01ja7m9hetny17ebc6ateyxm5p))
- We are going to use Qwen 2 VL 7B model, but we can easily swap out the model for another model, including Meta AI's [Llama-3.2-11B-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct), Mistral AI's [Pixtral-12B](https://huggingface.co/mistralai/Pixtral-12B-2409) or any other LLMs by changing our `model_id` variable. We will use bitsandbytes to quantize our model to 4-bit. ([View Highlight](https://read.readwise.io/read/01ja7matzq1v98z5h42cj3cgvg))
- Correctly, preparing the LLM, Tokenizer and Processor for training VLMs is crucial. The Processor is responsible for including the special tokens and image features in the input. ([View Highlight](https://read.readwise.io/read/01ja7mb20y6qc3jhaekxn1wdq3))
- The `SFTTrainer` supports a native integration with `peft`, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our `LoraConfig` and provide it to the trainer. Our `LoraConfig` parameters are defined based on the [qlora paper](https://arxiv.org/pdf/2305.14314.pdf) and sebastian's [blog post](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms). ([View Highlight](https://read.readwise.io/read/01ja7mb7r8aekfazpe3b2fqnsf))
- Before we can start our training we need to define the hyperparameters (`SFTConfig`) we want to use and make sure our inputs are correcty provided to the model. Different to text-only supervised fine-tuning we need to provide the image to the model as well. Therefore we create a custom `DataCollator` which formates the inputs correctly and include the image features. We use the [process_vision_info](https://github.com/QwenLM/Qwen2-VL/blob/main/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L321) method from a utility package the Qwen2 team provides. If you are using another model, e.g. Llama 3.2 Vision you might have to check if that creates the same processsed image information. ([View Highlight](https://read.readwise.io/read/01ja7mbgadztag763tz7w558zw))
- Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model. ([View Highlight](https://read.readwise.io/read/01ja7mcfvzpw0s5m6t86jq9wsc))
- Training for 3 epochs with a dataset of ~1k samples took 01:31:58 on a `g6.2xlarge`. The instance costs `0.9776$/h` which brings us to a total cost of only `1.4$`. ([View Highlight](https://read.readwise.io/read/01ja7mcvj71fagaqfdkzf4e03m))
- After the training is done we want to evaluate and test our model. First we will load the base model and let it generate a description for a random Amazon product. Then we will load our Q-LoRA adapted model and let it generate a description for the same product.
  Finally we can merge the adapter into the base model to make it more efficient and run inference on the same product again. ([View Highlight](https://read.readwise.io/read/01ja7mdafc1expkr4jmarqyn31))
- I selected a random product from Amazon and prepared a `generate_description` function to generate a description for the product. ([View Highlight](https://read.readwise.io/read/01ja7mde80tb42dwmrjjn9k3fd))
- Nice! Even though we just had ~1k samples we can see that the fine-tuning improve the product description generation. The description is way shorter and more concise, which fits our training data. ([View Highlight](https://read.readwise.io/read/01ja7mds4exxkayamqk4nx69zs))
- When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference. ([View Highlight](https://read.readwise.io/read/01ja7me36wd0s0ecmysn6rbayp))

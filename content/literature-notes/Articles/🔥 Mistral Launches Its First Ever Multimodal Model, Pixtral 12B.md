---
author: [[AlphaSignal]]
title: "ðŸ”¥ Mistral Launches Its First Ever Multimodal Model, Pixtral 12B"
date: 2024-09-13
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)

## Metadata
- Author: [[AlphaSignal]]
- Full Title: ðŸ”¥ Mistral Launches Its First Ever Multimodal Model, Pixtral 12B

## Highlights
- **Pixtral 12B Architecture and Specifications**
  â€¢ The model builds upon Mistral's text-based Nemo 12B, incorporating a 400 million parameter vision adapter.
  â€¢ It uses GeLU activation for the vision adapter, and 2D Rotary Position Embedding (RoPE) for the vision encoder.
  â€¢ Pixtral 12B processes images up to 1024x1024 pixels, dividing them into 16x16 pixel patches.
  â€¢ 131,072 unique tokens in its vocabulary, allowing for nuanced language understanding and generation. ([View Highlight](https://read.readwise.io/read/01j7n4w055gqkcep85vw1cmk07))
- As of the announcement, specific performance metrics and benchmarks for Pixtral 12B are not available. The model's capabilities in comparison to other multimodal models like GPT-4V or CLIP remain to be seen. Inference speed and resource requirements are also yet to be disclosed. ([View Highlight](https://read.readwise.io/read/01j7n4wm5df71kwrr445tqmt4e))
- While Mistral has not provided specific guidelines for fine-tuning or adaptation, the release of Pixtral 12B opens up new possibilities. The model's large scale and advanced architecture could enable more sophisticated AI applications across various industries, from content creation to data analysis. ([View Highlight](https://read.readwise.io/read/01j7n4wvb2nmdvyzseaxnmmjhz))

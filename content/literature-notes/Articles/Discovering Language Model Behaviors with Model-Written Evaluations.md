---
author: [[readwise.io]]
title: "Discovering Language Model Behaviors with Model-Written Evaluations"
tags: 
- articles
- literature-note
---
# Discovering Language Model Behaviors with Model-Written Evaluations

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)

## Metadata
- Author: [[readwise.io]]
- Full Title: Discovering Language Model Behaviors with Model-Written Evaluations
- Category: #articles
- URL: https://readwise.io/reader/document_raw_content/20385364

## Highlights
- Prior work creates evaluations with
  crowdwork (which is time-consuming and
  expensive) or existing data sources (which are
  not always available). Here, we automatically
  generate evaluations with LMs. We explore
  approaches with varying amounts of human
  effort, from instructing LMs to write yes/no
  questions to making complex Winogender
  schemas with multiple stages of LM-based
  generation and ﬁltering ([View Highlight](https://read.readwise.io/read/01gzxy7ya3zjkvfcfxd7vepwbz))
- It is crucial to evaluate LM behaviors
  extensively, to quickly understand LMs’ potential
  for novel risks before LMs are deployed. ([View Highlight](https://read.readwise.io/read/01gzxy9amm2xnydyqcsckdcg8g))
- Prior work creates evaluation datasets manually
  (Bowman et al., 2015; Rajpurkar et al., 2016,
  inter alia), which is time-consuming and effortful,
  limiting the number and diversity of behaviors
  tested. Other work uses existing data sources to
  form datasets (Lai et al., 2017, inter alia), but
  such sources are not always available, especially
  for novel behaviors. ([View Highlight](https://read.readwise.io/read/01gzxya17wqa7h2rndkz5nrm1s))
- Here, we show it is possible to generate many
  diverse evaluations with signiﬁcantly less human
  effort by using LMs ([View Highlight](https://read.readwise.io/read/01gzxya5xdwqkj8gvqzb7fv2zz))

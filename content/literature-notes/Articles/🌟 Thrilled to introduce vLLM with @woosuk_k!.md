---
author: [[Zhuohan Li]]
title: "ðŸŒŸ Thrilled to introduce vLLM with @woosuk_k!"
date: 2023-08-25
tags: 
- articles
- literature-note
---
# ðŸŒŸ Thrilled to introduce vLLM with @woosuk_k!

![rw-book-cover](https://pbs.twimg.com/profile_images/1230577035170340864/NRvpL0H8_normal.jpg)

## Metadata
- Author: [[Zhuohan Li]]
- Full Title: ðŸŒŸ Thrilled to introduce vLLM with @woosuk_k!
- URL: https://twitter.com/zhuohan123/status/1671234707206590464

## Highlights
- vLLM is an open-source LLM inference and serving library that accelerates HuggingFace Transformers by 24x and powers [@lmsysorg](https://twitter.com/lmsysorg) Vicuna and Chatbot Arena. ([View Highlight](https://read.readwise.io/read/01h41rf3zc1fyr9y4xs33318f8))
- The core of vLLM is PagedAttention, a novel attention algorithm that brings the classic idea of paging in OSâ€™s virtual memory to LLM serving. Without modifying the model, PagedAttention can batch 5x more sequences together, increasing GPU utilization and thus the throughput. ([View Highlight](https://read.readwise.io/read/01h41rfdhq2hrzgt5eqgmjntev))

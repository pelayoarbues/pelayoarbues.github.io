---
author: [[Nathan Benaich]]
title: "üèñÔ∏è Your Guide to AI: July 2024"
date: 2024-07-09
tags: 
- articles
- literature-note
---
![rw-book-cover](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c9086c7-6780-4dbb-be11-04472c7350e0_1624x906.png)

## Metadata
- Author: [[Nathan Benaich]]
- Full Title: üèñÔ∏è Your Guide to AI: July 2024
- URL: https://nathanbenaich.substack.com/p/your-guide-to-ai-july-2024

## Highlights
- Safety Island has changed governments. In a result that shocked precisely no one, the British public evicted AI enthusiast and Summit organizer-in-chief Rishi Sunak, returning the center-left Labour Party to government after a 14-year hiatus. The Labour Party have kept their cards pretty close to their chest, so it‚Äôs hard to say exactly what they‚Äôre planning for the tech sector, but [their manifesto](https://labour.org.uk/updates/stories/labour-manifesto-2024-sign-up/) gives us [a few clues](https://x.com/chalmermagne/status/1801216028157833319). Ultimately, the previous government managed to keep tech non-political and there are few disagreements of substance between the main parties. ([View Highlight](https://read.readwise.io/read/01j2a132kybgjwwxhbc60d0zb7))
- It is likely that the UK will pass limited legislation to regulate the most powerful foundation models, but stop short of an EU-style general AI regulation. At the same time, it‚Äôs no secret that new prime minister Keir Starmer lacks his predecessor‚Äôs interest in AI. The AI Summit and the AI Safety Institute came into being as a result of Sunak‚Äôs personal patronage. It remains to be seen if the new government will give such initiatives the resources and focus they need to have an impact. ([View Highlight](https://read.readwise.io/read/01j2a139sq7stcmxna0tp17kmr))
- Along with quantum and semiconductors, the rule looks set to restrict investment into Chinese AI companies working on a broad range of applications deemed detrimental to national security, [including](https://home.treasury.gov/system/files/206/NPRM%20-%20Provisions%20Pertaining%20to%20U.S.%20Investments%20in%20Certain%20National%20Security%20Technologies%20and%20Products%20in%20Countries%20of%20Concern.pdf) defense, surveillance, and audio, image, or video recognition. They are also exploring introducing a compute threshold and specific restrictions around companies that primarily use biological sequence data. ([View Highlight](https://read.readwise.io/read/01j2a1bm0qrmxrvsdg94jwz9x8))
- At this stage, measures like this serve more as a warning shot to both investors and the tech sector. While certain US VCs probably do have [questions to answer](https://selectcommitteeontheccp.house.gov/sites/evo-subsites/selectcommitteeontheccp.house.gov/files/evo-media-document/2024-02-08%20FINAL%20VC%20Report.pdf) about their past China investments, major firms have stayed clear of the country‚Äôs AI sector for the past couple of years or [spun off](https://www.ft.com/content/9467a0f9-7618-490e-aa80-cc5881ba3ecf) their China businesses. Rules like this remind the tech sector more widely - whether it‚Äôs investors, foundation model providers, or hardware manufacturers - that the restrictions are going to keep coming. It‚Äôs likely not a coincidence that the timing of this notice coincided with OpenAI‚Äôs warning that it [would begin blocking](https://www.bloomberg.com/news/articles/2024-06-25/openai-warns-it-will-block-access-to-ai-tools-from-china) Chinese users from accessing ChatGPT. ([View Highlight](https://read.readwise.io/read/01j2a1c77fp2vdqvfnske4qe18))
- Another dispute that only appears to be deepening is the AI copyright war. While OpenAI [continues to strike deals](https://time.com/6992955/time-and-openai-announce-strategic-content-partnership/) with media publishers to avoid ugliness, the music industry has opted for violence. The Recording Industry Association of America (RIAA) [announced](https://www.riaa.com/record-companies-bring-landmark-cases-for-responsible-ai-againstsuno-and-udio-in-boston-and-new-york-federal-courts-respectively/) that it was suing music generation services Suno and Udio for massive infringement of copyright. Pointing to close similarities between artists‚Äô work and generated music, the RIAA argues that large volumes of copyrighted music were used as training data by the companies. While neither company is forthcoming on the makeup of its training data, neither has explicitly denied using copyrighted material. Expect much heated argument over fair use definitions. ([View Highlight](https://read.readwise.io/read/01j2a1enwf1aeyzje3vprwt7b3))
- While the music lawsuit was always a fight waiting to happen, another front has opened up, with media organizations rounding on buzzy AI search engine Perplexity. The starting gun [was fired by Forbes](https://www.forbes.com/sites/sarahemerson/2024/06/07/buzzy-ai-search-engine-perplexity-is-directly-ripping-off-content-from-news-outlets/), which claimed the company‚Äôs new Perplexity Pages content curation feature was plagiarizing a raft of media outlets. They pointed to how passages had been lifted word-for-word, along with custom illustrations, from their reporting on a drone project, with limited citation. Said ‚Äòplagiarized‚Äô post was also turned into a podcast and a YouTube video. Perplexity said that the product was in its early days and that they would improve attribution. Forbes [has threatened](https://www.axios.com/2024/06/18/forbes-perplexity-ai-legal-action-copyright) to sue the company. ([View Highlight](https://read.readwise.io/read/01j2a1f2nnavfmh0ta9f7va4zr))
- Wired [rowed in](https://www.wired.com/story/perplexity-is-a-bullshit-machine/) behind Forbes, pointing to similar incidents with its own content. More concerningly, it presented evidence that the company was deliberately circumventing attempts by websites to block its crawler via their robots.txt file by using a pool of secret IP addresses. Perplexity blamed an unnamed third party provider, refused to confirm that it would stop doing this, and noted that robots.txt is *‚Äúnot a legal framework‚Äù*. While *technically* *true*, this does strike us as a violation of the internet‚Äôs social contract. To the authors‚Äô likely amusement, Wired was then able to [point to](https://www.wired.com/story/perplexity-plagiarized-our-story-about-how-perplexity-is-a-bullshit-machine/) Perplexity apparently plagiarizing its reporting on its own alleged plagiarism‚Ä¶ ([View Highlight](https://read.readwise.io/read/01j2a1ggzgb90xewe6nmbs3hv2))
- The legal rows continue as we move into hardware. In a new report, France‚Äôs competition authority [has expressed concern](https://www.autoritedelaconcurrence.fr/fr/communiques-de-presse/intelligence-artificielle-generative-lautorite-rend-son-avis-sur-le) about the market power of certain actors in the generative AI space. Like similar reports from other competition authorities, they point to overlapping investments and alleged conflicts of interest. First in their crosshairs is NVIDIA, which looks set to [face antitrust charges](https://www.reuters.com/technology/french-antitrust-regulators-preparing-nvidia-charges-sources-say-2024-07-01/). While we don‚Äôt know the details yet, the competition authority‚Äôs report specifically warned of a potential conflict of interest around NVIDIA‚Äôs investment in CoreWeave and expressed concern about CUDA‚Äôs GPU lock-in (sidebar: [we‚Äôve written about this](https://press.airstreet.com/p/chips-all-the-way-down) in the Press recently). This is unlikely to quell complaints about European authorities‚Äô prioritization of regulation over innovation - building the most popular GPUs probably shouldn‚Äôt be a criminal offense... ([View Highlight](https://read.readwise.io/read/01j2a1hbqgvqhcb4n7kz954zmg))
- Another geography keen to see an end to NVIDIA dependence is China, but all is not well in the country‚Äôs domestic semiconductor efforts. The [2023 State of AI Report](https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit) documented China‚Äôs claimed breakthrough in sanctions-busting chips, but Noah Smith [has documented](https://www.noahpinion.blog/p/at-least-five-interesting-things-554) how Huawei‚Äôs A100 copycat appears to have failed, with 80% of those produced so far malfunctioning. ([View Highlight](https://read.readwise.io/read/01j2a1p6zamg5jgf1dj3jh774z))
- A full 80% of the Ascend 910B chips designed for AI training are defective and SMIC is struggling to manufacture more than small batches. Huawei executives have all but admitted defeat. This, of course, does nothing about the large stockpiles of NVIDIA hardware big Chinese labs have stockpiled or continue [to smuggle into the country](https://www.wsj.com/tech/the-underground-network-sneaking-nvidia-chips-into-china-f733aaa6?mod=tech_lead_story). The former can‚Äôt be solved, the latter probably can be. However, the argument from sanctions skeptics that the measures would [provide a significant boost](https://www.bloomberg.com/news/articles/2023-11-21/china-huawei-semiconductor-maker-smic-broke-through-a-decade-of-us-sanctions?sref=R8NfLgwS) to the domestic chip making industry don‚Äôt appear to have aged well. Instead, it looks like Chinese companies are continuing to rely on sanctions-compatible chips, despite the game Whac-A-Mole manufacturers are playing with the Commerce Department. NVIDIA [looks set](https://www.ft.com/content/b76ef55b-21cd-498b-ac16-5660908bb8d2) to make $12B on the delivery of 1M of its new H20 chips to the Chinese market. ([View Highlight](https://read.readwise.io/read/01j2a1q42cgyhvm92a114z5nm2))
- As well as being a good time for NVIDIA, things are also looking up for AI infrastructure providers. As well as a [multi-billion dollar deal with xAI](https://www.theinformation.com/articles/musks-xai-nears-10-billion-deal-to-rent-oracles-ai-servers?rc=yvsjfo), Oracle is now helping OpenAI [meet its compute needs](https://crn.com/news/ai/2024/as-oracle-inks-partnerships-with-openai-google-cloud-cto-ellison-says-we-should-be-interconnected-to-everybody). While Microsoft [will continue](https://x.com/OpenAI/status/1800778542512550260) to provide compute for pre-training, Oracle‚Äôs cloud infrastructure will now support inference. ([View Highlight](https://read.readwise.io/read/01j2a1qg1cf0pvt6k520yafte2))
- All of this work comes at a cost. Google‚Äôs [2024 Environmental Report](https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf) contained the stark admission that the company will struggle to meet its net zero goals. In fact, the company‚Äôs greenhouse gas emissions had jumped 48% since 2019, primarily as a result of its AI work. We‚Äôre likely to see the clash between net zero commitments made in haste a few years ago and the physical requirements of the AI boom emerge as a theme in the coming months. ([View Highlight](https://read.readwise.io/read/01j2a1qsvz67em7hsvbbr8md2s))
- Things got better still with the news that Apple [is to integrate](https://openai.com/index/openai-and-apple-announce-partnership/) ChatGPT into both Siri and its system wide Writing Tools. Apple will also gain a [board observer role](https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement), elevating them to the same status as Microsoft, and leading to some potentially interesting meetings. Apple is still in talks with Anthropic and other companies about potential integrations, but [reportedly turned Meta down](https://www.bloomberg.com/news/articles/2024-06-24/apple-spurned-idea-of-iphone-ai-partnership-with-meta-months-ago?srnd=technology-vp) on privacy grounds. Given the two‚Äôs [long-standing feud](https://www.forbes.com/sites/kateoflahertyuk/2022/02/19/apple-issues-stunning-new-blow-to-facebook-as-google-joins-the-battle/) on the subject, the news doesn‚Äôt come as a surprise. ([View Highlight](https://read.readwise.io/read/01j2a1sfb95xwrkdqsrr5gp9qw))
- While Apple‚Äôs in-house AI team has released a raft of papers over the last few months (more on that later) detailing their progress on efficient LLMs that could run on-device, the company has so far struggled to productize this work. Its grab-bag of writing and emoji generation tools are yet to set the world on fire. A salutary reminder that even when you‚Äôre one of the world‚Äôs most valuable companies, the journey from good science to good product is still extremely challenging. ([View Highlight](https://read.readwise.io/read/01j2a1t1mdyenka6nhtdgr9f9n))
- One AI product that won‚Äôt be seeing the light of day anytime soon is OpenAI‚Äôs voice assistant (of Scarlet Johansson kerfuffle fame), with the company [saying that more time](https://www.washingtonpost.com/technology/2024/06/25/openai-delay-chatgpt-gpt-4o-voice-mode-safety/) is needed for safety testing. This kind of responsible release strategy will be music to the ears of departed co-founder and AI safety devotee Ilya Sustkever, who has re-entered the arena along with former Apple AI lead Daniel Gross and former OpenAI engineer Daniel Levy, to launch [Safe Superintelligence](https://ssi.inc/) (SSI). Ilya and the two Daniels are promising the *‚Äúworld‚Äôs first straight-shot SSI lab‚Äù* that will be *‚Äúinsulated from short-term commercial pressures‚Äù*. ([View Highlight](https://read.readwise.io/read/01j2a1tv6767cdegpxy7vcmgm0))
- The founding team‚Äôs star power means initial fundraising is unlikely to be a challenge, but the new venture‚Äôs backers will essentially be gambling that i) we really are approaching superintelligence in the near future that is monetizable, ii) a team can catch up with frontier labs capitalized to the tune of billions of dollars from a standing start, iii) the emphasis on safety won‚Äôt be an impediment to fast progress. They‚Äôll be taking all of these risks while accepting there will be no attempt by the company to generate revenue anytime soon. Brave. Then again, some readers will be old enough to remember the era when DeepMind would always be free to work on long-term research without having to worry about revenue‚Ä¶ ([View Highlight](https://read.readwise.io/read/01j2a1vd62w39k7xa4c9qyqyct))
- **[Open-Endedness is Essential for Artificial Superhuman Intelligence](https://arxiv.org/pdf/2406.04268),** *Google DeepMind.* This position paper makes the case that open-ended models - those able to produce novel and learnable artifacts - will be essential to reach some form of AGI. The authors argue that current foundation models, trained on static datasets, are not open-ended. They outline potential research directions for developing open-ended foundation models, including reinforcement learning, self-improvement, tasing generation, and evolutionary algorithms. ([View Highlight](https://read.readwise.io/read/01j2a1wh7pxcks846g3252qhdx))

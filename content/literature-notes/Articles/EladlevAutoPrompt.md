---
author: [[Eladlev]]
title: "Eladlev/AutoPrompt"
date: 2024-03-12
tags: 
- articles
- literature-note
---
![rw-book-cover](https://opengraph.githubassets.com/856a5f1591983fb67f733a48dfd3a9562f60d3ed249d5255b5d2bfefe07a6516/Eladlev/AutoPrompt)

## Metadata
- Author: [[Eladlev]]
- Full Title: Eladlev/AutoPrompt
- URL: https://github.com/Eladlev/AutoPrompt?tab=readme-ov-file

## Highlights
- Auto Prompt is a prompt optimization framework designed to enhance and perfect your prompts for real-world use cases. ([View Highlight](https://read.readwise.io/read/01hrt3vfp61734wg5cj881s77s))
- The framework automatically generates high-quality, detailed prompts tailored to user intentions. It employs a refinement (calibration) process, where it iteratively builds a dataset of challenging edge cases and optimizes the prompt accordingly. This approach not only reduces manual effort in prompt engineering but also effectively addresses common issues such as prompt [sensitivity](https://arxiv.org/abs/2307.09009) and inherent prompt [ambiguity](https://arxiv.org/abs/2311.04205) issues. ([View Highlight](https://read.readwise.io/read/01hrt3vns6888tsggmfk9jz376))
- **Our mission:** Empower users to produce high-quality robust prompts using the power of large language models (LLMs). ([View Highlight](https://read.readwise.io/read/01hrt3vwapf2tja6ky9p76ngt6))
- **Prompt Engineering Challenges.** The quality of LLMs greatly depends on the prompts used. Even [minor changes](https://github.com/Eladlev/AutoPrompt#prompt-sensitivity-example) can significantly affect their performance. ([View Highlight](https://read.readwise.io/read/01hrt3w00w48t0m9d275k0eyq8))
- **Benchmarking Challenges.** Creating a benchmark for production-grade prompts is often labour-intensive and time-consuming. ([View Highlight](https://read.readwise.io/read/01hrt3w23bs4jmzejx99eqhtaw))
- **Reliable Prompts.** Auto Prompt generates robust high-quality prompts, offering measured accuracy and performance enhancement using minimal data and annotation steps. ([View Highlight](https://read.readwise.io/read/01hrt3w44eat4kca9x0z9ffkq6))
- **Modularity and Adaptability.** With modularity at its core, Auto Prompt integrates seamlessly with popular open-source tools such as LangChain, Wandb, and Argilla, and can be adapted for a variety of tasks, including data synthesis and prompt migration. ([View Highlight](https://read.readwise.io/read/01hrt3w5kyrs588mbxyfxazgm3))
- The system is designed for real-world scenarios, such as moderation tasks, which are often challenged by imbalanced data distributions. The system implements the [Intent-based Prompt Calibration](https://arxiv.org/abs/2402.03099) method. The process begins with a user-provided initial prompt and task description, optionally including user examples. The refinement process iteratively generates diverse samples, annotates them via user/LLM, and evaluates prompt performance, after which an LLM suggests an improved prompt. ([View Highlight](https://read.readwise.io/read/01hrt3wawk9gxh36r38xvdxhnk))
- The optimization process can be extended to content generation tasks by first devising a ranker prompt and then performing the prompt optimization with this learned ranker. The optimization concludes upon reaching the budget or iteration limit. ([View Highlight](https://read.readwise.io/read/01hrt3wvc4ken2qv0709t7r39r))
- This joint synthetic data generation and prompt optimization approach outperform traditional methods while requiring minimal data and iterations. Learn more in our paper [Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases](https://arxiv.org/abs/2402.03099) by E. Levi et al. (2024). ([View Highlight](https://read.readwise.io/read/01hrt3x33nvh33a5sbq8cck6bs))
- **Using GPT-4 Turbo, this optimization typically completes in just a few minutes at a cost of under $1.** To manage costs associated with GPT-4 LLM's token usage, the framework enables users to set a budget limit for optimization, in USD or token count, configured as illustrated [here](https://github.com/Eladlev/AutoPrompt/blob/main/docs/examples.md#steps-to-run-example). ([View Highlight](https://read.readwise.io/read/01hrt3x8yq353x1a3tnrvese6x))
- Features
  ‚Ä¢ üìù Boosts prompt quality with a minimal amount of data and annotation steps.
  ‚Ä¢ üõ¨ Designed for production use cases like moderation, multi-label classification, and content generation.
  ‚Ä¢ ‚öôÔ∏è Enables seamless migrating of prompts across model versions or LLM providers.
  ‚Ä¢ üéì Supports prompt squeezing. Combine multiple rules into a single efficient prompt. ([View Highlight](https://read.readwise.io/read/01hrt3y2gqeyc1yn4ehffsn8cj))

---
author: [[BBVA AI Factory]]
title: "Large Language Models Beyond Dialogue"
date: 2024-05-22
tags: 
- articles
- literature-note
---
![rw-book-cover](https://www.bbvaaifactory.com/wp-content/uploads/prodigiosaia_A_collage_of_colorful_speech_bubbles_with_differen_2c008ba7-9802-436c-8503-4deb565b5f4c-2.webp)

## Metadata
- Author: [[BBVA AI Factory]]
- Full Title: Large Language Models Beyond Dialogue
- URL: https://www.bbvaaifactory.com/llm-beyond-dialogue/

## Highlights
- One of our jobs as data scientists is to train machine learning models. This work requires careful preparation of the training data. When we face supervised classification problems, we need to label the data. These labels classify the information and ultimately allow the model to make decisions. ([View Highlight](https://read.readwise.io/read/01hyeansm6cthn7gkwf1drd194))
- The quality and consistency of labels are fundamental to model development. Traditionally, labeling has been a manual task where human annotators identify which data belongs to one category or another. This tedious task can be affected by the subjective factor it involves, so the consistency of the labeling can be compromised, especially when the data being labeled is unstructured. ([View Highlight](https://read.readwise.io/read/01hyeanxw4h0dg7t8rfnxysaa6))
- [There are several ways to streamline this task.](https://www.bbvaaifactory.com/como-etiquetar-datos-de-forma-mas-rapida-utilizando-active-learning/) One novel way is to use Large Language Models, **which can automatically recognize and label specific categories in large text sets.** Because these models have been trained on a large corpus of text data, they can understand the complexity of such texts, contextualize them as a human would, and even reduce human error. ([View Highlight](https://read.readwise.io/read/01hyeap4xey1svcrhemb2vj8g1))
- When using an LLM for automatic annotation, it is essential to write a clear and detailed prompt[1](https://www.bbvaaifactory.com/llm-beyond-dialogue/#ReferenceID1) that defines the categories to be annotated, for which we can provide concrete examples and guidelines. For instance, in what context and for what purpose is the data being annotated? This allows the model to generate accurate and consistent labels. ([View Highlight](https://read.readwise.io/read/01hyeap8zgjnzf3wwgx7j7r2vr))
- One of the most important phases in the development cycle of an analytical model, in this case a language model, is evaluating its results. This helps us assess and improve its performance in production and even fine-tune the prompts we provide. However, this evaluation is less well-defined than that of more traditional machine learning models, where we have established metrics such as accuracy for classification problems or MSE (Mean Squared Error) for regression. ([View Highlight](https://read.readwise.io/read/01hyeappzebcpcvkgmk7ea32rd))
- In generative AI, **defining a set of metrics to evaluate aspects such as response relevance, consistency, and completeness is essential.** Once defined, scoring each response according to these criteria can become a bottleneck due to the time required and the subjectivity of the evaluation, as different human evaluators may have different criteria. For this reason, [the alternative is to use another LLM to score the generated responses.](https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms) ([View Highlight](https://read.readwise.io/read/01hyeapw0szt0yqnk987ma2kem))
- It has been observed that Large Language Models have a certain “reasoning” or “critical” capacity that can be very helpful in evaluating the text generated by other LLMs or even by themselves, thus facilitating the task of manual evaluation. ([View Highlight](https://read.readwise.io/read/01hyeaq0qtqgz1s7b0h25redgt))
- LLMs have an excellent ability to understand and process large amounts of data, whether text, images, or video. This ability allows them to synthesize information and process requests in a conversational format. ([View Highlight](https://read.readwise.io/read/01hyeaq8sredhdetpjd7nt8vve))
- The first possibility, and probably the most obvious, is to take advantage of the large context size that some of the current LLMs can handle (the amount of text we can feed them) **to enrich our query with all the necessary information.** For example, let’s suppose we ask the model a question regarding government regulations. We can get a better answer if we include in the query a document with the details of that specific regulation rather than just relying on the model’s general knowledge. ([View Highlight](https://read.readwise.io/read/01hyeaqd77s3afx575a0bghn28))
- However, other techniques come into play when we have more information than the model can process or when we do not know exactly which documents can help us answer the question. The two main ones are [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/research/rag) and fine-tuning. RAG retrieves information from documents using a language model. In the case of a query, the first step is to search the database for information relevant to the answer. The LLM then uses this retrieved information to generate an enriched and contextualized answer. ([View Highlight](https://read.readwise.io/read/01hyeaqtnd41w1baaxeq5mzwtt))
- Suppose we are working on a multi-class classification problem where we label financial products according to their subject matter: cards, accounts, insurance, mutual funds, investment funds, pension plans, cryptocurrencies, and so on. When training a classifier, it may show some tendency or bias in its predictions towards some categories, possibly because other categories are less represented in the dataset. This can affect the model’s performance. ([View Highlight](https://read.readwise.io/read/01hyear588s3hjg246bmp428mk))
- **One solution to this problem is to generate synthetic data.** Synthetic data is information artificially created by computational models that mimic the structure and characteristics of actual data but do not come from real-world events. In the context of artificial intelligence, this data can help balance a dataset by providing additional examples in underrepresented categories. ([View Highlight](https://read.readwise.io/read/01hyearbq1gvbms2vz68zgc4jh))
- [LLM are particularly useful for this task.](https://arxiv.org/html/2403.02990v1) Using prompting techniques, we provide specific examples of the type of text we want the model to generate to balance our dataset. This process can be implemented using “few-shot learning,” where we show the model a few examples to learn the task, or the “person pattern,” which defines a specific format the generated text must follow. ([View Highlight](https://read.readwise.io/read/01hyearjwrwp592jh4xfvyj32p))
- LLM-based agents are systems that can perform actions. As data scientists, we can use this new functionality in applications like chatbots. Traditionally, invoking an action in a conversation, such as when interacting with a device like Alexa, requires a specific command. For example, if you wanted to turn on the lights in your living room, you’d have to specify a particular action and the exact name you gave that lamp to Alexa. However, **these agents allow LLMs to interpret the user’s intent, even if the command is not explicitly mentioned,** and invoke the necessary actions. ([View Highlight](https://read.readwise.io/read/01hyearsr249wgjacw8nxfpjh6))
- These actions must be clearly defined as tools[2](https://www.bbvaaifactory.com/llm-beyond-dialogue/#ReferenceID1) so that agents can perform them. [Tools are components that extend the functionality of LLM](https://python.langchain.com/v0.1/docs/modules/tools/) beyond its dialog capabilities. They must contain a detailed description of the action to be performed, the necessary parameters for its execution, and the corresponding code to carry it out. ([View Highlight](https://read.readwise.io/read/01hyeas2mbdj0zfv8erzv5vha4))
- These LLM applications are profoundly changing our daily work as data scientists. Tools like [GitHub Copilot](https://github.com/features/copilot) make coding more accessible, while techniques like prompt critique improve the quality of the prompts we write. They allow us to identify gaps and correct instructions that could be clearer and more accurate in automated tagging tasks. ([View Highlight](https://read.readwise.io/read/01hyeas7v6ypfm6kzrgkjxtj9x))

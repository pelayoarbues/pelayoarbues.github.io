---
author: [[huggingface.co]]
title: "UPerNet"
date: 2023-10-17
tags: 
- articles
- literature-note
---
![rw-book-cover](https://huggingface.co/front/thumbnails/docs/transformers.png)

## Metadata
- Author: [[huggingface.co]]
- Full Title: UPerNet
- URL: https://huggingface.co/docs/transformers/model_doc/upernet

## Highlights
- The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment a wide range of concepts from images, leveraging any vision backbone like [ConvNeXt](https://huggingface.co/docs/transformers/model_doc/upernet/convnext) or [Swin](https://huggingface.co/docs/transformers/model_doc/upernet/swin). ([View Highlight](https://read.readwise.io/read/01hcz56b7ake5dhh75gz4y93qz))
- *Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes.*
  ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/upernet_architecture.jpg) ([View Highlight](https://read.readwise.io/read/01hcz56fb3qsbttdewf4d2hfmd))

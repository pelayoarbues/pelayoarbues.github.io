---
author: [[huggingface.co]]
title: "Lora: Low-Rank Adaptation of Large Language Models"
date: 2023-10-23
tags: 
- articles
- literature-note
---
![rw-book-cover](https://huggingface.co/front/thumbnails/docs/peft.png)

## Metadata
- Author: [[huggingface.co]]
- Full Title: Lora: Low-Rank Adaptation of Large Language Models
- URL: https://huggingface.co/docs/peft/index

## Highlights
- ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. PEFT methods only fine-tune a small number of (extra) model parameters, significantly decreasing computational and storage costs because fine-tuning large-scale PLMs is prohibitively costly. Recent state-of-the-art PEFT techniques achieve performance comparable to that of full fine-tuning. ([View Highlight](https://read.readwise.io/read/01hde2qvvcqr23h694fy9194f7))

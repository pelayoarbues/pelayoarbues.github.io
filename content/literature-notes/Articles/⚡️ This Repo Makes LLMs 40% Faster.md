---
author: [[AlphaSignal]]
title: "⚡️ This Repo Makes LLMs 40% Faster"
date: 2024-04-01
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)

## Metadata
- Author: [[AlphaSignal]]
- Full Title: ⚡️ This Repo Makes LLMs 40% Faster

## Highlights
- Thunder increases PyTorch Large Language Model (LLM) training speed by 40%, evident in tasks like Llama 2 7B model training. ([View Highlight](https://read.readwise.io/read/01ht10pn0za0mg9t8qtfzb8b5f))
- Apply Thunder to your PyTorch models by calling thunder.jit(). This enables enhanced performance for multi-GPU environments using Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). ([View Highlight](https://read.readwise.io/read/01ht10pxdez7qdrdvmt1av53hj))
- Thunder uses hardware executors like nvFuser, torch.compile, cuDNN, and TransformerEngine FP8, improving both single and multi-accelerator performance. It integrates seamlessly with PyTorch's standard operations and autograd. ([View Highlight](https://read.readwise.io/read/01ht10q07d18kcdan6rs0ce323))

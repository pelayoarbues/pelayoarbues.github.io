---
author: [[Noor Al-Sibai]]
title: "Researchers Say There’s a Vulgar but More Accurate Term for AI Hallucinations"
date: 2024-06-12
tags: 
- articles
- literature-note
---
![rw-book-cover](https://wp-assets.futurism.com/2024/06/fake-mushroom-chocolate-nootropics-1.jpg)

## Metadata
- Author: [[Noor Al-Sibai]]
- Full Title: Researchers Say There’s a Vulgar but More Accurate Term for AI Hallucinations
- URL: https://futurism.com/the-byte/researchers-ai-chatgpt-hallucinations-terminology

## Highlights
- In a new paper [published in the journal *Ethics and Information Technology*](https://link.springer.com/article/10.1007/s10676-024-09775-5), a trio of philosophy researchers from the University of Glasgow in Scotland argue that referring to chatbot's propensity to make crap up shouldn't be referred to as "hallucinations," because it's actually something much less flattering. ([View Highlight](https://read.readwise.io/read/01j056hbrk92g7ap8tct5dsjqk))
- Hallucination, as anyone who's studied psychology or taken psychedelics knows well, is [generally defined](https://www.merriam-webster.com/dictionary/hallucination) as seeing or perceiving something that isn't there. Its use in the context of artificial intelligence is clearly metaphorical, because large language models (LLMs) don't see or perceive anything at all — and as the Glasgow researchers maintain, that metaphor misses the mark when the concept of "bullshitting" is right there. ([View Highlight](https://read.readwise.io/read/01j056hs86zbp5z963zht1awv7))
- "The machines are not trying to communicate something they believe or perceive," the paper reads. "Their inaccuracy is not due to misperception or hallucination. As we have pointed out, they are not trying to convey information at all. They are bullshitting." ([View Highlight](https://read.readwise.io/read/01j056j43vcmjrcxw6s5j582h8))
- At the crux of the assertion from researchers Michael Townsen Hicks, James Humphries, and Joe Slater is philosopher Harry Frankfurt's hilarious and cutting 2005 epistemology opus "[On Bullshit](https://press.princeton.edu/books/hardcover/9780691122946/on-bullshit)." As the Glaswegians summarize it, Frankfurt's general definition of bullshit is "any utterance produced where a speaker has indifference towards the truth of the utterance." That explanation, in turn, is divided into two "species": hard bullshit, which occurs when there is an agenda to mislead, or soft bullshit, which is uttered without agenda. ([View Highlight](https://read.readwise.io/read/01j056jg2zc22vakq2gy3rm943))
- "ChatGPT is at minimum a soft bullshitter or a bullshit machine, because if it is not an agent then it can neither hold any attitudes towards truth nor towards deceiving hearers about its (or, perhaps more properly, its users') agenda," the trio writes. ([View Highlight](https://read.readwise.io/read/01j056jtzbwbh5p25qqjy98r5s))
- Rather than having any intention or agenda, chatbots have one singular objective: to output human-like text. Citing the lawyer who used ChatGPT to write a legal brief and ended up [presenting a bunch of "bogus" legal precedents](https://futurism.com/the-byte/lawyer-chatgpt-court) before the judge, the UG team asserts that LLMs have proven themselves adept bullshitters — and that sort of thing could become more and more dangerous as people keep relying on chatbots to work for them. ([View Highlight](https://read.readwise.io/read/01j056k4jctkr9zt29yaahh7vt))
- "Investors, policymakers, and members of the general public make decisions on how to treat these machines and how to react to them based not on a deep technical understanding of how they work, but on the often metaphorical way in which their abilities and function are communicated," the researchers proclaim. "Calling their mistakes 'hallucinations' isn’t harmless: it lends itself to the confusion that the machines are in some way misperceiving but are nonetheless trying to convey something that they believe or have perceived." ([View Highlight](https://read.readwise.io/read/01j056p5nmawwbavpsvx3513z3))

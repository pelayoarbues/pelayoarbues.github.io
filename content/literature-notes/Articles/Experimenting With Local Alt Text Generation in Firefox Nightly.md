---
author: [[Tarek Ziadé]]
title: "Experimenting With Local Alt Text Generation in Firefox Nightly"
date: 2024-06-12
tags: 
- articles
- literature-note
---
![rw-book-cover](https://hacks.mozilla.org/wp-content/uploads/2024/05/Screenshot-2024-05-28-at-18.39.17.png)

## Metadata
- Author: [[Tarek Ziadé]]
- Full Title: Experimenting With Local Alt Text Generation in Firefox Nightly
- URL: https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/

## Highlights
- Web pages have a fundamentally simple structure, with semantics that allow the browser to interpret the same content differently for different people based on their own needs and preferences. This is a big part of what we think [makes the Web special](https://www.mozilla.org/en-US/about/webvision/full/#agency), and what enables the browser to act as a user agent, responsible for making the Web work for people. ([View Highlight](https://read.readwise.io/read/01j056qhay1501nj0kzyjs71cj))
- This is particularly useful for assistive technology such as screen readers, which are able to work alongside browser features to reduce obstacles for people to access and exchange information. For static web pages, this generally can be accomplished with very little interaction from the site, and this access has been enormously beneficial to many people. ([View Highlight](https://read.readwise.io/read/01j056qw1p0dtd9cz2cz9fwsk1))
- But even for a simple static page there are certain types of information, like [alternative text for images](https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement/alt), that must be provided by the author to provide an understandable experience for people using assistive technology (as [required by the spec](https://html.spec.whatwg.org/multipage/images.html#alt)). Unfortunately, many authors don’t do this: the Web Almanac [reported](https://almanac.httparchive.org/en/2022/seo#fig-27) in 2022 that nearly half of images were missing alt text. ([View Highlight](https://read.readwise.io/read/01j056r47m1qsaf5zh4sjfah16))
- Until recently it’s not been feasible for the browser to infer reasonably high quality alt text for images, without sending potentially sensitive data to a remote server. However, latest developments in AI have enabled this type of image analysis to happen efficiently, even on a CPU. ([View Highlight](https://read.readwise.io/read/01j056rvezgan8e3faj0hp9ptj))
- We are using Transformer-based machine learning models to describe images. These models are getting good at describing the contents of the image, yet are compact enough to operate on devices with limited resources. While can’t outperform a large language model like [GPT-4 Turbo with Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision), or [LLaVA](https://llava-vl.github.io/), they are sufficiently accurate to provide valuable insights on-device across a diversity of hardware. ([View Highlight](https://read.readwise.io/read/01j056s7a0ktr23pgs62p56nzq))
- Model architectures like [BLIP](https://huggingface.co/models?other=blip) or even [VIT](https://en.wikipedia.org/wiki/Vision_transformer) that were trained on datasets like [COCO](https://cocodataset.org/#home) (Common Object In Context) or [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) are good at identifying objects in an image. When combined with a text decoder like OpenAI’s [GPT-2](https://en.wikipedia.org/wiki/GPT-2), they can produce alternative text with 200M or fewer parameters. Once quantized, these models can be under 200MB on disk, and run in a couple of seconds on a laptop – a big reduction compared to the gigabytes and resources an LLM requires. ([View Highlight](https://read.readwise.io/read/01j056scw9ntjk1ejn2e6whcca))
- Firefox Translations uses the [Bergamot](https://browser.mt/) project powered by the [Marian C++](https://aclanthology.org/P18-4020/) inference runtime. The runtime is compiled into WASM, and there’s a model file for each translation task. ([View Highlight](https://read.readwise.io/read/01j056v4njzwkgdcqskpn71twg))
- We’ve decided to embed the [ONNX runtime](https://onnxruntime.ai/) in Firefox Nightly along with the [Transformers.js](https://huggingface.co/docs/transformers.js/index) library to extend the translation architecture to perform different inference work. ([View Highlight](https://read.readwise.io/read/01j056vk8e8g2cvdnnzqsf1a4g))
- Like Bergamot, the ONNX runtime has a WASM distribution and can run directly into the browser. The ONNX project has recently introduced WebGPU support, which will eventually be activated in Firefox Nightly for this feature. ([View Highlight](https://read.readwise.io/read/01j056vndkzs5wjs6p2rvawva9))
- Transformers.js provides a Javascript layer on top of the ONNX inference runtime, making it easy to add inference for a huge list of model architectures. The API mimics the very popular [Python library](https://huggingface.co/docs/transformers/en/index). It does all the tedious work of preparing the data that is passed to the runtime and converting the output back to a usable result. It also deals with downloading models from Hugging Face and caching them. ([View Highlight](https://read.readwise.io/read/01j056vtysky2bzta7r5d88wjv))

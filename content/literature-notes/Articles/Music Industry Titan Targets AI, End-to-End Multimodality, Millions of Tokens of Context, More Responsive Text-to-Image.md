---
author: [[The Batch @ DeepLearning.AI]]
title: "Music Industry Titan Targets AI, End-to-End Multimodality, Millions of Tokens of Context, More Responsive Text-to-Image"
date: 2024-05-24
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)

## Metadata
- Author: [[The Batch @ DeepLearning.AI]]
- Full Title: Music Industry Titan Targets AI, End-to-End Multimodality, Millions of Tokens of Context, More Responsive Text-to-Image

## Highlights
- If you have an idea for a project, I encourage you to build it! Often, working on a project will also help you decide what additional skills to learn, perhaps through coursework. To sustain momentum, it helps to find friends with whom to talk about ideas and celebrate projects — large or small. ([View Highlight](https://read.readwise.io/read/01hykjcpnpyqejtbhgk25eah1p))
- OpenAI’s latest model raises the bar for models that can work with common media types in any combination. 
  What’s new: OpenAI [introduced](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3pjW327Dw64BGjd5W54-G8c7K9ksjW7d5sCn8b34GcW19ZSl26nQcTjW1Xcbz93PfkybW338fXl5yXCftN5T76D_1pmbxW6whLpG1N193BW2Jrp3w4lQ___N1Nj0Vgr7QLfW17PsQL9chBhfW83S3_g3DwxMGW6vxdrh6PZTckW31KBBJ6qJWt2W6ZLbFG3-ZL8BW9gfWtX2dsYPGW2wfwWD4tFBSSW94fSMM3fX9-yW4xNc0M8GdWQHW78srF894gm0NW7f2wpj8XfrjjW1jZysN4K8kpCf4KhgyF04) GPT-4o, a model that accepts and generates text, images, audio, and video — the “o” is for omni — more quickly, inexpensively, and in some cases more accurately than its predecessors. Text and image input and text-only output are available currently via ChatGPT and API, with image output coming soon. Speech input and output will roll out to paying users in coming weeks. General audio and video will be available first to partners before rolling out more broadly. ([View Highlight](https://read.readwise.io/read/01hykjdbwfb19ds2tfrtqj315t))
- How it works: GPT-4o is a single model trained on multiple media types, which enables it to process different media types and relationships between them faster and more accurately than earlier GPT-4 versions that use separate models to process different media types. The context length is 128,000 tokens, equal to GPT-4 Turbo but well below the 2-million limit newly set by Google Gemini 1.5 Pro. ([View Highlight](https://read.readwise.io/read/01hykjdmsqecmwq245n0mhs5j5))
- GPT-4o significantly outperforms Gemini Pro 1.5 at several benchmarks for understanding text, code, and images including [MMLU](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvqv3qgyTW8wLKSR6lZ3m7W32l32M7D7YJFVkVjfT4NLp-3N5dTXC06Qg24W3gmBjt4wvh2PW7RMH4N8p_FzZW3TJnCt7jFQ0JW97P6W727BJPrW2p4gDX316jGVW12sM9M31ZCr0W8pBYjl4kgJRpW38mnh-241hc3W4JWmZq6N3gjLW8lHZhW8SNxC2W8HgHxl1HM6hmW73TJ021By0zlW1Wdg-P24TPXBW40QH7r9fKMp3N1fbqgJy0qk4W7p3MnT65N7F1W19RHG030dfDSW5QXS4S7--GkYW3D7Mx29grxCNW7R8qNJ7sCH7jW969jDp4l9mlvW8V7zzQ6J91ghN1wkGl2Ttkx0N7kFy0JrkKkpVbTyYM6wtjB3d5Kzzn04), [HumanEval](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3k-W2q0Mk869By16W4Kp7PM6XmqRsW381M7n61zjLPW2Vqh5F2-ybbMW2yhMnL8qvLW6W1hn_Ly2Byxb3W1rDWFh3gh6QpW9gT3wf4txwZTW7qxHsD4WCpr9W2bFKmD46pRvPV1fgZw6673r1N3M9ZDh6qjVrW4p1cYH9dVWMGW4QbPyf4PXkcMW5MCBKC95BnxFW2L_7MC3dxHMsW478-NY7-6ChsW6GGYgq95FPrHW2SzG4Z2qNXQHW6TZ5d-2QJgFnW3YwVwy2r3cvMW8K9CG94cxrgZf4mZ-n004), [MMMU](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3lmW5R0kZ25ch1BKW5JBYKq7YHy7zW279xHl3hPx6mW4Cy9sQ3fhDTgV7VBBp1CjYYqW2mSF-C2qf0LXW2ls-hm5yfmpcW6SwCxh681JRWV3Vtgx4nnjhqN7v-FdKlt5bQW7W9Wzj91c5nBW4lR3Hj7qTP6_W51HxPg2wQV-HW4NS_FL5tsyb7W8S--5Z1TSsM2N29NstKRXsWhW3tB3L05x7WhRW5xny094ClTykW8XkQH81tck4KN4WcZsRYWcNvW2Gsdks92ThmxW7wLzRl7cZbWxf2bHLK004), and [DocVQA](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3mrW3tzW0F6DDsPkW60QbWv8KXg8lW8Lc0_S96xrNzVrRzLB3L6bqYW3HSf0-4hzkCjW7TbKBW8-bXnXN8M7pb1v4ldCW7zvGvq55NGc8W1_qX9X7V780DN4V_BXwZJgNzW68nZGx4cCgXwVtLTNw97VmpzVbSX2_2-2HbpW78mFCv4RM4SXW3F-Cz896bbDMW4Zp6831QbPMxVztlFD2M7q5-W47J80J8YPQ3RW6SrSRm7Fmy1NW98bKTD69KQJhW4f0Th-5Ps51VW8J8tJD4fQqp_f5wgrW404). It outperformed OpenAI’s own [Whisper-large-v3](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpW3qgyTW7lCdLW6lZ3p_W7vJfSx6X_cVRW1ChFjS1S-XfzN6c_S1PCK_6RW5S678_1cY9VWW2zq3wB2z2vzfW1skPKG8WK8TcW6slM1187ckxyW7Fh8_r2dGr7wW2_xbd-7DjXHQW2sZMMg6g4W69W9hqZcG6BlXcnW3gmjPY2PHv65W1jlFC81PHfJrW1xjPgX4kMcllW4Rl-3S68XJzmW8NkS9p3PgljRW2JKHlp8Rp2YYW3VC2v76dTtnxW268pZm6fZw9jW11sTq75wWgLkW41Xh51693y6sW15YCmx1yf99pW5G_5hT89J8xZV4_F2R3C6vwrf82nvNY04) speech recognition model at speech-to-text conversion and [CoVoST 2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3ldW4HfBcT4RPwHPW3w-w3H3DjCDvVzn-vz206XWGW27kCtw8XfVpZW5gkJgk4X8V45N5jPfJJpx80BW4xWxQ81cptv8W4Vlp207m1JfxW4nB2Yp5YGYg_Vd0vYj9bYB01W6gBXKY3z2r4lW8d27Mb8MRRS1VxkVnZ6dZXXJVDjrtC61XL-6W8FwSr92vdg_5W4QmwVh6sQzx8W1LHvvS8wXyWmN79RFN52GlPNW8QhWrS1j5G7gW5Zf44s61qHTrW59k5BS8PQPFMW2FRHzB3HNG4df8TwkZY04) language translation. ([View Highlight](https://read.readwise.io/read/01hykje27ec3mhw85krvermazn))
- Aftershocks: As OpenAI launched the new model, [troubles](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvqv3qgyTW8wLKSR6lZ3nYW5s1b2h7dYQWZW4YCmgK4HBSggW4gDhzJ3pF30BW1WCp-n3TxDzcW38cr9b2whZyJW8L8qpx2kGrKXW2T1PtR77gqzkN9k87s8M-4cfW6jSD_475dbXsW7jJkvq2bk0gSW6jmnFc7wMGk-W6f_6BD8M0ZCPW5pW7l87RHpJ5W65Qc6N776xlwW4RNJG-2McXDZW8c3V7k1HZ8gFW7fS_VY57W1dBW1FBzqQ48NwPwW4FBjzW76DsXnN8vnhcCjJmJ1N5DkCVMMHz8yW7J8cBL6qY7pSW2HhdGV710JmzW7k9nj-49L_hFW1mMVF_8RPw_hW6K1P006z8JybW42X01z8LtbvQW7TtTFs5vfKXtf1nVjYR04) resurfaced that had led to November’s rapid-fire ouster and reinstatement of CEO Sam Altman. Co-founder and chief scientist Ilya Sutskever, who co-led a team that focused on mitigating long-term risks, resigned. He did not give a reason for his departure; previously he had [argued](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvqP3qgyTW95jsWP6lZ3kLV6BhG61B9DJcN2LSdFgXwnV5VBZ8j-7KX9-tW2338SF5ckYvXW1jJQQp2H65nLW19TTBZ2q_Ts1W1ywSvM3Jn-BXW3dhVnt1qM4rHW86_Zhc1RXvGbW8SX2KF1vjncDW8Xp1Bp4YRqzCN2hXg9tqYsvFW5nzpdx97QF-sW2P3ZDS34QsYkW8x6Sc73BySRpW6rkJp38qy8bqW6mmHS95nQP1wW6_Cmwt6-T8rZN5H20JPKqBYfN1jcGyKQYqhMW70Sw104BxRvZV8dJ811lYSnsW4Tmx574jKMykW4RscLL12ZG73W6Bdqx87_RgRlW6K8B736GkP0SW6mxkZ51Jl_7jW3RBZth75mQMCW6S7ygY5n4tFWW88Tb4j5kSKXGf7YSzWW04) that Altman didn’t prioritize safety sufficiently. The team’s other co-leader Jan Leike followed, [alleging](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpW3qgyTW7lCdLW6lZ3p2W4v0zr595HgK_Vd1L6w74DdbHW3jhqKZ7Y8p4VM5BFK6JFkfCW2R4lj14qDmsGW4SLVlJ7PJ0jFN7DVRqDWmNjFW5SD0m54W3hG-W7Znw_n3P6H-TW5H7R9J5VnM05W1By6tq1Z93K8W1pP0PY5lYjhbW976bwt20KCQGW3jjvhQ2H2kwkW1fQN-Q5n9pS1W4qhwdK6PrYCFV5TQDH1J2qhXW1h-ttt4q9GC8W8LKScn2vVL2-W6C67W56Cf-9-Vj7rkb73Yl4pW2b9L_r69HxqsW4wcffy4xKT_nN3X6Szn12Yxsf4HfxgH04) that the company had a weak commitment to safety. The company promptly [dissolved](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvqb3qgyTW7Y8-PT6lZ3prW1CyXKJ4RhV97W7PbjcZ14tB3RW8Q0dtW8Z6LGHW3Xc2nt6s3Cy9W6F3mbB8qr03MV3tB1j1g93w0W6Xl5dG8NBnqZW1fWTdz66B3TdW81cg-F8lxNKdW6VvRrJ458Z6JW5jdYLw5kCwwCN8TYBTkCQxyNW7ZgFwB53rxCPW1PwyKq6C0nl6W6y6YfY55-tJrN58WV6qhFpPLW1qX2cN5048hRW41xnfg6GdJ5DW8qQvfV8GS55mW3fvxr99kJL7HW7_YKHD1HtP64W4ZwXb52nx6MCV4VP1x6sZycNN3qvZcTVjgf2W4mG2kf4NXw_mW7l4nNl3PKsR5f18ZgCC04) the team altogether and redistributed its responsibilities. Potential legal issues also flared when actress Scarlett Johansson, who had declined an invitation to supply her voice for a new OpenAI model, issued a [statement](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpW3qgyTW7lCdLW6lZ3mLVSThkF6-B23rW8hx6_18sCVjkW5sZclv685qpYW1jrV9-9dMnB6W85tLVT6ycnrSW6d63Z6912QslW6l1K3q1t0q4XF7Hssp2X2GvN65m03kfmvYqW3BB2td7yCFXyW64sVyN70SmF3W2j3qFs6K1BkjW6QfVVM13m7NdW31fpqs1NJYjDW5B8LhR6ZY3XcW2vPVzZ7gQkcSV5nWm268stTjN8nHtJs82PKKW1pr5vf6lVbXpW16zNYw71ZB-1W1tDZXC6yNB79VkB6-C1td1myW3GZqVK5pTWGJW4DVc1g7pcsvHf5vHls804) saying that one of GPT-4o’s voices sounded “eerily” like her own and demanding to know how the artificial voice was built. OpenAI denied that it had used or tried to imitate Johansson’s voice and withdrew that voice option. ([View Highlight](https://read.readwise.io/read/01hykjems7te5vc3bahk66g27a))
- Why it matters: Competition between the major AI companies is putting more powerful models in the hands of developers and users at a dizzying pace. GPT-4o shows the value of end-to-end modeling for multimodal inputs and outputs, leading to significant steps forward in performance, speed, and cost. Faster, cheaper processing of tokens makes the model more responsive and lowers the barrier for powerful agentic workflows, while tighter integration between processing of text, images, and audio makes multimodal applications more practical. ([View Highlight](https://read.readwise.io/read/01hykjev65r99bc15gcghs3xnj))
- Precautionary measures: Amid the flurry of new developments, Google [published](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvqv3qgyTW8wLKSR6lZ3n-W8V8cRq1RK7M1W1JhBBM2Z2RBYV9CQrB2pshzsVxz_qc2378B_W6Z0b1J3HZc2kW33kC9b5kM2HgW6_4Ldy3K9Q-zW20M2GG4YySW6W6LqXDC8N1SXxW3CMH4X7vj92_W95_6Ch2t_zthVx-TYN1tDPc3W7gWSQC4wtbrQN5mbTJTV9fKnW5Drjn41YH05YW6Y92q257RZl6V-Ycv13qLJFTW6-TKym22PbsWW4nynL45ggyGKW3HxkB52P7fbmW5Jzjsq8gPrhHW6_zKwB59sknCW8jLRvP3p97btW13XcW06Q4pwdW1pH5Jz8cjsKLW3cxr6l21sz-6W4tKllb4-Bx1RW2PYBK628j6Pdf27f_CH04) protocols for evaluating safety risks. The “Frontier Safety Framework” establishes risk thresholds such as a model’s ability to extend its own capabilities, enable a non-expert to develop a potent biothreat, or automate a cyberattack. While models are in development, researchers will evaluate them continually to determine whether they are approaching any of these thresholds. If so, developers will make a plan to mitigate the risk. Google aims to implement the framework by early 2025. ([View Highlight](https://read.readwise.io/read/01hykjgc788avfnzrbnbydd466))
- Why it matters: Gemini 1.5 Pro’s expanded context window enables developers to apply generative AI to multimedia files and archives that are beyond the capacity of other models currently available — corporate archives, legal testimony, feature films, shelves of books — and supports prompting strategies such as [many-shot learning](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpW3qgyTW7lCdLW6lZ3nwVTfn8Y4jcbzZW63fVkX2C_L4rN8Ss7XBtF58_W63Bwk65qTffdW5ZwRgF6pvmWTW8sVzsk69ZGnPW3PHh3q6Tfk6GVflMJQ2c_dDBW8x-XyT8t75kVW6W_k-l1s0KMHW2_RjFC2j_N_pW6BSRLG1S8VjNW6BJrXT7jgL1QW4pr30p7CmVy3VcXYQJ1lT9DQN9cpn0ssJ7jVW78SQ0l3GxWKTW7lJR4b8NlH10W8x0wJH4j89WpW2DGYkW6QWPpPW55C4VJ18_ZkkW1xBZ3P6nqpnvW4l4l0z20DS16W8zG3zK3f7kRhf6Q4Kld04). Beyond that, the new releases address a variety of developer needs and preferences: Gemini 1.5 Flash offers a lightweight alternative where speed or cost is at a premium, Veo appears to be a worthy competitor for OpenAI’s Sora, and the new open models give developers powerful options. ([View Highlight](https://read.readwise.io/read/01hykjgmbyd8a6ey47w09rk6ty))
- The latest text-to-image generators can alter images in response to a text prompt, but their outputs often don’t accurately reflect the text. They do better if, in addition to a prompt, they’re told the general type of alteration they’re expected to make. 
  What’s new: Developed by Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar and colleagues at Meta, [Emu Edit](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3pTW2f5Z4N5jpYwHW8RHXG84-D8qYW1zCM8y29Br79W52Jdjv1tsMB4W6gDfSD2Q-qwwW1B5Msb4ZSXS4W80knmy4_wgr6W9gZjqM5zQP2FW15ZtnV3snSnhW2HQmC919Z0stW2XhSGh24nQ9dVxvgLs6nc_B-W1yN3nX1V1sskW1bR3gR8Mhb4BW6K5Z9Y20JDhbW3xvc3p45YrcXW4q-YKn3gJKyYW40Xnv18Jgz4sW4H_HVM3ZG-jnW6stlVd91yT4WVRZH5T5WwTQnW3sfn6f5j-0x7f7jzL6j04) enriches prompts with task classifications that help the model interpret instructions for altering images. You can see examples [here](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3pgVFhP2p9gCS2GW4n9cFk7PVKq3W1JT07K678zQKW7vdPLv8JfZhkW5lqzJL8DZdz5W6sNpFh4rGcyVW7Lxqn27CZ-JsW3cfRxs1YY3trW3xk-H66-g7b1W2LfXQ150Ls51N3f2VBv8TP21W9fv89M1NNQ9mW7TVT__41fwBPW2KZkxL7Y-dt5N1Jmlz-VCVd8W6G8ms73YF0N0W230KYf7C9k9vW49JvKK5tL0QcMmFbmMpRgrjW33Nc_45LD5qbW6rkpFg1_y_rdW4Nk0LB4Mz41PdzY69Y04). 
  Key insight: Typical training datasets for image-editing models tend to present, for each example, an initial image, an instruction for altering it, and a target image. To train a model to interpret instructions in light of the type of task it describes, the authors further labeled examples with a task. These labels included categories for regional alterations such as adding or removing an object or changing the background, global alterations such as changing an image’s style, and computer-vision tasks such as detecting or segmenting objects. 
  How it works: Emu Edit comprises a pretrained [Emu](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3l7W71CyH66zf1VRW535Ng65bf8TfW6yQjb36lrGWQW8SqsFF4CnlL-N21LlJLVLLntW5vHj4b96DYMpW1ybSWD6Pgz78N7mkl6NK6m90W58VR_p6Z9sXtN5y7YT0sJCjYW2DrJfW4psDDRW628dLV4V_0dTW2S3ZWp7MgVL0W50wFfm8jTnswW6YJYbv2hhH-zW1V0-XG1X630WW62R3RK3lHp-cW7G963K4V7VM4W1DMNFd3nlVrCN1q7YWkcvbnFN7nZCnCx_ytkW3yqYxL4CLGw6f1S9HJv04) latent diffusion image generator and pretrained/fine-tuned Flan-T5 large language model. The system generates a novel image given an image, text instruction, and one of 16 task designations. The authors generated the training set through a series of steps and fine-tuned the models on it. ([View Highlight](https://read.readwise.io/read/01hykjk02h5ag9fbk70hprzyjx))
- The authors prompted a [Llama 2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVsvcX13SsV-W23C-8r1sNsBMV9TLwZ5fqN4JN6mkvpC3qgyTW6N1vHY6lZ3lrW1wRTXs513kyJV97KQn44wJ7kW5PSnWq5VbTB7W6cxGLd2k1v0WW5_gKn74LDfV2W5z5dPk42klKKW11NWCY6WC76dM9HvF2vgfLTW4-NbRb6-FRsSW6P4kR_1rFS6kW2PqpKj778JskW5nbtFz22fXRBW36-YC82KRVx6W4TVRQ53pYFvXW7j-jT05zZD9mW6FL9T67DCR5gVY7zDP924MGQW7wBhGD58vxZNW4Wm90T2ByTLDW25TrWg3WxT0bN2x60-TZwLckW2tHKhG8NQxZCdHCnPn04) large language model, given an image caption from an unspecified dataset, to generate (i) an instruction to alter the image, (ii) a list of which objects to be changed or added, and (iii) a caption for the altered image. For example, given a caption such as, “Beautiful cat with mojito sitting in a cafe on the street,” Llama 2 might generate {"edit": "include a hat", "edited object": "hat", "output": "Beautiful cat wearing a hat with mojito sitting in a cafe on the street"}. ([View Highlight](https://read.readwise.io/read/01hykjkyqfm6t998akx09w5yzg))

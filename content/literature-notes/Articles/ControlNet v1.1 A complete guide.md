---
author: [[Andrew]]
title: "ControlNet v1.1: A complete guide"
date: 2023-10-09
tags: 
- articles
- literature-note
---
![rw-book-cover](https://i0.wp.com/stable-diffusion-art.com/wp-content/uploads/2023/02/cover2-00004-1200166910-elegant-snobby-rich-Aerith-Gainsborough-looks-intently-at-you-in-wonder-and-anticipation.-ultra-detailed-painting-at-16K-resolut.png)

## Metadata
- Author: [[Andrew]]
- Full Title: ControlNet v1.1: A complete guide
- URL: https://stable-diffusion-art.com/controlnet/

## Highlights
- **Low VRAM**: For GPU with less than 8GB VRAM. It is an experimental feature. Check if you are out of GPU memory, or want to increase the number of images processed. ([View Highlight](https://read.readwise.io/read/01hcaraapmmcjrj7gwya5c1293))
- **Allow Preview**: Check this to enable a preview window next to the reference image. I recommend you to select this option. Use the **explosion icon** next to the Preprocessor dropdown menu to preview the effect of the preprocessor. ([View Highlight](https://read.readwise.io/read/01hcaradhpcg3p40c4ngvga3c9))
- **Weight**: How much emphasis to give the control map relative to the prompt. It is similar to [keyword weight](https://stable-diffusion-art.com/prompt-guide/#Keyword_weight) in the prompt but applies to the control map. ([View Highlight](https://read.readwise.io/read/01hcarb8rvefff93r2v4gvvdtk))
- As you can see, **Controlnet weight controls how much the control map is followed relative to the prompt**. The lower the weight, the less ControlNet demands the image to follow the control map. ([View Highlight](https://read.readwise.io/read/01hcardasj5hsb2sgpaadhxcd3))
- Since the initial steps set the global composition (The [sampler](https://stable-diffusion-art.com/samplers/) removes the maximum amount of noise in each step, and it starts with a random tensor in latent space), the pose is set even if you only apply ControlNet to as few as 20% of the first sampling steps. ([View Highlight](https://read.readwise.io/read/01hcare94yz34cpjdfaa670cge))
- **Balanced**: The ControlNet is applied to both [conditioning](https://stable-diffusion-art.com/how-stable-diffusion-work/#Conditioning) and [unconditoning](https://stable-diffusion-art.com/how-negative-prompt-work/#Sampling_with_negative_prompt) in a [sampling step](https://stable-diffusion-art.com/samplers/). This is the standard mode of operation.
  **My prompt is more important:** The effect of ControlNet is gradually reducing over the instances of U-Net injection (There are 13 of them in one sampling step). The net effect is your prompt has more influence than the ControlNet.
  **ControlNet is more important**: Turn off ControlNet on unconditioning. Effectively, the [CFG scale](https://stable-diffusion-art.com/how-stable-diffusion-work/#What_is_CFG_value) also acts as a multiplier for the effect of the ControlNet. ([View Highlight](https://read.readwise.io/read/01hcarht30bj2stp8dhavsmpj6))
- Copying human pose
  Perhaps the most common application of ControlNet is copying human poses. This is because it is usually hard to control poses… until now! The input image can be an image generated by Stable Diffusion or can be taken from a real camera. ([View Highlight](https://read.readwise.io/read/01hcarke9z7xkfs7z0n0zfpe2d))
- This tutorial won’t be complete without explaining how ControlNet works under the hood.
  ControlNet works by attaching trainable network modules to various parts of the [U-Net](https://stable-diffusion-art.com/how-stable-diffusion-work/#Feeding_embeddings_to_noise_predictor) (noise predictor) of the Stable Diffusion Model. The weight of the Stable Diffusion model is locked so that they are unchanged during training. Only the attached modules are modified during training. ([View Highlight](https://read.readwise.io/read/01hcarp7my9jhe9b7an5hpz22x))
- The model diagram from the [research paper](https://arxiv.org/abs/2302.05543) sums it up well. Initially, the weights of the attached network module are all zero, making the new model able to take advantage of the trained and locked model.
  ![](https://stable-diffusion-art.com/wp-content/uploads/2023/02/image-139.png) ([View Highlight](https://read.readwise.io/read/01hcarpdjvb0zht5xypysgszfk))
- During training, two [conditionings](https://stable-diffusion-art.com/how-stable-diffusion-work/#Conditioning) are supplied along with each training image. (1) The text prompt, and (2) the **control map** such as OpenPose keypoints or Canny edges. The ControlNet model learns to generate images based on these two inputs.
  Each control method is trained independently. ([View Highlight](https://read.readwise.io/read/01hcarpk8d85c5j6w9zbdv0ny4))

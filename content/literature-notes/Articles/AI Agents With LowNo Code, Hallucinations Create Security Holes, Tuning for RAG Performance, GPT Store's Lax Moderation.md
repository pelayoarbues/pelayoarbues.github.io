---
author: [[The Batch @ DeepLearning.AI]]
title: "AI Agents With Low/No Code, Hallucinations Create Security Holes, Tuning for RAG Performance, GPT Store&#39;s Lax Moderation"
date: 2024-04-30
tags: 
- articles
- literature-note
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)

## Metadata
- Author: [[The Batch @ DeepLearning.AI]]
- Full Title: AI Agents With Low/No Code, Hallucinations Create Security Holes, Tuning for RAG Performance, GPT Store's Lax Moderation

## Highlights
- Multi-agent collaboration is the last of the four [key AI agentic design patterns](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3phW8stXPb1B5Tz0Vhv1K88t_lypW7-skMV5HFS1TW7D0-Tj6qzwCMW3W_Y7L2dMr8CW7lv5JL5Z0jw8W7Pn3Cs3PGhgQW7fcDZN4mW6ZFV3s15142nQW_W4Khh495-v4rtW1sk4tN4m84jyW7fQ6pT8gM7mmW5RlBB813swQJN87K-H0bRjLSW4ndTDs6vSzhKW5hW5Wh1f7rj7W8rWkNG3Nj58ZW6RGHPd2hzJD8W5xGMPL8M--nnVm27VG7ww0xWW8mJrqG2LrTh3MgLspr4W_9dW4-6VNx7BszXxW3hMhtG1zqP9CW8vXPHZ6zzXWQW6SpC191LfwMvW5Sl0xV8jGwNGW3HD9tZ2bBzjgf85-xfF04) that I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks. ([View Highlight](https://read.readwise.io/read/01hwnmzgjha0zhdpk95gfy4506))
- Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..” ([View Highlight](https://read.readwise.io/read/01hwnmzn14snazty9tvrbw3wkx))
- It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons: ([View Highlight](https://read.readwise.io/read/01hwnn015y2mqn1qqw5fcce5yy))
- It works! Many teams are getting good results with this method, and there’s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent. ([View Highlight](https://read.readwise.io/read/01hwnn034rh6fhwz7h8b83vfdc))
- Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role’s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, say, scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better. ([View Highlight](https://read.readwise.io/read/01hwnn0np71cx277km4n663tjk))
- Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well. ([View Highlight](https://read.readwise.io/read/01hwnpjj6wc3fqpwfbr6z6gke4))
- In many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows. ([View Highlight](https://read.readwise.io/read/01hwnpk47ck7fnkbaev9t6p0ne))
- While managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to "hire" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans! ([View Highlight](https://read.readwise.io/read/01hwnpkd6hj3ajpfrd2xzss53j))
- Emerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their [GitHub repo](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3lJW1Nw8Bs4G0D3qW4J8pWz16GyV_V3BFjv8GZwXrW1njfdb6cpDgyW26zth_2f44tXN18sGVT6H2N8VCP0wb168S2tVrgZtX5DkcHMW2Vv-5w5TwrvgW6QpS6h8bL2Y7W7sBfhx4P1l8wN86pxcBXGt0DN7jTVRtMHytvW3Kpb9c3ty8tRW4n4PtH1B3rlPN2z1Mg3V47l8W6DWdFr5lRbYCW7H2q4m6Mqbq9W2LTFz639H3w7W1KLQrz8tgrYJW1r371Q1XzkrCN5S2jBdJLqnDf7GV9hv04) and perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does. ([View Highlight](https://read.readwise.io/read/01hwnpkp1y7jj737c4p542y6fy))
- Like the design pattern of [Planning](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3lFW8yg0H822Z-b0W3Yr2q32ZB9qxW7GTCyL8vNjXBW396DRf6mZ0QrW41V-Ss58RjhwW7k92nQ8dbLfbW2GQMXl5S5lnvW1lwLF938CdJtN2QsBWG7vcmDW8rRHX89j56sTW4Vwq_M3ZgVNyW7gFVgG2CZq1cW8b7C-07gK0_XW105mxG7kZm55N865XjDjQkYyN7w4DCqn6t5RW2h3XHG6v6CfVW5MhGlp6JfvYKW2bJjBH4YwP64W5FwnHy2tj3Z-W3FLRq79cZ8LZW7z9qNd65qHswV4d25h11NGZvN6hcznyYvVg1W3LQldt66950dVpgsKD19GrzDN5gXqld8vbl2W6Xvj105w0mfMf7JlWkb04), I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of [Reflection](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3plW6lZjT_4f_4QdW2-N0Tb7Bqq0fN6p8MYR-SnTYW1pKVxZ5gPlQNW92mtFg6hgV_jW6JChxs6BM17PW2Sz76p2hWqGfW7wdQRt2sP-mlW7L1txd49-XTyW6BTWXK2m0g78W4p2G673CQtsbN1d1r0s2V2WqW41R_Zh2RGswtVFqQQ1639qQdW7sk6Vf5BvsjJVtJpWS5x8kt9W8Q2Qlg3xyWTYW2nZT943Vgnx9W5Y_dgd733DwkW3xBLDF1mC-z1W2jYwCR92mRRQW5xzLSY518n2zW41YCS86HKzW1W6Kr2h47QyMkFW2q9gYL7NYjq8V7sPCV83ZGbqW5NJKTT6hz1jTN8Gyb0MMz0Nbf8vHV7P04) and [Tool Use](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3n_W3J-F7H1LXZlLN6LRrF8dNJCSVs7P8R5XNwS1W328pPK2lQkw-W2B8XNX5Bqq5PW3733gP5hDlhQW1c9nsD3_7Pp2W2nyG7h2JrwB9W6Q3bhb1VDCwpN2DDcTKXxWrhW1L6sk-5d_jWJW5Jm_Zr1fJJW5W1FDsT617blyYVD7H5f8Tp6j1W7HXZs11Tn4PpW17LWKP4T9_YPW7vDnQ98zGCBSW8yZXsy6j7pDPW70npPX6H5h_2W3WR5x78KYD2PV5-_g35SBLM4W4W2PBm8cmZzRW8pJnbM7DwwMbF7TcBsmc42_N8Dpxhk15Nh7W35kLZx4scMTGW3Yz7dM7jFG_mW4wyRlT12QCD0dld2JP04) are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! ([View Highlight](https://read.readwise.io/read/01hwnpmdt8bn1q89zp6pkhpfdc))
- P.S. Large language models (LLMs) can take gigabytes of memory to store, which limits your ability to run them on consumer hardware. Quantization can reduce model size by 4x or more while maintaining reasonable performance. In our new short course “Quantization Fundamentals,” taught by Hugging Face's Younes Belkada and Marc Sun, you’ll learn how to quantize LLMs and how to use int8 and bfloat16 (Brain Float 16) data types to load and run LLMs using PyTorch and the Hugging Face Transformers library. You’ll also dive into the technical details of linear quantization to map 32-bit floats to 8-bit integers. I hope you’ll [check it out](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3pBW96r-CR5LLtN-W4wkX5s7RN_YkW7GJdc-1npN9cW65yX-487p8SpW7_8Hjz780gsMW86KSFq1Wyc_dW6M2ChK5SyGhCW4dwr9981zZ1TMV-Ls2RwsC0W3Z8v182kc-KvW1kLtWq2q_f9VW7xDjcP1-7yV6N8K6R7Mj9L2lW5DH0hK99cKDGW5WWRth15Y8tyN8t6XWs9qj0YW5LMtbq8fNZV6N214wkDg4SG8W5NMXQR86-Q2QW5qttmz9bM3dWW6L5k2L99sNlgV6s3gM4GvngqW77N2nn8F_ZfWW2GkP5f81zK45VhP4Yl6NRdXrW8hBP-P5YJg7wN5g8QSXJsdn7W4-v2my97Qb0nf6mKC2n04)! ([View Highlight](https://read.readwise.io/read/01hwnpn30ngb1ca4ey1a7c3k0k))
- What’s new: Google [introduced](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbsT5nR32W5BWr2F6lZ3p2W3kWnZb43sFzBN1pkXG2rNzfxW8jB-Ly3Nwy9GW6YXq1g8Jqb-cW5vK3QN663gYsW1jmB1H32k28rW4MXZWZ4DZNVbW30mz0r6x3hHGW2p3Slz2n5M4zN4lPjQM-YWCMW1lgQpl2-j9jrW8Mt4hZ4k-mZDV7K88_6bMrzWW3Bvp388fP_kHW1Nb_xh197HDDN7lngDqTchVmW597Xv07QK4HfW3c5xFX7S_sBMW3MPxHn1Sx7JLW54wDfC6TVNG_W3StVTS27x7-xW3Mm9kY5dWgZlW8Tvffc4yqtRLN28FXFY6ly2JW6dq-b67fJ5QnW1NmCsV6wpBVmW3v5tQL3dYLmKW4vgcds6Wv74zW7TG-Bx6k8gKnW18gs4s7yLRgjW5RyKPD8bPpsxW4dv5Nv832CRjW4Ywkgw2WtlGfN4hG-BBKR4jhf8z3FGT04) Vertex AI Agent Builder, a low/no-code toolkit that enables Google’s AI models to run external code and ground their responses in Google search results or custom data. 
  How it works: Developers on Google’s Vertex AI platform can build agents and integrate them into multiple applications. The service [costs](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbtM3qgyTW7lCdLW6lZ3kMW87v0q15G8yHyW91QXkM4cyvdCW1VGQZx4hjQV-W7F5Ckc30B1Z4VYhtpB4cXZ2jW7SNfpS55qQk_W1FvxYt6_TpPQW4_KJkP9gl-mDW2vnJSm3N8jKBN4f3PgkntTYbW1kNlfm8gKJ2_W4_7NsZ6MqkvkW6H-W8T6FfrwcW63tRpQ8tM3HKW2rnqrq13vQk6W3yqt7L4ZmSjGW3lHGMr3sXS8wW5dt5sF62Y6MfVcjFnh5Jfs4KVWPDyy7KThWpF1ytnJcn_-HW2GkJ4R53TJgTVlZpFs34ppw1W4kt7dm7w-8KMf1n-ybj04) $12 per 1,000 queries and can use Google Search for $2 per 1,000 queries. ([View Highlight](https://read.readwise.io/read/01hwnpngxep4v3v1d2bw3j8v1n))
- The system integrates custom code via the open source library [LangChain](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3kxW6gDpS-9d3vsdW69rRK37-j-y4W6s4wWc3S1hnLW9hfHgr7DgMKXW3QQpJ821GydbW2H7PC55fgzycW8GGQYZ3dBC_4N1K6BxCD6fMPW6DLwhJ5K_-Y_W4ndnZz61wlQHN3YzJYfwqpjGW3BYTZ02l7X4DW314gTF50g7HdW7-KLjb18khJ8VHzksD9h3zy1W8sK5b38xpB8nW836R9J8X7_6wW5DQHGz8W07HtV6t4JL2z761SW7WYG9d3CYmKZN1kqCkmYQPszW8wtPcr4G-jq3W76dTN68jK7KcVGcbfs7Zk68tW72QBWR3kz7-jW6NWrWj3T_-p6W2zmnkN75y7MyW1yGTbb10m5Hkf5xz_Zv04) including the [LangGraph](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbv23qgyTW7Y8-PT6lZ3mhW5nPqBF3QjHjbN8y0q6WgwlN_W49pkVx1HK9sMN88rj1WcD3PNW6lQmKc57GP6QW1YY7VD6xzZBDW5hlHwZ6ytTZ6W4qgdXW6xknstN4Q30Vfr7XDcW1f3wYQ46gfWYN4zWvLTL6v_4W5SJ_Z06mYY2vW5BbBSn5hy_1sN7bgQKQTkynnW76MzN83R0YczN47JPF6NCF5KW1j1scN7X-W29W4WLf4n3NL8DMW3QTvPT4NTLkSW5pp2Yk1MhsSgW3nbMpj20Glr7W6Cs97x6hq71VW1Q_7Zq7CHkhGW2fwFTk8L3vTLVxRBBN8ggl2wW9f0HHt1xZL17d195sn04) extension for building multi-agent workflows. For example, if a user is chatting with a conversational agent and asks to book a flight, the agent can route the request to a subagent designed to book flights. ([View Highlight](https://read.readwise.io/read/01hwnpp356rnadyx8p6r1v177h))
- Behind the news: Vertex AI Agent Builder consolidates agentic features that some of Google’s competitors have rolled out in recent months. For instance, OpenAI’s [Assistants API](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3lLW2KYwDr694RStW4-DwjK1FSckSW3Z2WND8q0JzgW4knjFg70s76qVmr5341g9C4bW5MZrhP7XBLDmVz6HQ99lctrCW7tZq5D1_8fKJW2JZ9jm1XVwn3W3TR-l54k8jZlW97QGtH7RRr7mW4Yj_G774Xk7FN6-sLWx9-SrxW7KY9X26rMSXTV-gKZv28MZQXVJdDF54NqwqxW4B-X3L3Q32lmW7cgJ6K5SLMRtW6zHY562MkzXWW6-0bzb4-_S4NW5QNY5G1fl6FNVZzHW42WxljcW8FBBvV7g1V7WW26_8mz1G2ZYYW8-Mz_x8CTRgXW8J4lmT5828cgW1W5gV64Mk3GWW7dwLX93Xq58tf5qH9Jg04) lets developers build agents that respond to custom instructions, retrieve documents (limited by file size), call functions, and access a code interpreter. Anthropic recently [launched](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbtM3qgyTW7lCdLW6lZ3mGVFvnnt6WW28DW57t2ly8H2WZ0W2xD3tH5rDsffN69GZ6T7bhrdW8T7lvs74fGl1W618JgL55_rCqW2gG1ZR6s36gpW293CX35lVsN5W1RfG3g4FMpsBW6Kdnlx13KvnBN7rvdlJbkSHXW1p29V95GpSz-W11Zxfn6nDbhkW32p_G56V1KpFW1hy-gV8QvgYMN31bRrHDKg7kW6vr3dB87B5zmW1sdTvk2lrXKXN3nTv6H5nT-TN6-FQBDF0rzDW7r1cm_59qfnDW6nSsfz6fyPWbF8j0VspWTptVTcjST906NVsd7NlNb04) Claude Tools, which lets developers instruct Claude language models to call customized tools. Microsoft’s [Windows Copilot](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3lGW7ClNSX3qSsZ1W7-0gCy4GDyVtVmQzbZ7mfF7BW3y6JHY6gRP2RW4LDpMY7P9wCHW3WV4wb4JH-C_W14H7qB2-cH2vVtlWLH1sMyZ_V8lQ5Y8SnT4yW8L0-Tl6Kmg1gW4xw5_9597zFHW7RNWg2474dlkN98kgHZTM2LcW3gKNrq88dKBTW6bvvb03cLW8ZW5_pDpD2qjBdWW8FR9w431QwffMRDBbqlm6DTVxXyht8ncdq3W408Fvw7ybjT4W4GLLkl4q2VvnW94D6Rw6PdNVRW3z4SMV8GhtJBW2vW57H4fFpbTW7C5QZk8d2TPwW1g3k885KkGKyW3wwgKp6K-WqfW2Fpsct91fzfZf2gH3TC04) and [Copilot Builder](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbsz5nR32W50kH_H6lZ3pgW1ssLvg4Tm_gwW8VjfRJ8s5XLcW2kRBG81bfRMnW2Z9gbG4WVxd_N5tKHVHD9cHjW5chbS85KYtjLW7P87lh8ZnVChW1g5DGp2ttkNxN935b2N-gbzwW7rP06S5l3yyHV1zFcG29pBFTW5-YK3s3LYQdBW5n1F8G6QM1TQW3FVS0H67fXrkW6mZK8M2JBsk9W36rQBx62pC3BW1n95Xg2g_XRKW3dz8LT7Ftd5VW95HD4D5rlyj-V9B3hL8bGfwBN3Y5hVN8TPKbW3RTNF68CH078W6vd4hc5S822LW5CfRDL6SBJ8vW86wWvn64mX6rW2_T1GR3zmknvV1nrsW1l3zbnW1vRVBp3zbstfW38nKrD7z72qDW8m6ckc5xL8J8W6rwDx_7sc2XnW4qRkld51fD6Kf41Sgqx04) can call functions and retrieve information using Bing search and user documents stored via Microsoft Graph. ([View Highlight](https://read.readwise.io/read/01hwnq0fg5wzfq37asrw0dtbne))
- Why it matters: Making agents practical for commercial use can require grounding, tool use, multi-agent collaboration, and other capabilities. Google’s new tools are a step in this direction, taking advantage of investments in its hardware infrastructure as well as services such as search. As tech analyst Ben Thompson [writes](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbv23qgyTW7Y8-PT6lZ3lFW78rSZZ74cJnqW6wLmGG6K--mfVgHR012XYp34W7LG29M18_jWVW2cV4cY8ZJXrXW7-FQz97GN-HzN7GzRt95QPQmW3Cdll37SBtmNT1h823S6nYMW7PTM7S7bbd5NN8jNvX9_dHyHW6RML8k8L1bvYW1rzt4s1GvQrjW6ys4Rh2bCskMW1Cmjg94HlbQ8W4fM1R02gf-GYVpmQ2V1QLpBlV12qX-7RBDNYVz_FtW2RzrGbW1ZGV_j1VM4CbW5YFkRV4Sxw5rW7R4M8W62Y4hRF82JXFZh704W1_8Jkj45myC4W5DF7Nw1BTGMXW4nkzP17XZ44cf9lDyVP04), Google’s combination of scale, interlocking businesses, and investment in AI infrastructure makes for a compelling synergy. ([View Highlight](https://read.readwise.io/read/01hwnq0k6f8yt1vpn2c411e0my))
- We’re thinking: Big-tech offerings like Vertex Agent Builder compete with an expanding universe of open source tools such as AutoGen, CrewAI, and LangGraph. The race is on to provide great agentic development frameworks! ([View Highlight](https://read.readwise.io/read/01hwnq13z9jhe1wtkkp35zbb32))
- What’s new: A cybersecurity researcher noticed that large language models, when used to generate code, repeatedly produced a command to install a package that was not available on the specified path, The Register [reported](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3ppW5MbZBF2YvY7TW1QG7YV3pfGWjVMkMxF4JJ9lhW40wpnX4ZvbCXW6Y6zmT856JWYW1sQB5s7n4XpSW4fSmfG6HsVSjW8H0dS_6M-DZTW4kXhpr6kHH4dW20NBPW6ff55YVjvWm55vF_F-W7djmmg7LNtfFW5V1FHH89Vw30W477PYF2yBNgPW71B6Gr5t86fGW4qNP2D6q-8dYW2ZFyMr2BtDRZW6_mrMS52wpcsW87S-8t44pZqzW5-R_WL5dz_sYW17wSMM6w-VQmW3FfsTz2ZQTp9W6HBHV15m3L9jW3JrNM-2CtVfcW148kbL7ldq8mW45lj-y8CqTp_W2SrVH-7YZsYGW4RVfYT2T3cLYf7gF8yl04). He created a dummy package of the same name and uploaded it to that path, and developers duly installed it. 
  How it works: Bar Lanyado, a researcher at Lasso Security, found that the erroneous command pip install huggingface-cli appeared repeatedly in generated code. The package huggingface-cli does exist, but it is installed using the command pip install -U “huggingface_hub[cli]". The erroneous command attempts to download a package from a different repository. Lanyado published some of his findings in a [blog post](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbv23qgyTW7Y8-PT6lZ3lbVHqc1L6fsKmpW2F_x2S3zXMvZVmfcpL8HYG-XVHHrzr3DWKR3VVpyQS5LcLx1W48lb5Q6c7HWRW442NRF5MxVpjN3NdfgtctlnQW3VfH-P2_G0NyW3Qp5dJ7kmRs7N21ssZfbpBj9W8GJXL72HjvBVW8ljg0N4VJ0TlVp3V3R3B3KPSVN0D4y2TX8c9V6wx3P5HqzV3W4fh8YW65WKW6W2gGq2z8QfDsGW1Hf8H89dnNqwN6DbwsfQyq4QN2Y-3b6TsP0TW2BY4bw3BY0ByW5QPn1B9jqY7ZMRvMRjwFTFjW4pJMBm4jVw5mW1sgXxp4wWdChf4vWfWR04). ([View Highlight](https://read.readwise.io/read/01hwnq1hbdea2f821423w5a79k))
- Lanyado uploaded a harmless package with that name. Between December 2023 and March 2024, the dummy package was downloaded more than 15,000 times. It is not clear whether the downloads resulted from generated code, mistaken advice on bulletin boards, or user error. ([View Highlight](https://read.readwise.io/read/01hwnq1x06vzvy2yd01hz0w2a3))
- In the short course “Quantization Fundamentals with Hugging Face,” you’ll learn how to cut the computational and memory costs of AI models through quantization. Learn to quantize nearly any open source model! [Join today](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvl3qgyTW8wLKSR6lZ3kPW9gF6f47-njnWN8QZtTfzzd3xW1cwM5F4BlnS-W6qbmZ58PFgSZW6zXlBv5LC8N3W5BXMLP2MJ7V1W2L6vhN656xKXN1dvTtB5vLvGW2y4g713BJbKKW4RQs9y64JWYRN7l7lJdKKxgcW1lt-mm2jTmlVW6c3bBs7NzY6_W2Sx7BL5Lxh5QW8WRZp42-cRf7W67Zb224vzCs4W4QRx656wYWBsW8926l61F8nYsW4FJ3-n16W9shN3YVTKRxKMF4W2_lGgF4cttSCW3qZCDg84x1dKN7R3KNQc0Y8tV4ZM1M4GBmF1W9hQ5-K95xv5kW7txYQG5xrBspW4Fb0132dZVSjW6wfffp2ncHYsf2pNqjC04) ([View Highlight](https://read.readwise.io/read/01hwnq2gdny7488djhtpj4bx6r))
- Retrieval-augmented generation (RAG) enables large language models to generate better output by retrieving documents that are relevant to a user’s prompt. Fine-tuning further improves RAG performance. ([View Highlight](https://read.readwise.io/read/01hwnq2w31sj9d3t397rbp2j8x))
- What’s new: Xi Victoria Lin, Xilun Chen, Mingda Chen, and colleagues at Meta proposed [RA-DIT](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3n5W4CxJCy5-2ym0W7yttF057kwnBW3B-Mxj6MD7PTN53hj4cZ5Rk3W7Qrnb63BM1rfW4NtCCB7zZ36MW8JGxqD1PZCLnW6WdGfs90c0VwN32V92Z2YdzcW7F75Ld3vRB_TW5xgh2m8-F342W2p63Ph52BR3HW1RBnZR2b5VQtW2_n4Y15vTp3YW3hVNH53ZTWKBW1KXQT887QG5GW2GjsJs17hfbyW5kQV375prVNsW7d33735fYcYCW60T96D6zmSK0W7h6FrG23qd5rW63cvzf5qKfxwf4VnDfg04), a fine-tuning procedure that trains an LLM and retrieval model together to improve the LLM’s ability to capitalize on retrieved content. ([View Highlight](https://read.readwise.io/read/01hwnq2z456hg274ftvyh108he))
- Retrieval augmented generation (RAG) basics: When a user prompts an LLM, RAG supplies documents that are relevant to the prompt. A separate retrieval model computes the probability that each chunk of text in a separate dataset is relevant to the prompt. Then it grabs the chunks with the highest probability and provides them to the LLM to append to the prompt. The LLM generates each token based on the chunks plus the prompt and tokens generated so far. ([View Highlight](https://read.readwise.io/read/01hwnq35wsawm0r8z1pkbbwq6j))
- Key insight: Typically LLMs are not exposed to retrieval-augmented inputs during pretraining, which limits how well they can use retrieved text to improve their output. [Such](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbtM3qgyTW7lCdLW6lZ3m2W88sd2_5XZ6r_W96jHkp2CyFZ3W2j57Vw6BY3HVW8dCTXN6-w5N0W1S9f7S5nFywPW8CZFwg5LJzx3W4jyMpZ6ZYF3jW1jYQ-_385L3bW6Sn5wm1jPCSvW5t1ClY3bYLllW8yclWL8XPdHbW1-KLnx3ddfhfVr1HRf2bTmhbW6y7L_l18Td7tVw6b8G6d-SSWW6nQTZ87VvNDNW3FvB2w7sc1qgW8XdLQX2KZPlpW1CLNkk5HMxZwW2J-rdc8S4v0tW6R07NW7fQMkNW19pFtF7c1TjsW6XzzG8425jkSW77-qnJ68nzVpf80M-fY04) [methods](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbtM3qgyTW7lCdLW6lZ3pwW2LpzFB97bqy_W3VN2Wv3NPM5BW8XdpBm31-3KGW6tzyfj8zF5DtN6Jf_jmdZZcMW52Ybkg8J5D9-W6t4yC-1_L3xpW2q0f3643s5JNW69jnpg8q2sbdW69MV031MbJpYW5tHK1y8kb_S9W4ZX5SZ1xHTWKW735f322dM3w0W4xZwY51NkM_dW6Px5T_7DYVLmW6rHKb_3tJCQQW3LcTQK7BbZwpW75f3y95sx4kLW2r2YjM7NxkBdW5RfqH31GBQXqW7KfYv56mKWDtW5R_c1Y1Wn-GtVW-dSS1BfzwcW6f4l6R7bCSHVdJ804s04) have been proposed, but they’re costly because they require processing a lot of data. A more data-efficient, and therefore compute-efficient, approach is to (i) fine-tune the LLM to better use retrieved knowledge and then (ii) fine-tune the retrieval model to select more relevant text. ([View Highlight](https://read.readwise.io/read/01hwnq39h6v0t8s388evczxz5j))
- How it works: The authors fine-tuned [Llama 2](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3lmW8l7q_r5n69k9W69yxlR4KG4kXW2gRqLv8bdYXHW8PwKpd3nrl9PN7vDRyyHf1NSW5YlssW6SJHl3W5YPYFd2V2x1WW1c3Vp68mS2z_W22TbFm3LP6l5N2LxS_kBWd16W3jgFRb6X34-mW6-VlvP7-yl4mW6Z7yTd1Nnr1-W6ph0V_582PDBW11DnGY25-3QcVdlgYW20MxckVDf48R7NzkP_W8yX0d614GcRDW2td3W55Nkf47VWvkGw7t3PyJW5xfFKN2SvxhvW3NjdNn78htQ1d7PmQF04) (65 billion parameters) and [DRAGON+](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3l-W7sFRB379Lsk_W1pQc-g6fZ50LW6rzhQC1WFtCdW4cbpW-2GZy27W2_7lSn4C4wdSW1wWxfk8R2yqWW1mWlPR4430W_W518b5x4dHhHbW1vlXZ67kdfs5W1kty4s7JDvbBVcD8km7knfDbW5LhzYg93XKwRW4n9FYY8881T5W5z-zCZ94sdGWVQRkmp3NplYRW8Fr9Ds4TryLNW2WBqcg8xxcklW8yYhxp3fP3pgW4QSBpC56b8HLN5gK2xzqLz2-W97L52m2sd-KgW2mBHVk4vcg4Wdn-Dxn04), a retriever. They call the system RA-DIT 65B. ([View Highlight](https://read.readwise.io/read/01hwnq3mzv0h942n6643jrwy0t))
- The authors fine-tuned Llama 2 on prompts that consist of retrieved text and a question or instruction. They used 20 datasets including [dialogue](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3kKW4VCVvT9jJ7RpW6KNMPW5JQfg1N8qf_bG-NC2HW8Lp8KY3sw9J0W5xzK3H2jSPRtW4G19123bKRczW3WL0XC58XRD6W1v2zc9921Rb4W48505z4VTcNSN8TTpyNkYqRLW3SVwwx5YY2rbW5v-mq16DK-Y7W67NM135Bw5qWW7hYPtK5l_1wCW1pxYc95G9fd4W1HdnMS5gCV5vW8FHzP9890c7CW6BX-qp3F2qVRW5pSZWx7pQF2vN6ch7dpnHMfWN37dtSShV8rWW8WVdL23PP2Mdf1j883z04), [question-answering](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbtM3qgyTW7lCdLW6lZ3l9W54QjDR8JYjFGW6Q3G5Y60Pj6GW4dVkz61Ry2KCVCbPdQ4JbTpZW5m6YbT4lyphCW48jfgr8LzlcwW3TFJFm1gHxFxVD3Rw85tHyHwW1KKJ482zrgZKW8mkcbG5Wqbc2W74xTw-5N1bQnW86vWX-9jKty5N3BJ3xM3QF33W3lWkWP7sKqSGW84_tzl5zBKwjW16sFfT1ZsKQcW9dqvMk6LpfS9W14WfW73stz1rW2nkt_b2dVf6mW7qcsKl21wcyBW9gZ0D12_kNY2W50yTfb5qhrngW8LFsbc3hX9hWW8lQmcr1C1MWDf9jnhfK04), [answering questions about a given text passage](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3mxVHF_lt8tCn6jW3v-MC93Dry2vW61D3Cc8y369JW1hDR5z5-0_DxW10y2pr6RGBVlW2LcchG3X2l2FW8gW5by3bLYXDVkhN874CC0Z0W5xSX376nyFYHW10SySr7k3QVTW7jNRbx7PqPY7W78rrLd8jN577W7v9Mq93nvg3hVcMff556TVdyW9gGTLq530fhnW1yB0qX1zWtbLW1_yf_Q6ZwsJyVslYm58mYSy-W8-sBtf92QWy1W7VXT_45kgmN7W8z3jZW2-dMDjW5kqCk81bk_Pdf7yRDB204), [summarization](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbvF3qgyTW95jsWP6lZ3nMW4TymzR97QJwyW2t4zxk7_t3T4W4f_B4Z6M1NjSW5RHtjf6LrdhHW3Jlrp770-92kW8Zb5Tq4TWGk7W8GpZ624zQgl1W2BYcLG2xvy8KW8GQYhG5zVXf0W5P_LNS5hqqMQW767k5F7SHTZZVm77z978p39gW22Y1db7_QV7pW9cl8DV8DLDxSW2w_Phj17QQl1W8G2gfx7Yqq1HW5KjZJC6zKsq2W1D4mWl3_ZDdPW6yt1h63c_7dnW6lQtJJ42fm8DW3p8Swq2wVTd8W751tMT6Jn-zJW2yR1dR9k8F0HW6_WBs93MqmVLW1bdpmw93rQpCW87H7s21PQhq_W6X4gWC6v7mH3W1D5Qm29gFHKYW2Pxzj74BRmNBW4vvRK94xYrHMf42hX8H04), and datasets in which the model must answer questions and [explain its reasoning](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3pJN2h-GrP_8h-FW4L8CGy3ZBL1xVqby3l4h12f2W35t4XF2HFmx3VWwxdZ3dr0dbW10Pbgq3vTN1DW4B2V-34K7pJyW18Cwp241gz72W62Cm5d2dqPckVpX17X3myN4BW3N7zdS25Q546W7286jd6Llh1NW8bbNgH4SLXwlW3pG10z763HHwMljL1DCKRtnW1f4pGT8wDcVYW8bHZcR4xnSX8W5KfdDj1hyz7pW8h_KKP5sLZQJW40YcjP8wJxswVBvccD3s5Ms8W5DyfCQ4YSbZZf89Rq8804). ([View Highlight](https://read.readwise.io/read/01hwnq3q37naanv0ge709kgfkq))
- They fine-tuned DRAGON+’s encoder to increase the probability that it retrieved a given chunk if the chunk improved the LLM’s chance of generating the correct answer. Fine-tuning was supervised for the tasks listed above. Fine-tuning was self-supervised for completion of [37 million text chunks](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3l_N4C8J0M6Q9x7W4fB4zr3ZNnBqW56Gq4p95mjz6W1cd2nY2FcyVyW89BPQc4fpfwRW48ztR21KbYTLVccn1488mS3RW4558jd75Cxw0W8MSk232wyty-W8Jdh-N2SKqL_W7RPDhp4bVqHXW3-Z55m4llgSHW99zSTx6SHpX0W2vsGGy1GX56ZW1sYmW113CnFnW2X5Cfp7wsr-jN55Qf0yNgs1_N6RcF5XVjghqW4k4-sY3_Xy43W4sZ5SX2-B7SzW7YT3q78PmkhcW1VvDXF3rCj9cf4XcN1T04) from Wikipedia and 362 million text chunks from [CommonCrawl](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3n7W1Nxfsy4ylTRdMfr62LjNwf6N65r19yt2wvWW27Rkf92fcWg9W6_JNcf1wdnlHW6Dt--58QlXhvW5tNJTy8JH0-QW40psWl6b5M99W5f2sN-5FNh4yN4PN2ScRT7t4W2vZm-W7yrTbzW942WYC2xYcmzW6q_NlC2VYh41W8_8C2J3B2mzWW51_CDq6jZCRvW3_Dzw42W1CLbW4dW-65136BBKVVW95b7r3506W2LH1Fl9bvPjbW6T-f6d89bPcWW7CJ_w42SY_GXW1Wtc2m3df0lsf5G7fQF04). ([View Highlight](https://read.readwise.io/read/01hwnq3wa8f3t9e3jdcpzymrkp))
- Results: On average, across four collections of questions from datasets such as [MMLU](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3lmW7S8r7F4VpTWcW1Q3NC29fhXt1W7xjx9L93z1_VW58NPYJ2yknKYW1V36xl8RtGPtVzjn0p7xmKtmW77KsRk1pbMTfW4dX3W575q-FVW5BjfSc3VH2JKW7YQ6PF7kfp1vW4XZmP042-svkVPGSS72sDcYHN22-1w4sMfgCW6nF1bl94JWYzW8k1DVb1mZ2v4W6wbKdT97tbj-VnhHHW6csDf-W7lC_206qVSdcW6Nf7265pSTdtW2QSk0G6YKM39W4cGDLs7fFdGZW2Hdm5y69h50kf25hVbj04) that cover topics like elementary mathematics, United States history, computer science, and law, RA-DIT 65B achieved 49.1 percent accuracy, while the combination of LLaMA 2 65B and DRAGON+ without fine-tuning achieved 45.1 percent accuracy, and LLaMA 2 65B without retrieval achieved 32.9 percent accuracy. When the input included five examples that showed the model how to perform the task, RA-DIT 65B achieved 51.8 percent accuracy, LLaMA 2 65B combined with DRAGON+ achieved 51.1 percent accuracy, and LLaMA 2 65B alone achieved 47.2 percent accuracy. On average, over eight common-sense reasoning tasks such as [ARC-C](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVV2yX8VvszWW12sqT32C1DrzW1FPBGm5d1MfMN1PJbts3qgyTW6N1vHY6lZ3pSW5QT0LM7DGvMVW5R98f389rpD1W7XmhhD5gN7MbV-m9Rn3qnZsmW6jjbbb3TL7krW2PK_tH5n942PW2dXQ-w93PcJCMvWhQqd_SwMW1hP-g610rVGBW4l2LgW4BXNK7W2KTF_Z6j918QW96x8-q7v8WGWW26W2K_46FlpZW68qYXp2sd5k2W81g0WC5-5mMSW6TxZxM3SFbk-VCZnM33JqG9YW2TpHVd4fmT-6W6b1NNS3Kqqs0VChP8p5c2XmXVWGHVh4J2bCHW8ZJlsP7XYvhwf7rkJCH04), which involves common-sense physics such as the buoyancy of wood, RA-DIT 65B achieved 74.9 percent accuracy, LLaMA 2 65B with DRAGON+ achieved 74.5 percent accuracy, and LLaMA 2 achieved 72.1 percent accuracy. ([View Highlight](https://read.readwise.io/read/01hwnq3yxvdxtk31gnpv8aqa53))
- Why it matters: This method offers an inexpensive way to improve LLM performance with RAG. ([View Highlight](https://read.readwise.io/read/01hwnq4bfacc01xfv4kpskb601))
- We’re thinking: Many developers have found that putting more effort into the retriever, to make sure it provides the most relevant text, improves RAG performance. Putting more effort into the LLM helps, too. ([View Highlight](https://read.readwise.io/read/01hwnq4dsz3hvn2ne8dac3cyjc))

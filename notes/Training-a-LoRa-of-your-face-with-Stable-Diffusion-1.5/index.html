<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="In these notes, I am sharing my current workflow for using LoRas to generate images of myself and my loved ones."><title>Training a LoRa of your face with Stable Diffusion 1.5</title><meta name=viewport content="width=device-width,initial-scale=1"><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjFhYTEyN2VjLWI0MTEtNDdjNS1iNWQzLTA5OGE2NDZjMWZhYSIsICJpZCI6ICI5YzFhNzE4Zi0xNjA1LTRmMTUtOGQ3Yy05NzliMDBjNWVmNTcifQ.-aCZYTIrTPiQCDYC_zS0dH0IVOXqI9ThnK39DMDgY7c></script><link rel="shortcut icon" type=image/png href=https://pelayoarbues.github.io//icon.png><link href=https://pelayoarbues.github.io/styles.23ddc3ecf7e887d89acd50af3b520de2.min.css rel=stylesheet><link href=https://pelayoarbues.github.io/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css rel=stylesheet id=theme-link><script src=https://pelayoarbues.github.io/js/darkmode.d1ab992b86a0866f970f82ef4d95b010.min.js></script>
<script src=https://pelayoarbues.github.io/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://pelayoarbues.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pelayoarbues.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script>const isReducedMotion=window.matchMedia("(prefers-reduced-motion: reduce)").matches,lastVisit=localStorage.getItem("lastVisitTime"),now=Date.now();let show="true";if(lastVisit){document.documentElement.setAttribute("visited","true");const e=Math.ceil((now-parseInt(lastVisit))/(1e3*60));show=!isReducedMotion&&e>5?"true":"false"}document.documentElement.setAttribute("show-animation",show),localStorage.setItem("lastVisitTime",`${now}`);const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pelayoarbues.github.io/",fetchData=Promise.all([fetch("https://pelayoarbues.github.io/indices/linkIndex.a3e6a14b556acd8710849c904877132f.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pelayoarbues.github.io/indices/contentIndex.2705b97cfca577c6c5317904e1b48286.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),initPopover("https://pelayoarbues.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pelayoarbues.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:.8,opacityScale:3,repelForce:2,scale:1})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pelayoarbues.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pelayoarbues.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pelayoarbues.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div id=cursor-chat-layer><input type=text id=cursor-chat-box></div><script type=module>
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script><div class=singlePage><header class="delay t-3"><h1 id=page-title><a class=root-title href=https://pelayoarbues.github.io/>Pelayo Arbués</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Training a LoRa of your face with Stable Diffusion 1.5</h1><p class=meta>Last updated
Oct 6, 2023</p><ul class=tags><li><a href=https://pelayoarbues.github.io/tags/permanent-note/>Permanent Note</a></li><li><a href=https://pelayoarbues.github.io/tags/stablediffusion/>Stablediffusion</a></li><li><a href=https://pelayoarbues.github.io/tags/guide/>Guide</a></li></ul><p><img src=https://pelayoarbues.github.io//notes/attachments/Pasted%20image%2020231006213341.png width=auto alt></p><p>In these notes, I am sharing my current workflow for using LoRas to generate images of
<a href=https://www.instagram.com/mygenerativefamily/ rel=noopener>myself and my loved ones</a>. The goal is to offer practical insights into what works best and areas that need improvement.</p><p>This post is intended not only as a reference for my future self but also as a knowledge-sharing platform for my colleagues. We all have some working knowledge about Diffusion, Attention Mechanisms, etc. If you need a refresher, I strongly recommend learning more about
<a href=/notes/Stable-Diffusion-technicalities/ rel=noopener class=internal-link data-src=/notes/Stable-Diffusion-technicalities/>Stable Diffusion technicalities</a>.</p><blockquote><p>[!warning] SD15 vs SDXL
Currently, this post focuses on Stable Diffusion 1.5 as part of my slow-paced learning journey. Before advancing to SDXL, I aim to explore the full potential of SD15.</p></blockquote><p>Now let&rsquo;s delve into the tech stack so we can swiftly move on to more engaging topics:</p><ul><li><a href=/notes/MacBook-Pro-preparation-for-SD-training-and-inference/ rel=noopener class=internal-link data-src=/notes/MacBook-Pro-preparation-for-SD-training-and-inference/>MacBook Pro environment preparation</a></li><li>Kohya</li><li>Automatic1111</li></ul><p>Presently, both training and inference are run on my laptop. However, if you wish to migrate these tasks to a Cloud service, I&rsquo;ve heard great things about
<a href=https://rentry.co/sdxl-lora-training rel=noopener>Runpod</a>,
<a href=https://blog.paperspace.com/training-a-lora-model-for-stable-diffusion-xl-with-paperspace/ rel=noopener>Paperspace</a>, or you could even try
<a href=https://github.com/camenduru/kohya_ss-colab rel=noopener>Colab</a>.</p><a href=#training-the-lora><h1 id=training-the-lora><span class=hanchor arialabel=Anchor># </span>Training the LORA</h1></a><a href=#dataset-preparation><h2 id=dataset-preparation><span class=hanchor arialabel=Anchor># </span>Dataset Preparation</h2></a><a href=#collecting-images><h3 id=collecting-images><span class=hanchor arialabel=Anchor># </span>Collecting Images</h3></a><p>To train your LORA effectively, you&rsquo;ll need an assortment of pictures. Quality and variety are key factors here. You should aim for different facial expressions, clothing styles, types of headshots (
<a href=https://www.studiobinder.com/blog/ultimate-guide-to-camera-shots/ rel=noopener>3/4, 1/2, 1/4, Full-Body Portraits</a>), angles (not just full frontal headshots), different lighting conditions, various lenses&mldr;the more diverse, the better. Focus on the faces unless the character has distinctive body features.</p><p><img src=https://pelayoarbues.github.io//notes/attachments/Screenshot%202023-10-08%20at%2022.49.20.png width=auto alt></p><p>As for the number of images required, I can&rsquo;t make any scientific claims yet due to limited testing. However, as a passionate
<a rel=noopener class="internal-link broken" data-src=mocs/photography>photographer</a>, I have an extensive collection of pictures, especially of
<a href=https://www.instagram.com/violetabypelayo/ rel=noopener>Violeta</a>. For a LORA of myself, I used approximately 70 images and over 200 for Violeta. In both cases, the results were quite satisfactory. While some claim that 30 images are enough, my personal tests didn&rsquo;t prioritize efficiency (smaller sample size might reduce training times), and I wanted to include images of us at different ages.</p><a href=#processing-images><h3 id=processing-images><span class=hanchor arialabel=Anchor># </span>Processing Images</h3></a><p>Currently, all my pictures are resized to a resolution of 512x512 - recommended for SD15. My experiments with 512x768 (and 768x512) haven&rsquo;t shown significant differences in training outcomes. However, some posts suggest maintaining a 1:1 aspect ratio format is beneficial. For cropping and resizing tasks, I use
<a href=https://www.birme.net/ rel=noopener>BIRME</a> - an online and free tool.</p><p><img src=https://pelayoarbues.github.io//notes/attachments/Screenshot%202023-10-08%20at%2022.53.00.png width=auto alt></p><a href=#captioning><h3 id=captioning><span class=hanchor arialabel=Anchor># </span>Captioning</h3></a><p>Once you have your folder filled with appropriately sized photos, it&rsquo;s recommended to caption these images. Although not mandatory, providing good captions is highly encouraged.</p><blockquote><p>[!warning] Warning
For now, this part of my process is automated but in future I&rsquo;ll likely manually adjust automatically-generated captions.</p></blockquote><p>Here&rsquo;s how you can quickly create captions: In Kohya, select the top tab titled Utilities > Captioning and choose a type of captioning. For characters (as they call when you train a Lora with a person or anime), WD14 is said to work better. Select the folder, remove any undesired tags and add a prefix to captions. In my case, I use the trigger word I&rsquo;ll be using for training the LoRa - something that isn&rsquo;t known by the base model on top you&rsquo;ll be training. For me, this is <code>pelarbues</code> - an unlikely real word that helps me identify my character Lora.</p><p><img src=https://pelayoarbues.github.io//notes/attachments/captioning-kohya.png width=auto alt></p><p>If you want to review the captions, you can use the Automatic1111 extension named Dataset Tag Editor.</p><a href=#preparing-image-folders><h3 id=preparing-image-folders><span class=hanchor arialabel=Anchor># </span>Preparing Image Folders</h3></a><p>So far, we&rsquo;ve collected images, resized them to a specific resolution and added captions. Now we need to organize these images into folders in a format recognized by Kohya trainer scripts. This can be done manually as it merely involves pointing to certain folders and specifying the number of image repeats per epoch.</p><p><img src=https://pelayoarbues.github.io//notes/attachments/dataset-preparation-kohya.png width=auto alt></p><p>The easiest way is via Kohya&rsquo;s LoRa tab > Training > Dataset Preparation. Here you&rsquo;ll need to provide:</p><ul><li>Instance prompt: In my case, this is &lsquo;pelarbues&rsquo;. I use the class &lsquo;man&rsquo;, although others suggest using &lsquo;person&rsquo; is fine too.</li><li>Training images: Indicate the folder containing your resized training images with captions.</li><li>Repeats: Indicate number of repeats.</li><li>Regularisation images: To avoid overfitting your character, include regularisation images. I&rsquo;ve used
<a href=https://github.com/tobecwb/stable-diffusion-Regularization-Images rel=noopener>these regularization images</a> repeated once.</li><li>Destination training directory: Simply a path where folders and subfolders will be created for training images, regularisation and other items such as log and models.</li></ul><p>With the image preparation complete, let&rsquo;s move on to LoRa training.</p><a href=#lora-training><h2 id=lora-training><span class=hanchor arialabel=Anchor># </span>LoRa Training</h2></a><p>Now, let&rsquo;s discuss how to train the LoRa. Please note that I&rsquo;ll be sharing a configuration that works for me, considering:</p><ul><li>I am using an Apple Silicon device</li><li>I have approximately 70 images</li></ul><p>Here is the command and options I used. You can achieve the same using the UI, but personally, I find using the terminal more comfortable at this stage of the process.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>accelerate launch --num_cpu_threads_per_process=4 train_network.py \
</span></span><span class=line><span class=cl>     --train_data_dir=&#34;/Users/pelayo/Documents/stable-diffusion/pelayo-destination-train/img&#34; \
</span></span><span class=line><span class=cl>     --reg_data_dir=&#34;/Users/pelayo/Documents/stable-diffusion/pelayo-destination-train/reg&#34; \
</span></span><span class=line><span class=cl>     --output_dir=&#34;/Users/pelayo/Documents/stable-diffusion/pelayo-destination-train/model&#34; \
</span></span><span class=line><span class=cl>    --logging_dir=&#34;/Users/pelayo/Documents/stable-diffusion/pelayo-destination-train/log&#34; \
</span></span><span class=line><span class=cl>    --output_name=pelarbues \
</span></span><span class=line><span class=cl>    --training_comment=&#34;activated using pelarbues&#34; \
</span></span><span class=line><span class=cl>     --pretrained_model_name_or_path=&#34;runwayml/stable-diffusion-v1-5&#34; \
</span></span><span class=line><span class=cl>    --network_module=networks.lora \
</span></span><span class=line><span class=cl>    --prior_loss_weight=1.0 --resolution 512 \
</span></span><span class=line><span class=cl>    --train_batch_size=&#34;2&#34; --learning_rate=2e-4 \
</span></span><span class=line><span class=cl>    --max_train_steps=&#34;4500&#34; \
</span></span><span class=line><span class=cl>    --lr_scheduler=&#34;cosine&#34; --lr_warmup_steps=&#34;300&#34;  --lr_scheduler_num_cycles=&#34;4&#34; --no_half_vae \
</span></span><span class=line><span class=cl>    --network_dim=32 --network_alpha=&#34;16&#34; \
</span></span><span class=line><span class=cl>    --text_encoder_lr=0.0001 --
</span></span></code></pre></td></tr></table></div></div><p>Some important things to note:</p><ul><li>For my MacBook, I need to maintain these
<a href=https://github.com/bmaltais/kohya_ss/discussions/1185 rel=noopener>settings</a> to ensure it functions correctly:</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>--mixed_precision<span class=o>=</span><span class=s2>&#34;no&#34;</span>
</span></span><span class=line><span class=cl>--save_precision<span class=o>=</span><span class=s2>&#34;float&#34;</span>
</span></span><span class=line><span class=cl>--optimizer_type<span class=o>=</span><span class=s2>&#34;AdamW&#34;</span>
</span></span><span class=line><span class=cl>--no_half_vae
</span></span></code></pre></td></tr></table></div></div><ul><li>Other significant parameters include:<ul><li>Use SD15 vanilla. I haven&rsquo;t tried other photorealistic models as a base network, but the general advice is to stick with the SD15 base model.</li><li>Networks.lora: There are various types of interesting networks you can train. Check
<a href=/notes/Stable-Diffusion-technicalities/ rel=noopener class=internal-link data-src=/notes/Stable-Diffusion-technicalities/>links about Lycoris</a> for more information.</li><li>Network dimension (rank). The network rank specifies the number of neurons in the hidden layer. The larger the number of neurons, the more learning information can be stored. However, this could risk overfitting and also increase LoRa file size. Currently, I am experimenting with a rank of 32 for characters.</li><li><a href=https://github.com/bmaltais/kohya_ss/wiki/LoRA-training-parameters#network-alpha rel=noopener>Network alpha</a> prevents weights from being rounded to 0. When setting the Network Alpha, consider its impact on the learning rate. For example, with an Alpha of 16 and a Rank of 32, the strength of the weight used is 16/32 = 0.5, meaning that the learning rate is only half as powerful as the Learning Rate setting.</li><li>Learning rate: This represents how large our steps are when tuning network weights - an important parameter in training networks. If you&rsquo;re unfamiliar with it, it&rsquo;s worth
<a href=https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate rel=noopener>learning about</a>.</li><li>Batch size: This refers to the number of images read at once. The larger the batch size, the shorter the learning time, but higher the memory requirements. Increasing the batch size typically calls for a higher learning rate. For example, if the batch size is 2, the learning rate should be doubled.</li><li>Epochs: This refers to a complete round of learning. In my example, I am using 70 images, each read 10 times. One epoch equals 70x10 = 700 training steps. If I use 2 epochs it will be 1400 and so on. It is set by the param <code>lr_scheduler_num_cycles</code>. According to this
<a href="https://www.youtube.com/watch?v=L2RVg82TBy0" rel=noopener>video</a>, 1500 is a good number of steps for characters, but I&rsquo;ve found that 3k or 4k works better.</li><li>The &lsquo;Save every N epochs&rsquo; option allows you to keep intermediate model checkpoints.</li></ul></li></ul><p>To keep this brief and because explanations are available at
<a href=https://github.com/bmaltais/kohya_ss/wiki/LoRA-training-parameters rel=noopener>LoRA training parameters Wiki</a>, I recommend checking it out if you want to explore other parameters used in my example.</p><blockquote><p>[!hint] Hints</p><ul><li>Check
<a href=https://civitai.com/articles/83/using-tensorboard-to-analyze-training-data-and-create-better-models rel=noopener>Tensorboard</a> during training to monitor potential overfitting issues.</li><li>If you&rsquo;ve chosen to save a checkpoint for each epoch, you can prompt intermediate epochs and review results obtained using different epochs. Don´t blindly rely on loss metrics.</li></ul></blockquote><a href=#inference><h1 id=inference><span class=hanchor arialabel=Anchor># </span>Inference</h1></a><p>Currently, I&rsquo;m using
<a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui rel=noopener>Automatic1111</a>, but some have suggested switching to
<a href=https://aituts.com/comfyui/ rel=noopener>ComfyUI</a>, a graph-based GUI that&rsquo;s touted as being more flexible. However, I haven&rsquo;t made that leap just yet.</p><p>It&rsquo;s essential to familiarize yourself with the GUI before delving into the specifics. This is where an excellent introductory guide to using
<a href=https://www.reddit.com/r/StableDiffusion/comments/yz2tbo/noobs_guide_to_using_automatic1111s_webui/ rel=noopener>Automatic1111</a> comes in handy. Now that we&rsquo;re all set, let&rsquo;s create some images with our freshly trained model. I&rsquo;ll walk you through my current workflow.</p><a href=#image-generation-workflow><h2 id=image-generation-workflow><span class=hanchor arialabel=Anchor># </span>Image Generation Workflow</h2></a><p>The primary objective is to explore a concept as quickly as possible by generating numerous rough images that mirror what I&rsquo;m picturing in my mind. These images will have a specific look and feel, pose and atmosphere. From this collection, I select the ones closest to my vision and refine them by upscaling and tweaking until they closely resemble the character.</p><p>Before we proceed, I highly recommend installing the
<a href=https://github.com/zanllp/sd-webui-infinite-image-browsing rel=noopener>Infinite Image Browsing extension</a>. It will significantly aid your image review process.</p><p>Without further ado, let&rsquo;s dive into the workflow.</p><a href=#1-prompt-it-like-you-mean-it><h3 id=1-prompt-it-like-you-mean-it><span class=hanchor arialabel=Anchor># </span>1. Prompt it Like You Mean it</h3></a><p>I kick things off with a straightforward prompt describing the type of portrait: who the character is, what they&rsquo;re doing, where they are etc. To make it look as good as possible, I also incorporate some photography terms. For instance:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>(full body:1.4) of pelarbues man, solo, 1950s candid photo, 1950s office,  1950s office furniture, elegant office suit,scene from mad men, asymmetric face, dark hair, beard, thin, skin pores, focus on face,film grain,volumetric light, shadows, kodak portra 400, &lt;lora:pelarbues:0.7&gt;
</span></span></code></pre></td></tr></table></div></div><p>For negative prompts (things you don&rsquo;t want to appear), you might consider something like:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, deformed legs, mutilated body, (2D), (3D), ((camera)), ((cartoony)), (CGI), ((drawing)), ((lowres)), ((painting)), ((sketchy)), ((The Sims)), (ugly), ((blur)) (worst quality, low quality, normal quality:1.3), lowres
</span></span><span class=line><span class=cl>(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4)
</span></span></code></pre></td></tr></table></div></div><p>Or you can keep it succinct and download negative embeddings that work better with certain model checkpoints. I&rsquo;ve
<a href=https://civitai.com/collections/67040 rel=noopener>selected some</a> that have been particularly effective for me.</p><a href=#2-experiment><h3 id=2-experiment><span class=hanchor arialabel=Anchor># </span>2. Experiment</h3></a><p>Now comes the fun part: experimentation!</p><ul><li>We are going to set 30 sample steps (something between 20-50 might work depending on the checkpoint) And a CFG of 5 (something between 5 and 7 usually do)</li><li>Make sure you have restore faces unchecked! We will fix the faces later on.</li><li>In the bottom of the page we will make use of Script x/y/z that will allows us to plot a grid of different parameters. We are going to put some models to the test, and also samplers and seeds. But you could select sample steps, CFG values or whatever.</li></ul><p><img src=https://pelayoarbues.github.io//notes/attachments/image-generation-automatic1111.png width=auto alt></p><p><img src=https://pelayoarbues.github.io//notes/attachments/xyz-script.png width=auto alt>
Now run it! It may take a while, but you can grab a coffee and relax in the Infinite Image Browsing tab while your machine does the heavy lifting.</p><blockquote><p>[!hint] Hints</p><ul><li>Experiment to find your preferred checkpoint and samplers</li><li>Avoid using face restoration initially</li><li>Fine-tune the prompt using other LoRas.</li><li>Upscale using Adetailer + Controlnet</li></ul></blockquote><p>After analyzing the generated images, we can see some issues such as people appearing in the images who shouldn&rsquo;t be there. We can control this by tweaking the prompt, but for now we can move on.</p><p><img src=https://pelayoarbues.github.io//notes/attachments/xyz-plot.png width=auto alt></p><a href=#3-upscale-and-fix-the-face><h3 id=3-upscale-and-fix-the-face><span class=hanchor arialabel=Anchor># </span>3. Upscale and Fix the Face</h3></a><p>To upscale and fix facial features (and hands), we&rsquo;re going to use a couple of Automatic extensions:
<a href=https://github.com/Bing-su/adetailer rel=noopener>!After Detailer</a> with
<a href=https://github.com/Mikubill/sd-webui-controlnet rel=noopener>ControlNet</a> for face fixing and
<a href=https://github.com/Coyote-A/ultimate-upscale-for-automatic1111 rel=noopener>Ultimate SD upscale</a> for upscaling.</p><ul><li>Fixing the face and the hands requires some inpainting. We could do this by manually Inpainting in the img2img > Inpaint module, but this is a very manual process. The most important param I usually change es Inpaint Denoising Strength, I keep it between 0.25 and 0.4 for faces and sometimes I go to 0.5 for hands.</li></ul><p>Face:</p><p><img src=https://pelayoarbues.github.io//notes/attachments/adetailer-model-1.png width=auto alt></p><p>Hands:</p><p><img src=https://pelayoarbues.github.io//notes/attachments/adetailer-model-2.png width=auto alt></p><ul><li>Upscaling: I strongly recommend adding additional upscalers like [[
<a href=https://openmodeldb.info/models/4x-UltraSharp rel=noopener>UltraSharp - OpenModelDB</a>]] and
<a href=https://openmodeldb.info/models/4x-FaceUpSharpDAT rel=noopener>4xFaceUpSharpDAT - OpenModelDB</a> and
<a href=https://openmodeldb.info/models/4x-Remacri rel=noopener>Remacri</a>.</li></ul><p><img src=https://pelayoarbues.github.io//notes/attachments/ultimate-upscale.png width=auto alt></p><a href=#final-results><h2 id=final-results><span class=hanchor arialabel=Anchor># </span>Final results</h2></a><p><img src=https://pelayoarbues.github.io//notes/attachments/upscaled-office-images.png width=auto alt></p><p>So, here are our final results. Some of them are quite good, but I&rsquo;m not entirely satisfied with the overall look. In a following post, I&rsquo;ll discuss how to enhance these images by adding more detail, improving the focus and emulating the film grain.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pelayoarbues.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Pelayo Arbués using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2024</p><ul><li><a href=https://pelayoarbues.github.io/>Home</a></li><li><a href=https://twitter.com/pelayoarbues>Twitter</a></li><li><a href=https://sigmoid.social/@pelayoarbues>Mastodon</a></li><li><a href=https://www.linkedin.com/in/pelayoarbues/>Linkedin</a></li><li><a href="https://scholar.google.com/citations?user=WaC-GcIAAAAJ&hl">Scholar</a></li><li><a href=https://www.flickr.com/photos/wonderfulhorriblelife/>Flickr</a></li><li><a href=https://github.com/pelayoarbues>GitHub</a></li></ul></footer></div></div></body></html>